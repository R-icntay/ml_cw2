{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D-Unet without attention, simial to nn-unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random\n",
    "from shutil import copyfile\n",
    "import os\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random.seed(2022)\n",
    "\n",
    "# Read all files ending with _img.nii\n",
    "img_path = Path(\"data/data\")\n",
    "img_files = list(img_path.glob(\"*_img.nii\")) # Image and mask are in the same folder\n",
    "num_images = len(img_files) \n",
    "\n",
    "# Create train, validation and test splits\n",
    "train_split = int(0.8 * num_images)\n",
    "val_split = int(0.10 * num_images)\n",
    "test_split = int(num_images - (train_split + val_split))\n",
    "\n",
    "# Shuffle the image files\n",
    "random.shuffle(img_files)\n",
    "\n",
    "# Split the dataset\n",
    "train_images = img_files[:train_split]\n",
    "val_images = img_files[train_split:(train_split + val_split)]\n",
    "test_images = img_files[(train_split + val_split): ]\n",
    "\n",
    "# Create train, validation and test directories\n",
    "train_image_dir = Path(img_path / \"train_images\")\n",
    "train_mask_dir = Path(img_path / \"train_masks\")\n",
    "val_image_dir = Path(img_path / \"val_images\")\n",
    "val_mask_dir = Path(img_path / \"val_masks\")\n",
    "test_image_dir = Path(img_path / \"test_images\")\n",
    "test_mask_dir = Path(img_path / \"test_masks\")\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "if not os.path.exists(train_image_dir) and not os.path.exists(train_mask_dir) and not os.path.exists(val_image_dir) and not os.path.exists(val_mask_dir) and not os.path.exists(test_image_dir) and not os.path.exists(test_mask_dir):\n",
    "    for directory in [train_image_dir, train_mask_dir, val_image_dir, val_mask_dir, test_image_dir, test_mask_dir]:\n",
    "        directory.mkdir(exist_ok = True, parents = True)\n",
    "\n",
    "    # Copy the images and their corresponding segmentation masks to their respective directories\n",
    "    for directory, images in zip([train_image_dir, val_image_dir, test_image_dir], [train_images, val_images, test_images]):\n",
    "        for image in images:\n",
    "            # Copy image\n",
    "            copyfile(image, directory / image.name)\n",
    "\n",
    "            # Get corresponding segmentation mask\n",
    "            mask = image.name.replace(\"_img.nii\", \"_mask.nii\")\n",
    "\n",
    "            # Copy segmentation mask\n",
    "            copyfile(image.parent / mask, image.parent / directory.name.replace(\"images\", \"masks\") / mask)\n",
    "\n",
    "\n",
    "# # Sanity check\n",
    "# train_images_s = list(train_image_dir.glob(\"*\"))\n",
    "# train_images_s = [image.name.removesuffix(\"_img.nii\") for image in train_images_s]\n",
    "# train_masks_s = list(train_mask_dir.glob(\"*\"))\n",
    "# train_masks_s = [mask.name.removesuffix(\"_mask.nii\") for mask in train_masks_s]\n",
    "# train_images_s == train_masks_s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "\n",
    "# Define a Residual block\n",
    "class residual_block(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements a residual block which consists of two convolution layers with group normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, n_groups = 8):\n",
    "        super().__init__()\n",
    "        # First convolution layer\n",
    "        self.first_conv = nn.Conv3d(in_channels = in_channels, out_channels = out_channels, kernel_size = 3, padding = 1, bias=False)\n",
    "        self.first_norm = nn.GroupNorm(num_groups = n_groups, num_channels = out_channels)\n",
    "        self.act1 = nn.SiLU() # Swish activation function\n",
    "\n",
    "        # Second convolution layer\n",
    "        self.second_conv = nn.Conv3d(in_channels = out_channels, out_channels = out_channels, kernel_size = 3, padding = 1, bias = False)\n",
    "        self.second_norm = nn.GroupNorm(num_groups = n_groups, num_channels = out_channels)\n",
    "        self.act2 = nn.SiLU() # Swish activation function\n",
    "\n",
    "        # Add dropout to the residual block\n",
    "        self.dropout = nn.Dropout3d(p = 0.2)\n",
    "\n",
    "        # If the number of input channels is not equal to the number of output channels,\n",
    "        # then use a 1X1 convolution layer to compensate for the difference in dimensions\n",
    "        # This allows the input to have the same dimensions as the output of the residual block\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv3d(in_channels = in_channels, out_channels = out_channels, kernel_size = 1, padding = 0, bias = False)\n",
    "        else:\n",
    "            # Pass the input as is\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    # Pass the input through the residual block\n",
    "    def forward(self, x):\n",
    "        # Store the input\n",
    "        input = x\n",
    "\n",
    "        # Pass input through the first convolution layer\n",
    "        x = self.act1(self.first_norm(self.first_conv(x)))\n",
    "\n",
    "        # Pass the output of the first convolution layer through the second convolution layer\n",
    "        x = self.act2(self.second_norm(self.second_conv(x)))\n",
    "\n",
    "        # Add dropout\n",
    "        x = self.dropout(x)\n",
    "\n",
    "\n",
    "        # Add the input to the output of the second convolution layer\n",
    "        # This is the skip connection\n",
    "        x = x + self.shortcut(input)\n",
    "        return x\n",
    "\n",
    "# Implement the DownSample block that occurs after each residual block\n",
    "class down_sample(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.max_pool = nn.MaxPool3d(kernel_size = 2, stride = 2)\n",
    "\n",
    "    # Pass the input through the downsample block\n",
    "    def forward(self, x):\n",
    "        x = self.max_pool(x)\n",
    "        return x\n",
    "\n",
    "# Implement the UpSample block that occurs in the decoder path/expanding path\n",
    "class up_sample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolution transpose layer to upsample the input\n",
    "        self.up_sample = nn.ConvTranspose3d(in_channels = in_channels, out_channels = out_channels, kernel_size = 2, stride = 2, bias = False)\n",
    "\n",
    "    # Pass the input through the upsample block\n",
    "    def forward(self, x):\n",
    "        x = self.up_sample(x)\n",
    "        return x\n",
    "\n",
    "# Implement the crop and concatenate layer\n",
    "class crop_and_concatenate(nn.Module):\n",
    "    def forward(self, upsampled, bypass):\n",
    "        # Crop the upsampled feature map to match the dimensions of the bypass feature map\n",
    "        if upsampled.shape[2:] != bypass.shape[2:]:\n",
    "            upsampled = nn.Upsample(size = bypass.shape[2:], mode=\"trilinear\", align_corners=True)(upsampled)\n",
    "\n",
    "        #upsampled = torchvision.transforms.functional.resize(upsampled, size = bypass.shape[2:], antialias=True)\n",
    "        x = torch.cat([upsampled, bypass], dim = 1) # Concatenate along the channel dimension\n",
    "        return x\n",
    "\n",
    "# Implement an attention block\n",
    "class attention_block(nn.Module):\n",
    "    def __init__(self, skip_channels, gate_channels, inter_channels = None, n_groups = 8):\n",
    "        super().__init__()\n",
    "\n",
    "        if inter_channels is None:\n",
    "            inter_channels = skip_channels // 2\n",
    "\n",
    "        # Implement W_g i.e the convolution layer that operates on the gate signal\n",
    "        # Upsample gate signal to be the same size as the skip connection\n",
    "        self.W_g = up_sample(in_channels = gate_channels, out_channels = skip_channels)\n",
    "        self.W_g_norm = nn.GroupNorm(num_groups = n_groups, num_channels = skip_channels)\n",
    "        self.W_g_act = nn.SiLU() # Swish activation function\n",
    "\n",
    "        # Implement W_x i.e the convolution layer that operates on the skip connection\n",
    "        self.W_x = nn.Conv3d(in_channels = skip_channels, out_channels = inter_channels, kernel_size = 1, padding = 0, bias = False)\n",
    "        self.W_x_norm = nn.GroupNorm(num_groups = n_groups, num_channels = inter_channels)\n",
    "        self.W_x_act = nn.SiLU() # Swish activation function\n",
    "\n",
    "        # Implement phi i.e the convolution layer that operates on the output of W_x + W_g\n",
    "        self.phi = nn.Conv3d(in_channels = inter_channels, out_channels = 1, kernel_size = 1, padding = 0, bias = False)\n",
    "        #self.phi_norm = nn.GroupNorm(num_groups = n_groups, num_channels = 1)\n",
    "        #self.phi_act = nn.SiLU() # Swish activation function\n",
    "\n",
    "        # Implement the sigmoid activation function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # Implement the Swish activation function\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        # Implement final group normalization layer\n",
    "        self.final_norm = nn.GroupNorm(num_groups = n_groups, num_channels = skip_channels)\n",
    "\n",
    "    # Pass the input through the attention block\n",
    "    def forward(self, skip_connection, gate_signal):\n",
    "        # Upsample the gate signal to match the channels of the skip connection\n",
    "        gate_signal = self.W_g(gate_signal)\n",
    "        # Ensure that the sizes of the skip connection and the gate signal match before addition\n",
    "        if gate_signal.shape[2:] != skip_connection.shape[2:]:\n",
    "            gate_signal = nn.Upsample(size = skip_connection.shape[2:], mode=\"trilinear\", align_corners=True)(gate_signal)\n",
    "            #gate_signal = torchvision.transforms.functional.resize(gate_signal, size = skip_connection.shape[2:], antialias=True)\n",
    "        # Project to the intermediate channels\n",
    "        gate_signal = self.W_x(gate_signal)\n",
    "\n",
    "        # Project the skip connection to the intermediate channels\n",
    "        skip_signal = self.W_x(skip_connection)\n",
    "\n",
    "        # Add the skip connection and the gate signal\n",
    "        add_xg = gate_signal + skip_signal\n",
    "\n",
    "        # Pass the output of the addition through the activation function\n",
    "        add_xg = self.act(add_xg)\n",
    "\n",
    "        # Pass the output of attention through a 1x1 convolution layer to obtain the attention map\n",
    "        attention_map = self.sigmoid(self.phi(add_xg))\n",
    "\n",
    "        # Multiply the skip connection with the attention map\n",
    "        # Perform element-wise multiplication\n",
    "        skip_connection = torch.mul(skip_connection, attention_map)\n",
    "\n",
    "        skip_connection = nn.Conv3d(in_channels = skip_connection.shape[1], out_channels = skip_connection.shape[1], kernel_size = 1, bias=False).to(device)(skip_connection)\n",
    "        skip_connection = self.act(self.final_norm(skip_connection))\n",
    "\n",
    "        return skip_connection\n",
    "\n",
    "\n",
    "## Implement a 3D residual attention U-Net\n",
    "class ResidualAttention3DUnet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_groups = 4, n_channels = [16, 32, 64, 128, 256]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the contracting path: residual blocks followed by downsampling\n",
    "        self.down_conv = nn.ModuleList(residual_block(in_chans, out_chans) for in_chans, out_chans in\n",
    "                                       [(in_channels, n_channels[0]), (n_channels[0], n_channels[1]), (n_channels[1], n_channels[2]), (n_channels[2], n_channels[3])])\n",
    "        self.down_samples = nn.ModuleList(down_sample() for _ in range(4))\n",
    "\n",
    "        # Define the bottleneck residual block\n",
    "        self.bottleneck = residual_block(n_channels[3], n_channels[4])\n",
    "\n",
    "\n",
    "        # Define the attention blocks\n",
    "        self.attention_blocks = nn.ModuleList(attention_block(skip_channels = residuals_chans, gate_channels = gate_chans) for gate_chans, residuals_chans in\n",
    "                                              [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
    "\n",
    "\n",
    "        # Define the expanding path: upsample blocks, followed by crop and concatenate, followed by residual blocks\n",
    "        self.upsamples = nn.ModuleList(up_sample(in_chans, out_chans) for in_chans, out_chans in\n",
    "                                       [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
    "        \n",
    "        self.concat = nn.ModuleList(crop_and_concatenate() for _ in range(4))\n",
    "\n",
    "        self.up_conv = nn.ModuleList(residual_block(in_chans, out_chans) for in_chans, out_chans in\n",
    "                                     [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
    "        \n",
    "        # Final 1X1 convolution layer to produce the output segmentation map:\n",
    "        # The primary purpose of 1x1 convolutions is to transform the channel dimension of the feature map,\n",
    "        # while leaving the spatial dimensions unchanged.\n",
    "        self.final_conv = nn.Conv3d(in_channels = n_channels[0] , out_channels = out_channels, kernel_size = 1, padding = 0, bias = False)\n",
    "\n",
    "    # Pass the input through the residual attention U-Net\n",
    "    def forward(self, x):\n",
    "        # Store the skip connections\n",
    "        skip_connections = []\n",
    "        # # Store the gate signals\n",
    "        # gate_signals = []\n",
    "\n",
    "        # Pass the input through the contracting path\n",
    "        for down_conv, down_sample in zip(self.down_conv, self.down_samples):\n",
    "            x = down_conv(x)\n",
    "            skip_connections.append(x)\n",
    "            #gate_signals.append(x)\n",
    "            x = down_sample(x)\n",
    "        \n",
    "        # Pass the output of the contracting path through the bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections.append(x)\n",
    "\n",
    "        # Attention on the residual connections\n",
    "        n = len(skip_connections)\n",
    "        indices = [(n - 1 - i, n - 2 - i) for i in range(n - 1)]\n",
    "        attentions = []\n",
    "        for i, g_x in enumerate(indices):\n",
    "            g_gate = g_x[0]\n",
    "            x_residual = g_x[1]\n",
    "            attn = self.attention_blocks[i](skip_connections[x_residual], skip_connections[g_gate])\n",
    "            attentions.append(attn)\n",
    "\n",
    "        #attentions = attentions[::-1]\n",
    "    \n",
    "        # Pass the output of the attention blocks through the expanding path\n",
    "        for up_sample, concat, up_conv in zip(self.upsamples, self.concat, self.up_conv):\n",
    "            x = up_sample(x)\n",
    "            x = concat(x, attentions.pop(0))\n",
    "            x = up_conv(x)\n",
    "\n",
    "        # Pass the output of the expanding path through the final convolution layer\n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 40, 256, 256]), torch.Size([2, 3, 40, 256, 256]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Implement a MTL 3D residual attention U-Net\n",
    "class MTLResidualAttention3DUnet(nn.Module):\n",
    "    def __init__(self, in_channels, main_out_channels, aux_out_channels, n_groups = 4, n_channels = [32, 64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the contracting path: residual blocks followed by downsampling\n",
    "        self.down_conv = nn.ModuleList(residual_block(in_chans, out_chans) for in_chans, out_chans in\n",
    "                                       [(in_channels, n_channels[0]), (n_channels[0], n_channels[1]), (n_channels[1], n_channels[2]), (n_channels[2], n_channels[3])])\n",
    "        self.down_samples = nn.ModuleList(down_sample() for _ in range(len(n_channels) - 1))\n",
    "\n",
    "        # Define the bottleneck residual block\n",
    "        self.bottleneck = residual_block(n_channels[3], n_channels[4])\n",
    "\n",
    "        ## ------ Decoder block for segmenting main prostate zones: central, transition, background ------ ##\n",
    "        # Define the attention blocks\n",
    "        self.attention_blocks_main = nn.ModuleList(attention_block(skip_channels = residuals_chans, gate_channels = gate_chans) for gate_chans, residuals_chans in\n",
    "                                              [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
    "        \n",
    "\n",
    "        # Define the expanding path: upsample blocks, followed by crop and concatenate, followed by residual blocks\n",
    "        self.upsamples_main = nn.ModuleList(up_sample(in_chans, out_chans) for in_chans, out_chans in\n",
    "                                        [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
    "        \n",
    "        self.concat_main = nn.ModuleList(crop_and_concatenate() for _ in range(len(n_channels) - 1))\n",
    "\n",
    "        self.up_conv_main = nn.ModuleList(residual_block(in_chans, out_chans) for in_chans, out_chans in\n",
    "                                            [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
    "        \n",
    "        # Final 1X1 convolution layer to produce the output segmentation map:\n",
    "        # The primary purpose of 1x1 convolutions is to transform the channel dimension of the feature map,\n",
    "        # while leaving the spatial dimensions unchanged.\n",
    "        self.final_conv_main = nn.Conv3d(in_channels = n_channels[0], out_channels = main_out_channels, kernel_size = 1, padding = 0, bias = False)\n",
    "\n",
    "\n",
    "\n",
    "        ## ------ Decoder block for segmenting the auxilliary zones: Bladder, Rectum, Seminal vesicle, Neurovascular bundle ------ ##\n",
    "        # Define the attention blocks\n",
    "        self.attention_blocks_aux = nn.ModuleList(attention_block(skip_channels = residuals_chans, gate_channels = gate_chans) for gate_chans, residuals_chans in\n",
    "                                              [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
    "        \n",
    "       # Define the expanding path: upsample blocks, followed by crop and concatenate, followed by residual blocks\n",
    "        self.upsamples_aux = nn.ModuleList(up_sample(in_chans, out_chans) for in_chans, out_chans in\n",
    "                                        [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
    "        \n",
    "        self.concat_aux = nn.ModuleList(crop_and_concatenate() for _ in range(len(n_channels) - 1))\n",
    "\n",
    "        self.up_conv_aux = nn.ModuleList(residual_block(in_chans, out_chans) for in_chans, out_chans in\n",
    "                                            [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
    "        \n",
    "        # Final 1X1 convolution layer to produce the output segmentation map:\n",
    "        # The primary purpose of 1x1 convolutions is to transform the channel dimension of the feature map,\n",
    "        # while leaving the spatial dimensions unchanged.\n",
    "        self.final_conv_aux = nn.Conv3d(in_channels = n_channels[0], out_channels = aux_out_channels, kernel_size = 1, padding = 0, bias = False) # 3 classes + background\n",
    "\n",
    "        \n",
    "        \n",
    "    # Pass the input through the residual attention U-Net\n",
    "    def forward(self, x):\n",
    "        # Store the skip connections\n",
    "        skip_connections = []\n",
    "\n",
    "        # Pass the input through the contracting path\n",
    "        for down_conv, down_sample in zip(self.down_conv, self.down_samples):\n",
    "            x = down_conv(x)\n",
    "            skip_connections.append(x)\n",
    "            x = down_sample(x)\n",
    "\n",
    "        # Pass the output of the contracting path through the bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "\n",
    "        # Define segmentation and reconstruction variables\n",
    "        x_main = x\n",
    "        x_aux = x\n",
    "\n",
    "        # --- Pass the output of the encoder through the decoder of the main prostate zones --- #\n",
    "        # Initialize the attention block counter and the skip connection counter\n",
    "        attn_block_count = 0\n",
    "        skip_connections_count = len(skip_connections)\n",
    "        for up_sample, concat, up_conv in zip(self.upsamples_main, self.concat_main, self.up_conv_main):\n",
    "            #gated_attn = self.attention_blocks_main[attn_block_count](skip_connections[skip_connections_count - 1], x_main)\n",
    "            #attn_block_count += 1\n",
    "            # skip_connections_count -= 1\n",
    "            x_main = up_sample(x_main)\n",
    "            #x_main = concat(x_main, gated_attn)\n",
    "            x_main = concat(x_main, skip_connections[skip_connections_count - 1])\n",
    "            x_main = up_conv(x_main)\n",
    "            skip_connections_count -= 1\n",
    "\n",
    "        # Pass the output of the main decoder through the final convolution layer\n",
    "        x_main = self.final_conv_main(x_main) # Output segmentation map for the main prostate zones\n",
    "\n",
    "    \n",
    "\n",
    "        # --- Pass the output of the encoder through the decoder of the auxilliary prostate zones --- #\n",
    "        # Initialize the attention block counter and the skip connection counter\n",
    "        attn_block_count = 0\n",
    "        skip_connections_count = len(skip_connections)\n",
    "        for up_sample, concat, up_conv in zip(self.upsamples_aux, self.concat_aux, self.up_conv_aux):\n",
    "            #gated_attn = self.attention_blocks_aux[attn_block_count](skip_connections[skip_connections_count - 1], x_aux)\n",
    "            #attn_block_count += 1\n",
    "            x_aux = up_sample(x_aux)\n",
    "            x_aux = concat(x_aux, skip_connections[skip_connections_count - 1])\n",
    "            x_aux = up_conv(x_aux)\n",
    "            skip_connections_count -= 1\n",
    "\n",
    "        # Pass the output of the auxilliary decoder through the final convolution layer\n",
    "        x_aux = self.final_conv_aux(x_aux) # Output segmentation map for the auxilliary prostate zones\n",
    "\n",
    "        # Return the segmentations for the main and auxilliary prostate zones\n",
    "        return x_main, x_aux\n",
    "    \n",
    "# Test whether the MTL model works\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "modelt = MTLResidualAttention3DUnet(in_channels = 1, main_out_channels = 4, aux_out_channels = 3).to(device)\n",
    "x = torch.randn(2, 1, 40, 256, 256).to(device)\n",
    "x_main, x_aux = modelt(x)\n",
    "x_main.shape, x_aux.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install monai\n",
    "from monai.utils import first, set_determinism \n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirstd, # Adjust or add the channel dimension of input data to ensure channel_first shape.\n",
    "    #AddChanneld,\n",
    "    CenterSpatialCropd,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    NormalizeIntensityd,\n",
    "    ScaleIntensityd,\n",
    "    Spacingd,\n",
    "    SpatialPadd,\n",
    "    ToTensord,\n",
    "    RandAffined, Resized, RandSpatialCropd,\n",
    "    CropForegroundd, # Crop the foreground region of the input image based on the provided mask to help training and evaluation if the valid part is small in the whole medical image\n",
    "    RandCropByPosNegLabeld, # Crop random patches from image and label based on positive / negative label ratio.\n",
    "    RandGaussianNoised, # Randomly add Gaussian noise to image.\n",
    "    RandGaussianSmoothd, # Randomly smooth image with Gaussian filter.\n",
    "    AdjustContrastd, # Adjust image contrast by gamma value.\n",
    "\n",
    ")\n",
    "\n",
    "# Set deterministic training for reproducibility\n",
    "set_determinism(seed = 2056)\n",
    "\n",
    "# Put the train images and masks in a dictionary\n",
    "train_images = sorted(train_image_dir.glob(\"*\"))\n",
    "train_masks = sorted(train_mask_dir.glob(\"*\"))\n",
    "train_files = [{\"image\": image_name, \"mask\": mask_name} for image_name, mask_name in zip(train_images, train_masks)]\n",
    "\n",
    "# Put the validation images and masks in a dictionary\n",
    "val_images = sorted(val_image_dir.glob(\"*\"))\n",
    "val_masks = sorted(val_mask_dir.glob(\"*\"))\n",
    "val_files = [{\"image\": image_name, \"mask\": mask_name} for image_name, mask_name in zip(val_images, val_masks)]\n",
    "\n",
    "# Put the test images and masks in a dictionary\n",
    "test_images = sorted(test_image_dir.glob(\"*\"))\n",
    "test_masks = sorted(test_mask_dir.glob(\"*\"))\n",
    "test_files = [{\"image\": image_name, \"mask\": mask_name} for image_name, mask_name in zip(test_images, test_masks)]\n",
    "\n",
    "# Create transforms for training\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys = [\"image\", \"mask\"]),\n",
    "        EnsureChannelFirstd(keys = [\"image\", \"mask\"]),\n",
    "        ScaleIntensityd(keys = \"image\"),\n",
    "        CropForegroundd(keys = [\"image\", \"mask\"], source_key = \"image\"),\n",
    "        Spacingd(\n",
    "            keys = [\"image\", \"mask\"],\n",
    "            pixdim = [0.75, 0.75, 2.5],\n",
    "            mode = (\"bilinear\", \"nearest\"), # Interpolation mode for image and mask\n",
    "        ),\n",
    "        # RandCropByPosNegLabeld(\n",
    "        #     keys = [\"image\", \"mask\"],\n",
    "        #     label_key = \"mask\",\n",
    "        #     spatial_size = (256, 256, 40), # Output size of the image [height, width, depth]\n",
    "        #     pos = 1, # Ratio of positive labels in the output image\n",
    "        #     neg = 1, # Ratio of negative labels in the output image\n",
    "        #     num_samples = 4, # Number of random crops\n",
    "        #     image_key = \"image\", # Key of the image to be cropped\n",
    "        #     image_threshold = 0# Threshold to determine the foreground of the image\n",
    "        # ),\n",
    "        RandAffined(\n",
    "            keys = [\"image\", \"mask\"],\n",
    "            mode = (\"bilinear\", \"nearest\"),\n",
    "            prob = 1.0,\n",
    "            spatial_size = (256, 256, 40), # Output size of the image [height, width, depth]\n",
    "            rotate_range = (np.pi / 36, np.pi / 36, np.pi / 36), # Rotation range\n",
    "            scale_range = (0.1, 0.1, 0.1), # will do [-0.1, 0.1] scaling then add 1 so a scaling in the range [0.9, 1.1]\n",
    "            padding_mode=\"zeros\", # This means that the image will be padded with zeros, some images are smaller than 256x256x40\n",
    "        ),\n",
    "        RandGaussianNoised(\n",
    "            keys = \"image\",\n",
    "            prob = 0.15,\n",
    "            mean = 0.0,\n",
    "            std = 0.1\n",
    "\n",
    "        ),\n",
    "        RandGaussianSmoothd(\n",
    "            keys = \"image\",\n",
    "            prob = 0.1,\n",
    "            sigma_x=(0.5, 1.5),\n",
    "            sigma_y=(0.5, 1.5),\n",
    "            sigma_z=(0.5, 1.5)\n",
    "        ),\n",
    "        AdjustContrastd(\n",
    "            keys = \"image\",\n",
    "            gamma = 1.3\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create transforms for validation\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys = [\"image\", \"mask\"]),\n",
    "        EnsureChannelFirstd(keys = [\"image\", \"mask\"]),\n",
    "        ScaleIntensityd(keys = \"image\"),\n",
    "        Spacingd(\n",
    "            keys = [\"image\", \"mask\"],\n",
    "            pixdim = [0.75, 0.75, 2.5],\n",
    "            mode = (\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        # since we are not doing data augmentation during validation,\n",
    "        #we simply center crop the image and mask to the specified size of [256, 256, 40]\n",
    "        CenterSpatialCropd(keys = [\"image\", \"mask\"], roi_size = (256, 256, 40)), \n",
    "        SpatialPadd(keys = [\"image\", \"mask\"], spatial_size= (256, 256, 40)) # Some images are smaller than 256x256x40, so we pad them to this size\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Background': 0,\n",
       " 'Bladder': 1,\n",
       " 'Bone': 2,\n",
       " 'Obturator internus': 3,\n",
       " 'Transition zone': 4,\n",
       " 'Central gland': 5,\n",
       " 'Rectum': 6,\n",
       " 'Seminal vesicle': 7,\n",
       " 'Neurovascular bundle': 8}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "organs = [\"Background\", \"Bladder\", \"Bone\", \"Obturator internus\", \"Transition zone\", \"Central gland\",\n",
    "          \"Rectum\", \"Seminal vesicle\", \"Neurovascular bundle\"]\n",
    "# Create an index dictionary\n",
    "organs_dict = {organ: idx for idx, organ in enumerate(organs)}\n",
    "organs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch\n",
    "#from tqdm import tqdm\n",
    "BATCH_SIZE = 2\n",
    "#train_ds = CacheDataset(data = train_files, transform = train_transforms, cache_rate = 1.0, num_workers = 4)\n",
    "train_ds = Dataset(data = train_files, transform = train_transforms)\n",
    "train_dl = DataLoader(dataset = train_ds, batch_size = BATCH_SIZE, shuffle = True, num_workers = 4)\n",
    "\n",
    "#val_ds = CacheDataset(data = val_files, transform = val_transforms, cache_rate = 1.0, num_workers = 4)\n",
    "val_ds = Dataset(data = val_files, transform = val_transforms)\n",
    "val_dl = DataLoader(dataset = val_ds, batch_size = BATCH_SIZE, num_workers = 4, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Input image has eight anatomical structures of planning interest\n",
    "model = MTLResidualAttention3DUnet(in_channels = 1, main_out_channels = 3, aux_out_channels = 4).to(device)#Main: 2 structures + background, Aux: 3 structures + background\n",
    "loss_function = DiceLoss(to_onehot_y = True, softmax = True, include_background=False) # For segmentation Expects BNHW[D] input i.e. batch, channel, height, width, depth, performs softmax on the channel dimension to get a probability distribution\n",
    "optimizer = torch.optim.Adam(model.parameters(), (1e-3)/4) # Decreased the loss after getting a somewhat good model\n",
    "dice_metric_main = DiceMetric(include_background=False, reduction=\"mean\")# Collect the loss and metric values for every iteration\n",
    "dice_metric_aux = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 100, eta_min = 1e-6) #** Adopt a cosine annealing learning rate schedule which reduces the learning rate as the training progresses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 1 / 60\n",
      "\n",
      "Epoch 1 average dice loss for main task: 0.1039\n",
      "\n",
      "Epoch 1 average dice loss for aux task: 0.0856\n",
      "\n",
      "Epoch 1 average total loss for both tasks: 0.2427\n",
      "--------------------\n",
      "Epoch 2 / 60\n"
     ]
    }
   ],
   "source": [
    "# Import AsDiscrete transform to convert the output to discrete values\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from monai.transforms import AsDiscrete\n",
    "from pathlib import Path\n",
    "# Create model directory\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create model save path\n",
    "MODEL_NAME = \"nn_MTL_pytorch_male_pelvic_segmentation_model_3.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "max_epochs = 100\n",
    "val_interval = 10\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "epoch_aux_loss_values = []\n",
    "epoch_total_loss_values = []\n",
    "main_metric_values = []\n",
    "aux_metric_values = []\n",
    "\n",
    "# Post transforms for the main prostate zones: 2 classes + background\n",
    "post_pred_transform_main = Compose([AsDiscrete(argmax = True, to_onehot = 3)])\n",
    "post_label_transform_main = Compose([AsDiscrete(to_onehot = 3)])\n",
    "\n",
    "# Post transforms for the auxilliary prostate zones: 3 classes + background\n",
    "post_pred_transform_aux = Compose([AsDiscrete(argmax = True, to_onehot = 4)])\n",
    "post_label_transform_aux = Compose([AsDiscrete(to_onehot = 4)])\n",
    "\n",
    "# Loss weights\n",
    "main_weight = 1.1\n",
    "aux_weight = 1.5\n",
    "\n",
    "#CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Epoch {epoch + 1} / {max_epochs}\")\n",
    "    # Put the model into training mode\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_aux_loss = 0\n",
    "    epoch_total_loss = 0\n",
    "    step = 0\n",
    "    for batch in train_dl:\n",
    "        step = step + 1\n",
    "        inputs = batch[\"image\"].permute(0, 1, 4, 2, 3).to(device)\n",
    "        labels = batch[\"mask\"].to(device) # Permute beccause of torch upsample\n",
    "\n",
    "        # Modify the main labels to match the output of the main decoder\n",
    "        main_labels = labels.clone()\n",
    "        main_labels[(main_labels != organs_dict['Transition zone']) & (main_labels != organs_dict['Central gland'])] = 0.0\n",
    "        main_labels[main_labels == organs_dict['Transition zone']] = 1.0\n",
    "        main_labels[main_labels == organs_dict['Central gland']] = 2.0\n",
    "\n",
    "        # Modify the auxilliary labels to match the output of the auxilliary decoder\n",
    "        aux_labels = labels.clone()\n",
    "        aux_labels[(aux_labels != organs_dict['Bladder']) & (aux_labels != organs_dict['Rectum']) & (aux_labels != organs_dict['Seminal vesicle'])] = 0.0\n",
    "        aux_labels[aux_labels == organs_dict['Bladder']] = 1.0\n",
    "        aux_labels[aux_labels == organs_dict['Rectum']] = 2.0\n",
    "        aux_labels[aux_labels == organs_dict['Seminal vesicle']] = 3.0\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        # Forward pass\n",
    "        main_seg, aux_seg = model(inputs) \n",
    "        main_seg, aux_seg = main_seg.permute(0, 1, 3, 4, 2), aux_seg.permute(0, 1, 3, 4, 2) # Permute back to BNHWD\n",
    "\n",
    "        # Compute the loss functions\n",
    "        main_seg_loss = loss_function(main_seg, main_labels)\n",
    "        aux_seg_loss = loss_function(aux_seg, aux_labels)\n",
    "\n",
    "        # Compute the total loss\n",
    "        loss = main_weight * main_seg_loss + aux_weight * aux_seg_loss\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Find the gradients of the loss w.r.t the model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add the loss to the epoch loss\n",
    "        epoch_loss = epoch_loss + main_seg_loss.item()\n",
    "        epoch_aux_loss = epoch_aux_loss + aux_seg_loss.item()\n",
    "        epoch_total_loss = epoch_total_loss + loss.item()\n",
    "    # Compute the average loss of the epoch\n",
    "    epoch_loss = epoch_loss / step\n",
    "    epoch_aux_loss = epoch_aux_loss / step\n",
    "    epoch_total_loss = epoch_total_loss / step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    epoch_total_loss_values.append(epoch_total_loss)\n",
    "    epoch_aux_loss_values.append(epoch_aux_loss)\n",
    "\n",
    "\n",
    "    # Print the average loss of the epoch\n",
    "    print(f\"\\nEpoch {epoch + 1} average dice loss for main task: {epoch_loss:.4f}\")\n",
    "    print(f\"\\nEpoch {epoch + 1} average dice loss for aux task: {epoch_aux_loss:.4f}\")\n",
    "    print(f\"\\nEpoch {epoch + 1} average total loss for both tasks: {epoch_total_loss:.4f}\")\n",
    "\n",
    "    # Step the scheduler after every epoch\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print loss and evaluate model when epoch is divisible by val_interval\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        #print(f\"Epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "        # Put the model into evaluation mode\n",
    "        model.eval()\n",
    "        # Disable gradient calculation\n",
    "        with torch.inference_mode():\n",
    "            # Loop through the validation data\n",
    "            for val_data in val_dl:\n",
    "                val_inputs, val_labels = val_data[\"image\"].permute(0, 1, 4, 2, 3).to(device), val_data[\"mask\"].to(device)\n",
    "                # Modify the main labels to match the output of the main decoder\n",
    "                val_main_labels = val_labels.clone()\n",
    "                val_main_labels[(val_main_labels != organs_dict['Transition zone']) & (val_main_labels != organs_dict['Central gland'])] = 0.0\n",
    "                val_main_labels[val_main_labels == organs_dict['Transition zone']] = 1.0\n",
    "                val_main_labels[val_main_labels == organs_dict['Central gland']] = 2.0\n",
    "\n",
    "                # Modify the auxilliary labels to match the output of the auxilliary decoder\n",
    "                val_aux_labels = val_labels.clone()\n",
    "                val_aux_labels[(val_aux_labels != organs_dict['Bladder']) & (val_aux_labels != organs_dict['Rectum']) & (val_aux_labels != organs_dict['Seminal vesicle'])] = 0.0\n",
    "                val_aux_labels[val_aux_labels == organs_dict['Bladder']] = 1.0\n",
    "                val_aux_labels[val_aux_labels == organs_dict['Rectum']] = 2.0\n",
    "                val_aux_labels[val_aux_labels == organs_dict['Seminal vesicle']] = 3.0\n",
    "\n",
    "                # Forward pass\n",
    "                val_main_outputs, val_aux_outputs = model(val_inputs)\n",
    "                val_main_outputs, val_aux_outputs = val_main_outputs.permute(0, 1, 3, 4, 2), val_aux_outputs.permute(0, 1, 3, 4, 2)\n",
    "\n",
    "                # Transform main outputs and labels to calculate inference loss\n",
    "                val_main_outputs = [post_pred_transform_main(i) for i in decollate_batch(val_main_outputs)]\n",
    "                val_main_labels = [post_label_transform_main(i) for i in decollate_batch(val_main_labels)]\n",
    "\n",
    "                # Transform aux outputs and labels to calculate inference loss\n",
    "                val_aux_outputs = [post_pred_transform_aux(i) for i in decollate_batch(val_aux_outputs)]\n",
    "                val_aux_labels = [post_label_transform_aux(i) for i in decollate_batch(val_aux_labels)]\n",
    "\n",
    "\n",
    "                # Compute dice metric for current iteration\n",
    "                dice_metric_main(y_pred = val_main_outputs, y = val_main_labels)\n",
    "                dice_metric_aux(y_pred = val_aux_outputs, y = val_aux_labels)\n",
    "\n",
    "            # Compute the average metric value across all iterations\n",
    "            main_metric = dice_metric_main.aggregate().item()\n",
    "            aux_metric = dice_metric_aux.aggregate().item()\n",
    "            main_metric_values.append(main_metric)\n",
    "            aux_metric_values.append(aux_metric)\n",
    "            \n",
    "            # Reset the metric for next validation run\n",
    "            dice_metric_main.reset()\n",
    "            dice_metric_aux.reset()\n",
    "\n",
    "\n",
    "            # If the metric is better than the best seen so far, save the model\n",
    "            if main_metric > best_metric:\n",
    "                best_metric = main_metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "                print(\"saved new best metric model\")\n",
    "            \n",
    "            print(\n",
    "                f\"\\nCurrent epoch: {epoch + 1} current mean dice for main task: {main_metric:.4f}\"\n",
    "                f\"\\nBest mean dice for main task: {best_metric:.4f} at epoch: {best_metric_epoch}\"\n",
    "                f\"\\nCurrent epoch: {epoch + 1} current mean dice for aux task: {aux_metric:.4f}\"\n",
    "                )\n",
    "            \n",
    "\n",
    "# When training is complete:\n",
    "print(f\"Done training! Best mean dice: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
    "\n",
    "                \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Trained model on half resolution\n",
    "2. Decreased LR and trained a bit more\n",
    "3. Use higher resolution\n",
    "4. Decrease LR and train a bit more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save epoch loss and metric values based on the model name\n",
    "import pickle\n",
    "pref = f\"{MODEL_NAME.split('.')[0]}\"\n",
    "with open(MODEL_PATH/f\"{pref}_epoch_loss_values.pkl\", \"wb\") as f:\n",
    "    pickle.dump(epoch_loss_values, f)\n",
    "with open(MODEL_PATH/f\"{pref}_epoch_aux_loss_values.pkl\", \"wb\") as f:\n",
    "    pickle.dump(epoch_aux_loss_values, f)\n",
    "with open(MODEL_PATH/f\"{pref}_epoch_total_loss_values.pkl\", \"wb\") as f:\n",
    "    pickle.dump(epoch_total_loss_values, f)\n",
    "with open(MODEL_PATH/f\"{pref}_main_metric_values.pkl\", \"wb\") as f:\n",
    "    pickle.dump(main_metric_values, f)\n",
    "with open(MODEL_PATH/f\"{pref}_aux_metric_values.pkl\", \"wb\") as f:\n",
    "    pickle.dump(aux_metric_values, f)\n",
    "\n",
    "# # Open the saved files\n",
    "# with open(MODEL_PATH/\"epoch_loss_values.pkl\", \"rb\") as f:\n",
    "#     epoch_loss_values = pickle.load(f)\n",
    "# with open(MODEL_PATH/\"metric_values.pkl\", \"rb\") as f:\n",
    "#     metric_values = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implement a MTL 3D residual attention U-Net\n",
    "class MTLResidualAttention3DUnet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_groups = 4, n_channels = [16, 32, 64, 128, 256]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the contracting path: residual blocks followed by downsampling\n",
    "        self.down_conv = nn.ModuleList(residual_block(in_chans, out_chans) for in_chans, out_chans in\n",
    "                                       [(in_channels, n_channels[0]), (n_channels[0], n_channels[1]), (n_channels[1], n_channels[2]), (n_channels[2], n_channels[3])])\n",
    "        self.down_samples = nn.ModuleList(down_sample() for _ in range(len(n_channels) - 1))\n",
    "\n",
    "        # Define the bottleneck residual block\n",
    "        self.bottleneck = residual_block(n_channels[3], n_channels[4])\n",
    "\n",
    "        ## ------ Decoder block for the segmentation task ------ ##\n",
    "        # Define the attention blocks\n",
    "        self.attention_blocks = nn.ModuleList(attention_block(skip_channels = residuals_chans, gate_channels = gate_chans) for gate_chans, residuals_chans in\n",
    "                                              [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
    "        \n",
    "\n",
    "        # Define the expanding path: upsample blocks, followed by crop and concatenate, followed by residual blocks\n",
    "        self.upsamples = nn.ModuleList(up_sample(in_chans, out_chans) for in_chans, out_chans in\n",
    "                                       [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
    "        \n",
    "        self.concat = nn.ModuleList(crop_and_concatenate() for _ in range(len(n_channels) - 1))\n",
    "\n",
    "        self.up_conv = nn.ModuleList(residual_block(in_chans, out_chans) for in_chans, out_chans in\n",
    "                                     [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
    "        \n",
    "        \n",
    "        # Final 1X1 convolution layer to produce the output segmentation map:\n",
    "        # The primary purpose of 1x1 convolutions is to transform the channel dimension of the feature map,\n",
    "        # while leaving the spatial dimensions unchanged.\n",
    "        self.final_conv = nn.Conv3d(in_channels = n_channels[0], out_channels = out_channels, kernel_size = 1, padding = 0, bias = False)\n",
    "\n",
    "        ## --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ##\n",
    "        ###** Add second decoder path for image reconstruction **###\n",
    "        self.attention_blocks_recon = nn.ModuleList(attention_block(skip_channels = residuals_chans, gate_channels = gate_chans) for gate_chans, residuals_chans in\n",
    "                                                [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
    "\n",
    "        self.upsamples_recon = nn.ModuleList(up_sample(in_chans, out_chans) for in_chans, out_chans in\n",
    "                                        [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
    "        self.concat_recon = nn.ModuleList(crop_and_concatenate() for _ in range(4))\n",
    "        self.up_conv_recon = nn.ModuleList(residual_block(in_chans, out_chans) for in_chans, out_chans in\n",
    "                                        [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
    "        # Final 1x1 convolution layer to produce the reconstructed image\n",
    "        self.final_conv_recon = nn.Conv3d(in_channels = n_channels[0], out_channels = in_channels, kernel_size = 1, padding = 0, bias = False)\n",
    "\n",
    "    # Pass the input through the residual attention U-Net\n",
    "    def forward(self, x):\n",
    "        # Store the skip connections\n",
    "        skip_connections = []\n",
    "\n",
    "        # Pass the input through the contracting path\n",
    "        for down_conv, down_sample in zip(self.down_conv, self.down_samples):\n",
    "            x = down_conv(x)\n",
    "            skip_connections.append(x)\n",
    "            x = down_sample(x)\n",
    "\n",
    "        # Pass the output of the contracting path through the bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "\n",
    "        # Define segmentation and reconstruction variables\n",
    "        x_seg = x\n",
    "        x_recon = x\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "       # --- Pass the ouput of encoder\n",
    "\n",
    "        # --- Pass the output of the attention blocks through the expanding path of the segmentation path --- #\n",
    "        # Initialize the attention block counter and the skip connection counter\n",
    "        attn_block_count = 0\n",
    "        skip_connections_count = len(skip_connections)\n",
    "        for up_sample, concat, up_conv in zip(self.upsamples, self.concat, self.up_conv):\n",
    "            gated_attn = self.attention_blocks[attn_block_count](skip_connections[skip_connections_count - 1], x_seg)\n",
    "            attn_block_count += 1\n",
    "            skip_connections_count -= 1\n",
    "            x_seg = up_sample(x_seg)\n",
    "            x_seg = concat(x_seg, gated_attn)\n",
    "            x_seg = up_conv(x_seg)\n",
    "\n",
    "        # Pass the output of the expanding path through the final convolution layer\n",
    "        x_seg = self.final_conv(x_seg) # Output segmentation map\n",
    "\n",
    "        # Pass the output of the attention blocks through the expanding path of the reconstruction path\n",
    "        attn_block_count = 0\n",
    "        skip_connections_count = len(skip_connections)\n",
    "        for up_sample, concat, up_conv in zip(self.upsamples_recon, self.concat_recon, self.up_conv_recon):\n",
    "            gated_attn = self.attention_blocks_recon[attn_block_count](skip_connections[skip_connections_count - 1], x_recon)\n",
    "            attn_block_count += 1\n",
    "            skip_connections_count -= 1\n",
    "            x_recon = up_sample(x_recon)\n",
    "            x_recon = concat(x_recon, gated_attn)\n",
    "            x_recon = up_conv(x_recon)\n",
    "\n",
    "        # Pass the output of the expanding path through the final convolution layer\n",
    "        x_recon = self.final_conv_recon(x_recon) # Output reconstructed image\n",
    "\n",
    "        return x_seg, x_recon\n",
    "    \n",
    "# Test whether the MTL model works\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MTLResidualAttention3DUnet(in_channels = 1, out_channels = 9).to(device)\n",
    "x = torch.randn(2, 1, 40, 256, 256).to(device)\n",
    "mask, recon = model(x)\n",
    "mask.shape, recon.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
