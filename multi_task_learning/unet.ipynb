{
  "cells": [
    {
      "cell_type": "raw",
      "id": "bb0c36e2",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"UNets 101\"\n",
        "format: \n",
        "  html:\n",
        "    number-sections: true\n",
        "    toc: true\n",
        "    toc-location: left\n",
        "    toc-depth: 4\n",
        "    code-tools: true\n",
        "    code-fold: true\n",
        "    code-link: true\n",
        "editor: visual\n",
        "execute: \n",
        "  eval: false\n",
        "  warning: false\n",
        "  message: false\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccd6fdf9",
      "metadata": {},
      "source": [
        "## Implementing the original U-Net model\n",
        "\n",
        "This code implements a U-Net model for semantic segmentation from the paper [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597):\n",
        "\n",
        "<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" alt=\"your_alt_text\" width=\"500\" height=\"300\"/>\n",
        "\n",
        "A U-Net consists of an encoder - a series of convolution and pooling layers which reduce the spatial resolution of the input, followed by a decoder - a series of transposed convolution and upsampling layers which increase the spatial resolution of the input. The encoder and decoder are connected by a bottleneck layer which is responsible for reducing the number of channels in the input.\n",
        "\n",
        "The key innovation of U-Net is the addition of skip connections that connect the contracting path to the corresponding layers in the expanding path, allowing the network to recover fine-grained details lost during downsampling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7c17785",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional\n",
        "\n",
        "# Implement the double 3X3 convolution blocks\n",
        "# The original paper did not use padding, but we will use padding to keep the image size the same\n",
        "\n",
        "class double_convolution(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements the double convolution block which consists of two 3X3 convolution layers,\n",
        "    each followed by a ReLU activation function.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels): # Initialize the class\n",
        "        super().__init__() # Initialize the parent class\n",
        "\n",
        "        # First 3X3 convolution layer\n",
        "        self.first_cnn = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 3, padding = 1)\n",
        "        self.act1 = nn.ReLU()\n",
        "\n",
        "        # Second 3X3 convolution layer\n",
        "        self.second_cnn = nn.Conv2d(in_channels = out_channels, out_channels = out_channels, kernel_size = 3, padding = 1)\n",
        "        self.act2 = nn.ReLU()\n",
        "\n",
        "    # Pass the input through the double convolution block\n",
        "    def forward(self, x):\n",
        "        x = self.first_cnn(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.act2(self.second_cnn(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# Implement the Downsample block that occurs after each double convolution block\n",
        "class down_sample(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements the downsample block which consists of a Max Pooling layer with a kernel size of 2.\n",
        "    The Max Pooling layer halves the image size reducing the spatial resolution of the feature maps\n",
        "    while retaining the most important features.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "    \n",
        "    # Pass the input through the downsample block\n",
        "    def forward(self, x):\n",
        "        x = self.max_pool(x)\n",
        "        return x\n",
        "    \n",
        "# Implement the UpSample block that occurs in the decoder part of the network\n",
        "class up_sample(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements the upsample block which consists of a convolution transpose layer with a kernel size of 2.\n",
        "    The convolution transpose layer doubles the image size increasing the spatial resolution of the feature maps\n",
        "    while retaining the learned features.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        # Convolution transpose layer\n",
        "        self.up_sample = nn.ConvTranspose2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 2, stride = 2)\n",
        "\n",
        "    # Pass the input through the upsample block\n",
        "    def forward(self, x):\n",
        "        x = self.up_sample(x)\n",
        "        return x \n",
        "\n",
        "# Implement the crop and concatenate block that occurs in the decoder part of the network\n",
        "# This block concatenates the output of the upsample block with the output of the corresponding downsample block\n",
        "# The output of the crop and concatenate block is then passed through a double convolution block\n",
        "class crop_and_concatenate(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements the crop and concatenate block which combines the output of the upsample block\n",
        "    with the corresponding features from the contracting path through skip connections,\n",
        "    allowing the network to recover the fine-grained details lost during downsampling\n",
        "    and produce a high-resolution output segmentation map.\n",
        "    \"\"\" \n",
        "    # def forward(self, upsampled, bypass):\n",
        "    #     # Crop the feature map from the contacting path to match the size of the upsampled feature map\n",
        "    #     bypass = torchvision.transforms.functional.center_crop(img = bypass, output_size = [upsampled.shape[2], upsampled.shape[3]]) \n",
        "    #     # Concatenate the upsampled feature map with the cropped feature map from the contracting path\n",
        "    #     x = torch.cat([upsampled, bypass], dim = 1) # Concatenate along the channel dimension\n",
        "    #     return x\n",
        "    # Alternatively crop the upsampled feature map to match the size of the feature map from the contracting path\n",
        "    def forward(self, upsampled, bypass):\n",
        "        upsampled = torchvision.transforms.functional.resize(img = upsampled, size = bypass.shape[2:], antialias=True)\n",
        "        x = torch.cat([upsampled, bypass], dim = 1) # Concatenate along the channel dimension\n",
        "        return x\n",
        "\n",
        "# m = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "# input = torch.randn(1, 1024, 28, 28)\n",
        "# m(input).shape \n",
        "\n",
        "# m = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "# xx = torch.randn(1, 1, 143, 143)\n",
        "# m(xx).shape\n",
        "\n",
        "## Implement the UNet architecture\n",
        "class UNet(nn.Module):\n",
        "    # in_channels: number of channels in the input image\n",
        "    # out_channels: number of channels in the output image\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define the contracting path: convolution blocks followed by downsample blocks\n",
        "        self.down_conv = nn.ModuleList(double_convolution(in_chans, out_chans) for in_chans, out_chans in\n",
        "                                       [(in_channels, 64), (64, 128), (128, 256), (256, 512)]) # List of downsample blocks\n",
        "        \n",
        "        self.down_samples = nn.ModuleList(down_sample() for _ in range(4))\n",
        "\n",
        "        # Define the bottleneck layer\n",
        "        self.bottleneck = double_convolution(in_channels = 512, out_channels = 1024)\n",
        "\n",
        "        # Define the expanding path: upsample blocks followed by convolution blocks\n",
        "        self.up_samples = nn.ModuleList(up_sample(in_chans, out_chans) for in_chans, out_chans in\n",
        "                                        [(1024, 512), (512, 256), (256, 128), (128, 64)]) # List of upsample blocks\n",
        "        \n",
        "        self.concat = nn.ModuleList(crop_and_concatenate() for _ in range(4))\n",
        "\n",
        "        self.up_conv = nn.ModuleList(double_convolution(in_chans, out_chans) for in_chans, out_chans in\n",
        "                                        [(1024, 512), (512, 256), (256, 128), (128, 64)]) # List of convolution blocks\n",
        "        \n",
        "        # Final 1X1 convolution layer to produce the output segmentation map:\n",
        "        # The primary purpose of 1x1 convolutions is to transform the channel dimension of the feature map,\n",
        "        # while leaving the spatial dimensions unchanged.\n",
        "        self.final_conv = nn.Conv2d(in_channels = 64, out_channels = out_channels, kernel_size = 1)\n",
        "\n",
        "    # Pass the input through the UNet architecture\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the contacting path\n",
        "        skip_connections = [] # List to store the outputs of the downsample blocks\n",
        "        for down_conv, down_sample in zip(self.down_conv, self.down_samples):\n",
        "            x = down_conv(x)\n",
        "            skip_connections.append(x)\n",
        "            x = down_sample(x)\n",
        "        \n",
        "        # Pass the output of the contacting path through the bottleneck layer\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        # Pass the output of the bottleneck layer through the expanding path\n",
        "        skip_connections = skip_connections[::-1] # Reverse the list of skip connections\n",
        "        for up_sample, concat, up_conv in zip(self.up_samples, self.concat, self.up_conv):\n",
        "            x = up_sample(x)\n",
        "            x = concat(x, skip_connections.pop(0)) # Remove the first element from the list of skip connections\n",
        "            x = up_conv(x)\n",
        "        \n",
        "        # Pass the output of the expanding path through the final convolution layer\n",
        "        x = self.final_conv(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb101896",
      "metadata": {},
      "source": [
        "### Sanity check for the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "840a2a08",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchsummary\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UNet(in_channels = 3, out_channels = 1).to(device)\n",
        "dummy_input = torch.randn((1, 3, 32, 32)).to(device)\n",
        "mask = model(dummy_input)\n",
        "mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92d261e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# See how data flows through the network\n",
        "torchsummary.summary(model, input_size = (3, 32, 32))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3a799f43",
      "metadata": {},
      "source": [
        "## Going deeper: Some more interesting UNets\n",
        "\n",
        "\n",
        "This section implemets a U-Net model that incorporates some of the recent advances in deep learning, that is:\n",
        "\n",
        "\n",
        "\n",
        "- [Residual networks for UNets](https://arxiv.org/abs/1802.06955): The key idea behind ResNets is the use of residual connections, which allow for the direct propagation of information through the network without being modified by the layers in between. The residual connection is achieved by adding the input of a layer to its output, so that the output of the layer becomes: `y = f(x) + x`. **The shortcut connection skips one or more layers, with the change in dimensions, if any, compensated with a 1x1 convolutional layer.**\n",
        "\n",
        "::: {style=\"text-align:center\"}\n",
        "<img src=\"rnns.PNG\" width=\"50%\" height=\"50%\"/>\n",
        ":::\n",
        "\n",
        "-   [Group normalization](https://arxiv.org/abs/1803.08494): works by normalizing the activations of a layer across groups of channels instead of the entire batch. See more explanations and comparisons between different normalizations in [this blog post](https://gaoxiangluo.github.io/2021/08/01/Group-Norm-Batch-Norm-Instance-Norm-which-is-better/).\n",
        "\n",
        "::: {style=\"text-align:center\"}\n",
        "<img src=\"normalizations.PNG\" width=\"50%\" height=\"50%\"/>\n",
        ":::\n",
        "\n",
        "-   [Swish activation function](https://arxiv.org/abs/1710.05941): is a self-gated activation function that is defined as `f(x) = x * sigmoid(x)`. It has been shown to outperform ReLU and other activation functions on deeper models across a number of challenging datasets.\n",
        "\n",
        "::: {style=\"text-align:center\"}\n",
        "<img src=\"swish.PNG\" width=\"40%\" height=\"40%\"/>\n",
        ":::\n",
        "\n",
        "-   [Attention gated Unets](https://arxiv.org/abs/1804.03999): is a modification of the U-Net architecture that uses attention gates to selectively focus on the most relevant parts of the input image. The attention gates are implemented as a 1x1 convolutional layer that learns a weight for each channel in the input. The output of the attention gate is then multiplied with the input to the layer, so that the output of the layer becomes: `y = f(x) * sigmoid(x)`. The attention gates are applied to the output of the contracting path and the input of the expanding path.\n",
        "\n",
        "::: {style=\"text-align:center\"}\n",
        "<img src=\"attention_unet.png\" alt=\"your_alt_text\" width=\"50%\" height=\"50%\"/>\n",
        ":::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9a4b2a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "# Define a Residual block\n",
        "class residual_block(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements a residual block which consists of two convolution layers with group normalization\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, n_groups = 8):\n",
        "        super().__init__()\n",
        "        # First convolution layer\n",
        "        self.first_conv = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 3, padding = 1)\n",
        "        self.first_norm = nn.GroupNorm(num_groups = n_groups, num_channels = out_channels)\n",
        "        self.act1 = nn.SiLU() # Swish activation function\n",
        "\n",
        "        # Second convolution layer\n",
        "        self.second_conv = nn.Conv2d(in_channels = out_channels, out_channels = out_channels, kernel_size = 3, padding = 1)\n",
        "        self.second_norm = nn.GroupNorm(num_groups = n_groups, num_channels = out_channels)\n",
        "        self.act2 = nn.SiLU() # Swish activation function\n",
        "\n",
        "        # If the number of input channels is not equal to the number of output channels,\n",
        "        # then use a 1X1 convolution layer to compensate for the difference in dimensions\n",
        "        # This allows the input to have the same dimensions as the output of the residual block\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 1)\n",
        "        else:\n",
        "            # Pass the input as is\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    # Pass the input through the residual block\n",
        "    def forward(self, x):\n",
        "        # Store the input\n",
        "        input = x\n",
        "\n",
        "        # Pass input through the first convolution layer\n",
        "        x = self.act1(self.second_norm(self.first_conv(x)))\n",
        "\n",
        "        # Pass the output of the first convolution layer through the second convolution layer\n",
        "        x = self.act2(self.second_norm(self.second_conv(x)))\n",
        "\n",
        "        # Add the input to the output of the second convolution layer\n",
        "        # This is the skip connection\n",
        "        x = x + self.shortcut(input)\n",
        "        return x\n",
        "\n",
        "# Implement the DownSample block that occurs after each residual block\n",
        "class down_sample(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "    # Pass the input through the downsample block\n",
        "    def forward(self, x):\n",
        "        x = self.max_pool(x)\n",
        "        return x\n",
        "\n",
        "# Implement the UpSample block that occurs in the decoder path/expanding path\n",
        "class up_sample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        # Convolution transpose layer to upsample the input\n",
        "        self.up_sample = nn.ConvTranspose2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 2, stride = 2)\n",
        "\n",
        "    # Pass the input through the upsample block\n",
        "    def forward(self, x):\n",
        "        x = self.up_sample(x)\n",
        "        return x\n",
        "\n",
        "# Implement the crop and concatenate layer\n",
        "class crop_and_concatenate(nn.Module):\n",
        "    def forward(self, upsampled, bypass):\n",
        "        # Crop the upsampled feature map to match the dimensions of the bypass feature map\n",
        "        upsampled = torchvision.transforms.functional.resize(upsampled, size = bypass.shape[2:], antialias=True)\n",
        "        x = torch.cat([upsampled, bypass], dim = 1) # Concatenate along the channel dimension\n",
        "        return x\n",
        "\n",
        "# Implement an attention block\n",
        "class attention_block(nn.Module):\n",
        "    def __init__(self, skip_channels, gate_channels, inter_channels = None, n_groups = 8):\n",
        "        super().__init__()\n",
        "\n",
        "        if inter_channels is None:\n",
        "            inter_channels = skip_channels // 2\n",
        "\n",
        "        # Implement W_g i.e the convolution layer that operates on the gate signal\n",
        "        # Upsample gate signal to be the same size as the skip connection\n",
        "        self.W_g = up_sample(in_channels = gate_channels, out_channels = skip_channels)\n",
        "        #self.W_g_norm = nn.GroupNorm(num_groups = n_groups, num_channels = skip_channels)\n",
        "        #self.W_g_act = nn.SiLU() # Swish activation function\n",
        "\n",
        "        # Implement W_x i.e the convolution layer that operates on the skip connection\n",
        "        self.W_x = nn.Conv2d(in_channels = skip_channels, out_channels = inter_channels, kernel_size = 1)\n",
        "        #self.W_x_norm = nn.GroupNorm(num_groups = n_groups, num_channels = inter_channels)\n",
        "        #self.W_x_act = nn.SiLU() # Swish activation function\n",
        "\n",
        "        # Implement phi i.e the convolution layer that operates on the output of W_x + W_g\n",
        "        self.phi = nn.Conv2d(in_channels = inter_channels, out_channels = 1, kernel_size = 1)\n",
        "        #self.phi_norm = nn.GroupNorm(num_groups = n_groups, num_channels = 1)\n",
        "        #self.phi_act = nn.SiLU() # Swish activation function\n",
        "\n",
        "        # Implement the sigmoid activation function\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        # Implement the Swish activation function\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        # Implement final group normalization layer\n",
        "        self.final_norm = nn.GroupNorm(num_groups = n_groups, num_channels = skip_channels)\n",
        "\n",
        "    # Pass the input through the attention block\n",
        "    def forward(self, skip_connection, gate_signal):\n",
        "        # Upsample the gate signal to match the channels of the skip connection\n",
        "        gate_signal = self.W_g(gate_signal)\n",
        "        # Ensure that the sizes of the skip connection and the gate signal match before addition\n",
        "        if gate_signal.shape[2:] != skip_connection.shape[2:]:\n",
        "            gate_signal = torchvision.transforms.functional.resize(gate_signal, size = skip_connection.shape[2:], antialias=True)\n",
        "        # Project to the intermediate channels\n",
        "        gate_signal = self.W_x(gate_signal)\n",
        "\n",
        "        # Project the skip connection to the intermediate channels\n",
        "        skip_signal = self.W_x(skip_connection)\n",
        "\n",
        "        # Add the skip connection and the gate signal\n",
        "        add_xg = gate_signal + skip_signal\n",
        "\n",
        "        # Pass the output of the addition through the activation function\n",
        "        add_xg = self.act(add_xg)\n",
        "\n",
        "        # Pass the output of attention through a 1x1 convolution layer to obtain the attention map\n",
        "        attention_map = self.sigmoid(self.phi(add_xg))\n",
        "\n",
        "        # Multiply the skip connection with the attention map\n",
        "        # Perform element-wise multiplication\n",
        "        skip_connection = torch.mul(skip_connection, attention_map)\n",
        "\n",
        "        skip_connection = nn.Conv2d(in_channels = skip_connection.shape[1], out_channels = skip_connection.shape[1], kernel_size = 1)(skip_connection)\n",
        "        skip_connection = self.act(self.final_norm(skip_connection))\n",
        "\n",
        "        return skip_connection\n",
        "\n",
        "\n",
        "## Implement a residual attention U-Net\n",
        "class ResidualAttentionUnet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, n_groups = 4, n_channels = [64, 128, 256, 512, 1024]):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define the contracting path: residual blocks followed by downsampling\n",
        "        self.down_conv = nn.ModuleList(residual_block(in_chans, out_chans) for in_chans, out_chans in\n",
        "                                       [(in_channels, n_channels[0]), (n_channels[0], n_channels[1]), (n_channels[1], n_channels[2]), (n_channels[2], n_channels[3])])\n",
        "        self.down_samples = nn.ModuleList(down_sample() for _ in range(4))\n",
        "\n",
        "        # Define the bottleneck residual block\n",
        "        self.bottleneck = residual_block(n_channels[3], n_channels[4])\n",
        "\n",
        "\n",
        "        # Define the attention blocks\n",
        "        self.attention_blocks = nn.ModuleList(attention_block(skip_channels = residuals_chans, gate_channels = gate_chans) for gate_chans, residuals_chans in\n",
        "                                              [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
        "\n",
        "\n",
        "        # Define the expanding path: upsample blocks, followed by crop and concatenate, followed by residual blocks\n",
        "        self.upsamples = nn.ModuleList(up_sample(in_chans, out_chans) for in_chans, out_chans in\n",
        "                                       [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
        "        \n",
        "        self.concat = nn.ModuleList(crop_and_concatenate() for _ in range(4))\n",
        "\n",
        "        self.up_conv = nn.ModuleList(residual_block(in_chans, out_chans) for in_chans, out_chans in\n",
        "                                     [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
        "        \n",
        "        # Final 1X1 convolution layer to produce the output segmentation map:\n",
        "        # The primary purpose of 1x1 convolutions is to transform the channel dimension of the feature map,\n",
        "        # while leaving the spatial dimensions unchanged.\n",
        "        self.final_conv = nn.Conv2d(in_channels = n_channels[0] , out_channels = out_channels, kernel_size = 1)\n",
        "\n",
        "    # Pass the input through the residual attention U-Net\n",
        "    def forward(self, x):\n",
        "        # Store the skip connections\n",
        "        skip_connections = []\n",
        "        # # Store the gate signals\n",
        "        # gate_signals = []\n",
        "\n",
        "        # Pass the input through the contracting path\n",
        "        for down_conv, down_sample in zip(self.down_conv, self.down_samples):\n",
        "            x = down_conv(x)\n",
        "            skip_connections.append(x)\n",
        "            #gate_signals.append(x)\n",
        "            x = down_sample(x)\n",
        "        \n",
        "        # Pass the output of the contracting path through the bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections.append(x)\n",
        "\n",
        "        # Attention on the residual connections\n",
        "        #skip_connections = skip_connections[::-1]\n",
        "        n = len(skip_connections)\n",
        "        indices = [(n - 1 - i, n - 2 - i) for i in range(n - 1)]\n",
        "        attentions = []\n",
        "        for i, g_x in enumerate(indices):\n",
        "            g_gate = g_x[0]\n",
        "            x_residual = g_x[1]\n",
        "            attn = self.attention_blocks[i](skip_connections[x_residual], skip_connections[g_gate])\n",
        "            attentions.append(attn)\n",
        "\n",
        "        #attentions = attentions[::-1]\n",
        "    \n",
        "        # Pass the output of the attention blocks through the expanding path\n",
        "        for up_sample, concat, up_conv in zip(self.upsamples, self.concat, self.up_conv):\n",
        "            x = up_sample(x)\n",
        "            x = concat(x, attentions.pop(0))\n",
        "            x = up_conv(x)\n",
        "\n",
        "        # Pass the output of the expanding path through the final convolution layer\n",
        "        x = self.final_conv(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "203dba96",
      "metadata": {},
      "source": [
        "### Sanity check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f861128",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Sanity check\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResidualAttentionUnet(in_channels = 3, out_channels = 1).to(device)\n",
        "x = torch.randn((1, 3, 32, 32)).to(device)\n",
        "mask = model(x)\n",
        "mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b103049",
      "metadata": {},
      "outputs": [],
      "source": [
        "# See how data flows through the network\n",
        "torchsummary.summary(model, input_size = (3, 32, 32))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "752253be",
      "metadata": {},
      "source": [
        "## Prepare the data\n",
        "\n",
        "In this section, we prepare the data for training and evaluation to match:\n",
        "\n",
        "- train images + segmentation masks\n",
        "- validation images + segmentation masks\n",
        "- test images + segmentation masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e4377b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# import random\n",
        "# from shutil import copyfile\n",
        "\n",
        "# # Set paths to original data and where to save the split data\n",
        "# data_dir = \"/path/to/data/\"\n",
        "# train_dir = \"/path/to/train/\"\n",
        "# val_dir = \"/path/to/val/\"\n",
        "# test_dir = \"/path/to/test/\"\n",
        "\n",
        "# # Set the percentage split\n",
        "# train_percent = 0.7\n",
        "# val_percent = 0.15\n",
        "# test_percent = 0.15\n",
        "\n",
        "# # Get the list of image files\n",
        "# image_files = os.listdir(os.path.join(data_dir, \"images\"))\n",
        "# num_images = len(image_files)\n",
        "\n",
        "# # Shuffle the image files\n",
        "# random.shuffle(image_files)\n",
        "\n",
        "# # Split the dataset\n",
        "# train_split = int(train_percent * num_images)\n",
        "# val_split = int((train_percent + val_percent) * num_images)\n",
        "\n",
        "# train_images = image_files[:train_split]\n",
        "# val_images = image_files[train_split:val_split]\n",
        "# test_images = image_files[val_split:]\n",
        "\n",
        "# # Copy the images and their corresponding segmentation masks to their respective directories\n",
        "# for split, images in zip([train_dir, val_dir, test_dir], [train_images, val_images, test_images]):\n",
        "#     for image in images:\n",
        "#         # Copy image\n",
        "#         copyfile(os.path.join(data_dir, \"images\", image), os.path.join(split, \"images\", image))\n",
        "        \n",
        "#         # Get corresponding segmentation mask\n",
        "#         mask = image.replace(\".png\", \"_mask.png\")\n",
        "        \n",
        "#         # Copy segmentation mask\n",
        "#         copyfile(os.path.join(data_dir, \"masks\", mask), os.path.join(split, \"masks\", mask))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "3098c3bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import random\n",
        "from shutil import copyfile\n",
        "import os\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "random.seed(2056)\n",
        "\n",
        "# Read all files ending with _img.nii\n",
        "img_path = Path(\"data/data\")\n",
        "img_files = list(img_path.glob(\"*_img.nii\")) # Image and mask are in the same folder\n",
        "num_images = len(img_files) \n",
        "\n",
        "# Create train, validation and test splits\n",
        "train_split = int(0.7 * num_images)\n",
        "val_split = int(0.15 * num_images)\n",
        "test_split = int(num_images - (train_split + val_split))\n",
        "\n",
        "# Shuffle the image files\n",
        "random.shuffle(img_files)\n",
        "\n",
        "# Split the dataset\n",
        "train_images = img_files[:train_split]\n",
        "val_images = img_files[train_split:(train_split + val_split)]\n",
        "test_images = img_files[(train_split + val_split): ]\n",
        "\n",
        "# Create train, validation and test directories\n",
        "train_image_dir = Path(img_path / \"train_images\")\n",
        "train_mask_dir = Path(img_path / \"train_masks\")\n",
        "val_image_dir = Path(img_path / \"val_images\")\n",
        "val_mask_dir = Path(img_path / \"val_masks\")\n",
        "test_image_dir = Path(img_path / \"test_images\")\n",
        "test_mask_dir = Path(img_path / \"test_masks\")\n",
        "\n",
        "# Create the directories\n",
        "for directory in [train_image_dir, train_mask_dir, val_image_dir, val_mask_dir, test_image_dir, test_mask_dir]:\n",
        "    directory.mkdir(exist_ok = True, parents = True)\n",
        "\n",
        "# Copy the images and their corresponding segmentation masks to their respective directories\n",
        "for directory, images in zip([train_image_dir, val_image_dir, test_image_dir], [train_images, val_images, test_images]):\n",
        "    for image in images:\n",
        "        # Copy image\n",
        "        copyfile(image, directory / image.name)\n",
        "\n",
        "        # Get corresponding segmentation mask\n",
        "        mask = image.name.replace(\"_img.nii\", \"_mask.nii\")\n",
        "\n",
        "        # Copy segmentation mask\n",
        "        copyfile(image.parent / mask, image.parent / directory.name.replace(\"images\", \"masks\") / mask)\n",
        "\n",
        "\n",
        "# # Sanity check\n",
        "# train_images_s = list(train_image_dir.glob(\"*\"))\n",
        "# train_images_s = [image.name.removesuffix(\"_img.nii\") for image in train_images_s]\n",
        "# train_masks_s = list(train_mask_dir.glob(\"*\"))\n",
        "# train_masks_s = [mask.name.removesuffix(\"_mask.nii\") for mask in train_masks_s]\n",
        "# train_images_s == train_masks_s\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
