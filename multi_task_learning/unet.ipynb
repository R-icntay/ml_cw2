{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a U-Net model\n",
    "\n",
    "This code implements a U-Net model for semantic segmentation from the paper [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597).\n",
    "\n",
    "A U-Net consists of an encoder - a series of convolution and pooling layers  which reduce the spatial resolution of the input, followed by a decoder - a series of transposed convolution and upsampling layers which increase the spatial resolution of the input. The encoder and decoder are connected by a bottleneck layer which is responsible for reducing the number of channels in the input.\n",
    "\n",
    "The key innovation of U-Net is the addition of skip connections that connect the contracting path to the corresponding layers in the expanding path, allowing the network to recover fine-grained details lost during downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional\n",
    "\n",
    "# Implement the double 3X3 convolution blocks\n",
    "# The original paper did not use padding, but we will use padding to keep the image size the same\n",
    "\n",
    "class double_convolution(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the double convolution block which consists of two 3X3 convolution layers,\n",
    "    each followed by a ReLU activation function.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels): # Initialize the class\n",
    "        super().__init__() # Initialize the parent class\n",
    "\n",
    "        # First 3X3 convolution layer\n",
    "        self.first_cnn = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 3, padding = 1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        # Second 3X3 convolution layer\n",
    "        self.second_cnn = nn.Conv2d(in_channels = out_channels, out_channels = out_channels, kernel_size = 3, padding = 1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "    # Pass the input through the double convolution block\n",
    "    def forward(self, x):\n",
    "        x = self.first_cnn(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.act2(self.second_cnn(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Implement the Downsample block that occurs after each double convolution block\n",
    "class down_sample(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the downsample block which consists of a Max Pooling layer with a kernel size of 2.\n",
    "    The Max Pooling layer halves the image size reducing the spatial resolution of the feature maps\n",
    "    while retaining the most important features.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "    \n",
    "    # Pass the input through the downsample block\n",
    "    def forward(self, x):\n",
    "        x = self.max_pool(x)\n",
    "        return x\n",
    "    \n",
    "# Implement the UpSample block that occurs in the decoder part of the network\n",
    "class up_sample(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the upsample block which consists of a convolution transpose layer with a kernel size of 2.\n",
    "    The convolution transpose layer doubles the image size increasing the spatial resolution of the feature maps\n",
    "    while retaining the learned features.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolution transpose layer\n",
    "        self.up_sample = nn.ConvTranspose2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 2, stride = 2)\n",
    "\n",
    "    # Pass the input through the upsample block\n",
    "    def forward(self, x):\n",
    "        x = self.up_sample(x)\n",
    "        return x \n",
    "\n",
    "# Implement the crop and concatenate block that occurs in the decoder part of the network\n",
    "# This block concatenates the output of the upsample block with the output of the corresponding downsample block\n",
    "# The output of the crop and concatenate block is then passed through a double convolution block\n",
    "class crop_and_concatenate(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the crop and concatenate block which combines the output of the upsample block\n",
    "    with the corresponding features from the contracting path through skip connections,\n",
    "    allowing the network to recover the fine-grained details lost during downsampling\n",
    "    and produce a high-resolution output segmentation map.\n",
    "    \"\"\" \n",
    "    # def forward(self, upsampled, bypass):\n",
    "    #     # Crop the feature map from the contacting path to match the size of the upsampled feature map\n",
    "    #     bypass = torchvision.transforms.functional.center_crop(img = bypass, output_size = [upsampled.shape[2], upsampled.shape[3]]) \n",
    "    #     # Concatenate the upsampled feature map with the cropped feature map from the contracting path\n",
    "    #     x = torch.cat([upsampled, bypass], dim = 1) # Concatenate along the channel dimension\n",
    "    #     return x\n",
    "    # Alternatively crop the upsampled feature map to match the size of the feature map from the contracting path\n",
    "    def forward(self, upsampled, bypass):\n",
    "        upsampled = torchvision.transforms.functional.resize(img = upsampled, size = bypass.shape[2:], antialias=True)\n",
    "        x = torch.cat([upsampled, bypass], dim = 1) # Concatenate along the channel dimension\n",
    "        return x\n",
    "\n",
    "# m = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "# input = torch.randn(1, 1024, 28, 28)\n",
    "# m(input).shape \n",
    "\n",
    "# m = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "# xx = torch.randn(1, 1, 143, 143)\n",
    "# m(xx).shape\n",
    "\n",
    "## Implement the UNet architecture\n",
    "class UNet(nn.Module):\n",
    "    # in_channels: number of channels in the input image\n",
    "    # out_channels: number of channels in the output image\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the contracting path: convolution blocks followed by downsample blocks\n",
    "        self.down_conv = nn.ModuleList(double_convolution(in_chans, out_chans) for in_chans, out_chans in\n",
    "                                       [(in_channels, 64), (64, 128), (128, 256), (256, 512)]) # List of downsample blocks\n",
    "        \n",
    "        self.down_samples = nn.ModuleList(down_sample() for _ in range(4))\n",
    "\n",
    "        # Define the bottleneck layer\n",
    "        self.bottleneck = double_convolution(in_channels = 512, out_channels = 1024)\n",
    "\n",
    "        # Define the expanding path: upsample blocks followed by convolution blocks\n",
    "        self.up_samples = nn.ModuleList(up_sample(in_chans, out_chans) for in_chans, out_chans in\n",
    "                                        [(1024, 512), (512, 256), (256, 128), (128, 64)]) # List of upsample blocks\n",
    "        \n",
    "        self.concat = nn.ModuleList(crop_and_concatenate() for _ in range(4))\n",
    "\n",
    "        self.up_conv = nn.ModuleList(double_convolution(in_chans, out_chans) for in_chans, out_chans in\n",
    "                                        [(1024, 512), (512, 256), (256, 128), (128, 64)]) # List of convolution blocks\n",
    "        \n",
    "        # Final 1X1 convolution layer to produce the output segmentation map:\n",
    "        # The primary purpose of 1x1 convolutions is to transform the channel dimension of the feature map,\n",
    "        # while leaving the spatial dimensions unchanged.\n",
    "        self.final_conv = nn.Conv2d(in_channels = 64, out_channels = out_channels, kernel_size = 1)\n",
    "\n",
    "    # Pass the input through the UNet architecture\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the contacting path\n",
    "        skip_connections = [] # List to store the outputs of the downsample blocks\n",
    "        for down_conv, down_sample in zip(self.down_conv, self.down_samples):\n",
    "            x = down_conv(x)\n",
    "            skip_connections.append(x)\n",
    "            x = down_sample(x)\n",
    "        \n",
    "        # Pass the output of the contacting path through the bottleneck layer\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        # Pass the output of the bottleneck layer through the expanding path\n",
    "        skip_connections = skip_connections[::-1] # Reverse the list of skip connections\n",
    "        for up_sample, concat, up_conv in zip(self.up_samples, self.concat, self.up_conv):\n",
    "            x = up_sample(x)\n",
    "            x = concat(x, skip_connections.pop(0)) # Remove the first element from the list of skip connections\n",
    "            x = up_conv(x)\n",
    "        \n",
    "        # Pass the output of the expanding path through the final convolution layer\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 572, 572]           1,792\n",
      "              ReLU-2         [-1, 64, 572, 572]               0\n",
      "            Conv2d-3         [-1, 64, 572, 572]          36,928\n",
      "              ReLU-4         [-1, 64, 572, 572]               0\n",
      "double_convolution-5         [-1, 64, 572, 572]               0\n",
      "         MaxPool2d-6         [-1, 64, 286, 286]               0\n",
      "       down_sample-7         [-1, 64, 286, 286]               0\n",
      "            Conv2d-8        [-1, 128, 286, 286]          73,856\n",
      "              ReLU-9        [-1, 128, 286, 286]               0\n",
      "           Conv2d-10        [-1, 128, 286, 286]         147,584\n",
      "             ReLU-11        [-1, 128, 286, 286]               0\n",
      "double_convolution-12        [-1, 128, 286, 286]               0\n",
      "        MaxPool2d-13        [-1, 128, 143, 143]               0\n",
      "      down_sample-14        [-1, 128, 143, 143]               0\n",
      "           Conv2d-15        [-1, 256, 143, 143]         295,168\n",
      "             ReLU-16        [-1, 256, 143, 143]               0\n",
      "           Conv2d-17        [-1, 256, 143, 143]         590,080\n",
      "             ReLU-18        [-1, 256, 143, 143]               0\n",
      "double_convolution-19        [-1, 256, 143, 143]               0\n",
      "        MaxPool2d-20          [-1, 256, 71, 71]               0\n",
      "      down_sample-21          [-1, 256, 71, 71]               0\n",
      "           Conv2d-22          [-1, 512, 71, 71]       1,180,160\n",
      "             ReLU-23          [-1, 512, 71, 71]               0\n",
      "           Conv2d-24          [-1, 512, 71, 71]       2,359,808\n",
      "             ReLU-25          [-1, 512, 71, 71]               0\n",
      "double_convolution-26          [-1, 512, 71, 71]               0\n",
      "        MaxPool2d-27          [-1, 512, 35, 35]               0\n",
      "      down_sample-28          [-1, 512, 35, 35]               0\n",
      "           Conv2d-29         [-1, 1024, 35, 35]       4,719,616\n",
      "             ReLU-30         [-1, 1024, 35, 35]               0\n",
      "           Conv2d-31         [-1, 1024, 35, 35]       9,438,208\n",
      "             ReLU-32         [-1, 1024, 35, 35]               0\n",
      "double_convolution-33         [-1, 1024, 35, 35]               0\n",
      "  ConvTranspose2d-34          [-1, 512, 70, 70]       2,097,664\n",
      "        up_sample-35          [-1, 512, 70, 70]               0\n",
      "crop_and_concatenate-36         [-1, 1024, 71, 71]               0\n",
      "           Conv2d-37          [-1, 512, 71, 71]       4,719,104\n",
      "             ReLU-38          [-1, 512, 71, 71]               0\n",
      "           Conv2d-39          [-1, 512, 71, 71]       2,359,808\n",
      "             ReLU-40          [-1, 512, 71, 71]               0\n",
      "double_convolution-41          [-1, 512, 71, 71]               0\n",
      "  ConvTranspose2d-42        [-1, 256, 142, 142]         524,544\n",
      "        up_sample-43        [-1, 256, 142, 142]               0\n",
      "crop_and_concatenate-44        [-1, 512, 143, 143]               0\n",
      "           Conv2d-45        [-1, 256, 143, 143]       1,179,904\n",
      "             ReLU-46        [-1, 256, 143, 143]               0\n",
      "           Conv2d-47        [-1, 256, 143, 143]         590,080\n",
      "             ReLU-48        [-1, 256, 143, 143]               0\n",
      "double_convolution-49        [-1, 256, 143, 143]               0\n",
      "  ConvTranspose2d-50        [-1, 128, 286, 286]         131,200\n",
      "        up_sample-51        [-1, 128, 286, 286]               0\n",
      "crop_and_concatenate-52        [-1, 256, 286, 286]               0\n",
      "           Conv2d-53        [-1, 128, 286, 286]         295,040\n",
      "             ReLU-54        [-1, 128, 286, 286]               0\n",
      "           Conv2d-55        [-1, 128, 286, 286]         147,584\n",
      "             ReLU-56        [-1, 128, 286, 286]               0\n",
      "double_convolution-57        [-1, 128, 286, 286]               0\n",
      "  ConvTranspose2d-58         [-1, 64, 572, 572]          32,832\n",
      "        up_sample-59         [-1, 64, 572, 572]               0\n",
      "crop_and_concatenate-60        [-1, 128, 572, 572]               0\n",
      "           Conv2d-61         [-1, 64, 572, 572]          73,792\n",
      "             ReLU-62         [-1, 64, 572, 572]               0\n",
      "           Conv2d-63         [-1, 64, 572, 572]          36,928\n",
      "             ReLU-64         [-1, 64, 572, 572]               0\n",
      "double_convolution-65         [-1, 64, 572, 572]               0\n",
      "           Conv2d-66          [-1, 1, 572, 572]              65\n",
      "================================================================\n",
      "Total params: 31,031,745\n",
      "Trainable params: 31,031,745\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.74\n",
      "Forward/backward pass size (MB): 4386.96\n",
      "Params size (MB): 118.38\n",
      "Estimated Total Size (MB): 4509.08\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torchsummary\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(in_channels = 3, out_channels = 1).to(device)\n",
    "dummy_input = torch.randn((1, 3, 572, 572)).to(device)\n",
    "mask = model(dummy_input)\n",
    "mask.shape\n",
    "torchsummary.summary(model, input_size = (3, 572, 572))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a UNet model with skip connections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section implemets a U-Net model that incorporates some of the recent advances in deep learning, that is:\n",
    "- [Residual networks](https://arxiv.org/abs/1512.03385): The key idea behind ResNets is the use of residual connections, which allow for the direct propagation of information through the network without being modified by the layers in between. The residual connection is achieved by adding the input of a layer to its output, so that the output of the layer becomes: `y = f(x) + x`. **The shortcut connection skips one or more layers, with the change in dimensions, if any, compensated with a 1x1 convolutional layer.** \n",
    "\n",
    "- [Group normalization](https://arxiv.org/abs/1803.08494): works by normalizing the activations of a layer across groups of channels instead of the entire batch. See more explanations and comparisons between different normalizations in [this blog post](https://gaoxiangluo.github.io/2021/08/01/Group-Norm-Batch-Norm-Instance-Norm-which-is-better/).\n",
    "\n",
    "- [Swish activation function](https://arxiv.org/abs/1710.05941): is a self-gated activation function that is defined as `f(x) = x * sigmoid(x)`. It has been shown to outperform ReLU and other activation functions on deeper models across a number of challenging datasets.\n",
    "\n",
    "- [Attention gated Unets](https://arxiv.org/abs/1804.03999): is a modification of the U-Net architecture that uses attention gates to selectively focus on the most relevant parts of the input image. The attention gates are implemented as a 1x1 convolutional layer that learns a weight for each channel in the input. The output of the attention gate is then multiplied with the input to the layer, so that the output of the layer becomes: `y = f(x) * sigmoid(x)`. The attention gates are applied to the output of the contracting path and the input of the expanding path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "# Define a Residual block\n",
    "class residual_block(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements a residual block which consists of two convolution layers with group normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, n_groups = 8):\n",
    "        super().__init__()\n",
    "        # First convolution layer\n",
    "        self.first_conv = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 3, padding = 1)\n",
    "        self.first_norm = nn.GroupNorm(num_groups = n_groups, num_channels = out_channels)\n",
    "        self.act1 = nn.SiLU() # Swish activation function\n",
    "\n",
    "        # Second convolution layer\n",
    "        self.second_conv = nn.Conv2d(in_channels = out_channels, out_channels = out_channels, kernel_size = 3, padding = 1)\n",
    "        self.second_norm = nn.GroupNorm(num_groups = n_groups, num_channels = out_channels)\n",
    "        self.act2 = nn.SiLU() # Swish activation function\n",
    "\n",
    "        # If the number of input channels is not equal to the number of output channels,\n",
    "        # then use a 1X1 convolution layer to compensate for the difference in dimensions\n",
    "        # This allows the input to have the same dimensions as the output of the residual block\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 1)\n",
    "        else:\n",
    "            # Pass the input as is\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    # Pass the input through the residual block\n",
    "    def forward(self, x):\n",
    "        # Store the input\n",
    "        input = x\n",
    "\n",
    "        # Pass input through the first convolution layer\n",
    "        x = self.act1(self.second_norm(self.first_conv(x)))\n",
    "\n",
    "        # Pass the output of the first convolution layer through the second convolution layer\n",
    "        x = self.act2(self.second_norm(self.second_conv(x)))\n",
    "\n",
    "        # Add the input to the output of the second convolution layer\n",
    "        # This is the skip connection\n",
    "        x = x + self.shortcut(input)\n",
    "        return x\n",
    "\n",
    "# Implement the DownSample block that occurs after each residual block\n",
    "class down_sample(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\n",
    "    # Pass the input through the downsample block\n",
    "    def forward(self, x):\n",
    "        x = self.max_pool(x)\n",
    "        return x\n",
    "\n",
    "# Implement the UpSample block that occurs in the decoder path/expanding path\n",
    "class up_sample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolution transpose layer to upsample the input\n",
    "        self.up_sample = nn.ConvTranspose2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 2, stride = 2)\n",
    "\n",
    "    # Pass the input through the upsample block\n",
    "    def forward(self, x):\n",
    "        x = self.up_sample(x)\n",
    "        return x\n",
    "\n",
    "# Implement the crop and concatenate layer\n",
    "class crop_and_concatenate(nn.Module):\n",
    "    def forward(self, upsampled, bypass):\n",
    "        # Crop the upsampled feature map to match the dimensions of the bypass feature map\n",
    "        upsampled = torchvision.transforms.functional.resize(upsampled, size = bypass.shape[2:], antialias=True)\n",
    "        x = torch.cat([upsampled, bypass], dim = 1) # Concatenate along the channel dimension\n",
    "        return x\n",
    "\n",
    "# Implement an attention block\n",
    "class attention_block(nn.Module):\n",
    "    def __init__(self, skip_channels, gate_channels, inter_channels = None, n_groups = 8):\n",
    "        super().__init__()\n",
    "\n",
    "        if inter_channels is None:\n",
    "            inter_channels = skip_channels // 2\n",
    "\n",
    "        # Implement W_g i.e the convolution layer that operates on the gate signal\n",
    "        # Upsample gate signal to be the same size as the skip connection\n",
    "        self.W_g = up_sample(in_channels = gate_channels, out_channels = skip_channels)\n",
    "        #self.W_g_norm = nn.GroupNorm(num_groups = n_groups, num_channels = skip_channels)\n",
    "        #self.W_g_act = nn.SiLU() # Swish activation function\n",
    "\n",
    "        # Implement W_x i.e the convolution layer that operates on the skip connection\n",
    "        self.W_x = nn.Conv2d(in_channels = skip_channels, out_channels = inter_channels, kernel_size = 1)\n",
    "        #self.W_x_norm = nn.GroupNorm(num_groups = n_groups, num_channels = inter_channels)\n",
    "        #self.W_x_act = nn.SiLU() # Swish activation function\n",
    "\n",
    "        # Implement phi i.e the convolution layer that operates on the output of W_x + W_g\n",
    "        self.phi = nn.Conv2d(in_channels = inter_channels, out_channels = 1, kernel_size = 1)\n",
    "        #self.phi_norm = nn.GroupNorm(num_groups = n_groups, num_channels = 1)\n",
    "        #self.phi_act = nn.SiLU() # Swish activation function\n",
    "\n",
    "        # Implement the sigmoid activation function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # Implement the Swish activation function\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        # Implement final group normalization layer\n",
    "        self.final_norm = nn.GroupNorm(num_groups = n_groups, num_channels = skip_channels)\n",
    "\n",
    "    # Pass the input through the attention block\n",
    "    def forward(self, skip_connection, gate_signal):\n",
    "        # Upsample the gate signal to match the channels of the skip connection\n",
    "        gate_signal = self.W_g(gate_signal)\n",
    "        # Ensure that the sizes of the skip connection and the gate signal match before addition\n",
    "        if gate_signal.shape[2:] != skip_connection.shape[2:]:\n",
    "            gate_signal = torchvision.transforms.functional.resize(gate_signal, size = skip_connection.shape[2:], antialias=True)\n",
    "        # Project to the intermediate channels\n",
    "        gate_signal = self.W_x(gate_signal)\n",
    "\n",
    "        # Project the skip connection to the intermediate channels\n",
    "        skip_signal = self.W_x(skip_connection)\n",
    "\n",
    "        # Add the skip connection and the gate signal\n",
    "        add_xg = gate_signal + skip_signal\n",
    "\n",
    "        # Pass the output of the addition through the activation function\n",
    "        add_xg = self.act(add_xg)\n",
    "\n",
    "        # Pass the output of attention through a 1x1 convolution layer to obtain the attention map\n",
    "        attention_map = self.sigmoid(self.phi(add_xg))\n",
    "\n",
    "        # Multiply the skip connection with the attention map\n",
    "        # Perform element-wise multiplication\n",
    "        skip_connection = torch.mul(skip_connection, attention_map)\n",
    "\n",
    "        skip_connection = nn.Conv2d(in_channels = skip_connection.shape[1], out_channels = skip_connection.shape[1], kernel_size = 1)(skip_connection)\n",
    "        skip_connection = self.act(self.final_norm(skip_connection))\n",
    "\n",
    "        return skip_connection\n",
    "\n",
    "\n",
    "## Implement a residual attention U-Net\n",
    "class ResidualAttentionUnet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_groups = 4, n_channels = [64, 128, 256, 512, 1024]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the contracting path: residual blocks followed by downsampling\n",
    "        self.down_conv = nn.ModuleList(residual_block(in_chans, out_chans) for in_chans, out_chans in\n",
    "                                       [(in_channels, n_channels[0]), (n_channels[0], n_channels[1]), (n_channels[1], n_channels[2]), (n_channels[2], n_channels[3])])\n",
    "        self.down_samples = nn.ModuleList(down_sample() for _ in range(4))\n",
    "\n",
    "        # Define the bottleneck residual block\n",
    "        self.bottleneck = residual_block(n_channels[3], n_channels[4])\n",
    "\n",
    "\n",
    "        # Define the attention blocks\n",
    "        self.attention_blocks = nn.ModuleList(attention_block(skip_channels = residuals_chans, gate_channels = gate_chans) for gate_chans, residuals_chans in\n",
    "                                              [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
    "\n",
    "\n",
    "        # Define the expanding path: upsample blocks, followed by crop and concatenate, followed by residual blocks\n",
    "        self.upsamples = nn.ModuleList(up_sample(in_chans, out_chans) for in_chans, out_chans in\n",
    "                                       [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
    "        \n",
    "        self.concat = nn.ModuleList(crop_and_concatenate() for _ in range(4))\n",
    "\n",
    "        self.up_conv = nn.ModuleList(residual_block(in_chans, out_chans) for in_chans, out_chans in\n",
    "                                     [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
    "        \n",
    "        # Final 1X1 convolution layer to produce the output segmentation map:\n",
    "        # The primary purpose of 1x1 convolutions is to transform the channel dimension of the feature map,\n",
    "        # while leaving the spatial dimensions unchanged.\n",
    "        self.final_conv = nn.Conv2d(in_channels = n_channels[0] , out_channels = out_channels, kernel_size = 1)\n",
    "\n",
    "    # Pass the input through the residual attention U-Net\n",
    "    def forward(self, x):\n",
    "        # Store the skip connections\n",
    "        skip_connections = []\n",
    "        # # Store the gate signals\n",
    "        # gate_signals = []\n",
    "\n",
    "        # Pass the input through the contracting path\n",
    "        for down_conv, down_sample in zip(self.down_conv, self.down_samples):\n",
    "            x = down_conv(x)\n",
    "            skip_connections.append(x)\n",
    "            #gate_signals.append(x)\n",
    "            x = down_sample(x)\n",
    "        \n",
    "        # Pass the output of the contracting path through the bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections.append(x)\n",
    "\n",
    "        # Attention on the residual connections\n",
    "        #skip_connections = skip_connections[::-1]\n",
    "        n = len(skip_connections)\n",
    "        indices = [(n - 1 - i, n - 2 - i) for i in range(n - 1)]\n",
    "        attentions = []\n",
    "        for i, g_x in enumerate(indices):\n",
    "            g_gate = g_x[0]\n",
    "            x_residual = g_x[1]\n",
    "            attn = self.attention_blocks[i](skip_connections[x_residual], skip_connections[g_gate])\n",
    "            attentions.append(attn)\n",
    "\n",
    "        #attentions = attentions[::-1]\n",
    "    \n",
    "        # Pass the output of the attention blocks through the expanding path\n",
    "        for up_sample, concat, up_conv in zip(self.upsamples, self.concat, self.up_conv):\n",
    "            x = up_sample(x)\n",
    "            x = concat(x, attentions.pop(0))\n",
    "            x = up_conv(x)\n",
    "\n",
    "        # Pass the output of the expanding path through the final convolution layer\n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 572, 572])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sanity check\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResidualAttentionUnet(in_channels = 3, out_channels = 1).to(device)\n",
    "x = torch.randn((1, 3, 572, 572)).to(device)\n",
    "mask = model(x)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4 3\n",
      "1 3 2\n",
      "2 2 1\n",
      "3 1 0\n"
     ]
    }
   ],
   "source": [
    "for i, g_x in enumerate(indices):\n",
    "    print(i, g_x[0], g_x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): attention_block(\n",
       "    (W_g): up_sample(\n",
       "      (up_sample): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (W_x): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (phi): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (sigmoid): Sigmoid()\n",
       "    (act): SiLU()\n",
       "    (final_norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "  )\n",
       "  (1): attention_block(\n",
       "    (W_g): up_sample(\n",
       "      (up_sample): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (W_x): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (phi): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (sigmoid): Sigmoid()\n",
       "    (act): SiLU()\n",
       "    (final_norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "  )\n",
       "  (2): attention_block(\n",
       "    (W_g): up_sample(\n",
       "      (up_sample): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (W_x): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (phi): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (sigmoid): Sigmoid()\n",
       "    (act): SiLU()\n",
       "    (final_norm): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "  )\n",
       "  (3): attention_block(\n",
       "    (W_g): up_sample(\n",
       "      (up_sample): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (W_x): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (phi): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (sigmoid): Sigmoid()\n",
       "    (act): SiLU()\n",
       "    (final_norm): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attention_block(\n",
       "  (W_g): up_sample(\n",
       "    (up_sample): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  )\n",
       "  (W_x): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (phi): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (sigmoid): Sigmoid()\n",
       "  (act): SiLU()\n",
       "  (final_norm): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       ")"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_channels = [64, 128, 256, 512, 1024]\n",
    "attention_blocks = nn.ModuleList(attention_block(skip_channels = residuals_chans, gate_channels = gate_chans) for gate_chans, residuals_chans in\n",
    "                                            [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
    "\n",
    "attention_blocks[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 3), (3, 2), (2, 1), (1, 0)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 5\n",
    "indices = [(n-1-i, n-2-i) for i in range(n-1)]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 4\n",
      "1 2 5\n",
      "2 3 6\n"
     ]
    }
   ],
   "source": [
    "x = [1, 2, 3]\n",
    "y = [4, 5, 6]\n",
    "for a, b in enumerate(zip(x, y)):\n",
    "    print(a, b[0], b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 4, 4])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test attention block\n",
    "att_b = attention_block(skip_channels=512, gate_channels = 1024)\n",
    "x = torch.randn(1, 512, 4, 4)\n",
    "g = torch.randn(1, 1024, 2, 2)\n",
    "y = att_b(x, g)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 3\n",
      "3 2\n",
      "2 1\n",
      "1 0\n"
     ]
    }
   ],
   "source": [
    "a = [0, 1, 2, 3, 4]\n",
    "for i, j in indices:\n",
    "    print(a[i], a[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 71, 71])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_connection = torch.randn((1, 512, 71, 71))\n",
    "skip_connection_shape = skip_connection.shape\n",
    "input = skip_connection\n",
    "gate_signal = torch.randn((1, 1024, 35, 35))\n",
    "# Upsample gate signal\n",
    "upp = up_sample(in_channels=1024, out_channels=512)\n",
    "gate_signal = upp(gate_signal)\n",
    "if gate_signal.shape[2:] != skip_connection_shape[2:]:\n",
    "    gate_signal = torchvision.transforms.functional.resize(gate_signal, size = skip_connection_shape[2:], antialias=True)\n",
    "gate_signal.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gate_signal.shape[2:] == gate_signal.shape[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Resize the skip connection to match the dimensions of the gate_signal\n",
    "skip_connection = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 1)(skip_connection)\n",
    "skip_connection.shape\n",
    "#skip_connection = torchvision.transforms.functional.resize(skip_connection, size = gate_signal.shape[2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match the channel dimensions of the skip connection and the gate signal\n",
    "skip_connection = nn.Conv2d(in_channels = 512, out_channels = 1024, kernel_size = 1)(skip_connection)\n",
    "\n",
    "# Add the skip connection and the gate signal\n",
    "attn = gate_signal + skip_connection\n",
    "\n",
    "# Apply a non-linear activation function\n",
    "attn = nn.ReLU()(attn)\n",
    "\n",
    "# Psi\n",
    "psi = nn.Conv2d(in_channels = 1024, out_channels = 1, kernel_size = 1)(attn)\n",
    "\n",
    "# Apply a sigmoid activation function to transform the output to a range between 0 and 1\n",
    "psi = nn.Sigmoid()(psi)\n",
    "\n",
    "# Upsample the output to the size of the skip connection\n",
    "upsampled_psi = torch.nn.UpsamplingNearest2d(size = skip_connection_shape[2:])(psi)\n",
    "\n",
    "# Multiply the skip connection with the upsampled output\n",
    "attn = torch.mul(upsampled_psi, input)\n",
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 64, 64])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_connection.shape[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 64, 64])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64, 64])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsampled_psi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (72) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m upsampled_psi \u001b[39m*\u001b[39;49m torch\u001b[39m.\u001b[39;49mrandn((\u001b[39m1\u001b[39;49m, \u001b[39m512\u001b[39;49m, \u001b[39m72\u001b[39;49m, \u001b[39m72\u001b[39;49m))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (72) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "upsampled_psi * torch.randn((1, 512, 72, 72))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.5226, -0.5840,  1.4101,  ...,  0.2814,  1.1430, -1.2001],\n",
       "          [ 0.3523, -0.7049,  1.7028,  ...,  0.3190,  1.3144, -0.2240],\n",
       "          [ 0.0247,  0.2418,  0.2530,  ...,  0.0825, -0.6044,  1.6606],\n",
       "          ...,\n",
       "          [ 1.3069,  0.4218,  0.2553,  ...,  1.0103,  1.9774, -0.4712],\n",
       "          [-0.5925,  0.7382, -0.3403,  ...,  0.6515, -0.1832,  0.0458],\n",
       "          [-0.5867, -1.6009, -1.1576,  ..., -0.0732,  0.6630,  1.6776]],\n",
       "\n",
       "         [[ 0.1562,  0.1886,  0.6047,  ..., -0.2483,  0.4273,  0.1439],\n",
       "          [ 0.1227,  0.2132, -0.6852,  ..., -1.6059,  0.4704, -1.9172],\n",
       "          [ 0.8583, -2.0347, -1.0380,  ..., -0.7814,  0.2466,  1.1290],\n",
       "          ...,\n",
       "          [-1.1470, -1.5456,  0.8223,  ...,  0.2230,  0.9603, -0.3140],\n",
       "          [ 0.1218, -2.3055,  1.5006,  ...,  1.8543,  0.9587, -0.8977],\n",
       "          [ 1.4204,  1.6201,  0.8486,  ...,  0.6392, -1.4300, -1.0203]],\n",
       "\n",
       "         [[-0.1478,  1.3004, -0.0982,  ..., -1.8086,  1.1026,  0.3511],\n",
       "          [ 0.4592, -0.6684,  0.5091,  ...,  1.4471, -1.5876, -1.4812],\n",
       "          [-0.9051,  1.3700,  0.3087,  ..., -0.2555, -1.1197, -0.7774],\n",
       "          ...,\n",
       "          [-0.6694,  0.1556,  0.4504,  ..., -1.6014,  0.9934,  0.5709],\n",
       "          [-0.1992,  0.3308, -1.0691,  ...,  2.0722,  0.8369, -0.4652],\n",
       "          [ 0.4757, -0.5029, -0.3211,  ..., -0.9666,  1.1803, -0.3124]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0028,  0.2952, -0.4741,  ..., -0.2608, -2.3447, -0.2002],\n",
       "          [ 0.2999,  0.7960,  0.0515,  ..., -2.5095, -2.3874, -0.9824],\n",
       "          [ 1.0947, -1.5796, -0.8259,  ..., -1.5042,  0.4684, -2.1680],\n",
       "          ...,\n",
       "          [-0.6584,  0.3721, -0.4769,  ..., -1.1133,  1.9335,  1.6465],\n",
       "          [-1.7260, -0.2593,  1.8391,  ...,  0.5264,  0.8353,  0.4173],\n",
       "          [-0.3894, -0.8533, -1.7257,  ..., -2.1393,  0.5581, -0.5247]],\n",
       "\n",
       "         [[ 0.5630, -0.1823, -0.1571,  ...,  1.3343,  0.5015, -0.9000],\n",
       "          [-0.2478, -0.6486, -0.2361,  ...,  1.8317, -2.0180, -1.4764],\n",
       "          [-0.7753, -0.5257,  1.9337,  ..., -1.1322,  0.8584,  1.7016],\n",
       "          ...,\n",
       "          [-0.8151, -0.6531,  0.2814,  ...,  0.4579, -1.5676,  0.0044],\n",
       "          [-0.4840,  0.7237, -0.2291,  ..., -1.1600,  0.2802,  1.0645],\n",
       "          [-0.9728, -1.1521,  1.7429,  ...,  0.1592, -2.4510, -0.7889]],\n",
       "\n",
       "         [[-0.2853, -0.3908,  1.3510,  ...,  0.0171,  0.5491,  1.4175],\n",
       "          [ 2.3252,  1.9441, -0.7489,  ...,  1.9185,  0.8181,  0.9272],\n",
       "          [-1.2627,  0.6548, -0.3422,  ...,  1.8421, -1.1452,  0.8356],\n",
       "          ...,\n",
       "          [-0.9697, -0.5487, -0.5016,  ...,  1.5603, -1.6222, -0.4830],\n",
       "          [ 0.6720,  1.3928,  0.4883,  ...,  0.0067, -0.7107, -0.2542],\n",
       "          [ 0.6287, -0.2813,  0.5098,  ...,  0.9020,  0.7078,  0.2757]]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.2981,  0.4661],\n",
       "         [ 0.8026,  0.5173]]),\n",
       " tensor([[-1.2981,  0.4661],\n",
       "         [ 0.8026,  0.5173]]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.randn(2,2)\n",
    "b= nn.Identity()\n",
    "xx = b(y)\n",
    "y, xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 32, 32])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how 1x1 works\n",
    "m = nn.Conv2d(512, 1024, kernel_size=1)\n",
    "input = torch.randn(1, 512, 32, 32)\n",
    "out = m(input)\n",
    "ss = out + torch.randn(1, 1024, 32, 32)\n",
    "ss.shape\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 32, 32])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 32, 32])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = model(dummy_input)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Unet Definition\n",
    "\n",
    "import math\n",
    "from typing import Optional, Tuple, Union, List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# A fancy activation function\n",
    "class Swish(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Swish actiavation function\n",
    "    $$x \\cdot \\sigma(x)$$\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "# The time embedding \n",
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Embeddings for $t$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int):\n",
    "        \"\"\"\n",
    "        * `n_channels` is the number of dimensions in the embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        # First linear layer\n",
    "        self.lin1 = nn.Linear(self.n_channels // 4, self.n_channels)\n",
    "        # Activation\n",
    "        self.act = Swish()\n",
    "        # Second linear layer\n",
    "        self.lin2 = nn.Linear(self.n_channels, self.n_channels)\n",
    "\n",
    "    def forward(self, t: torch.Tensor):\n",
    "        # Create sinusoidal position embeddings\n",
    "        # [same as those from the transformer](../../transformers/positional_encoding.html)\n",
    "        #\n",
    "        # \\begin{align}\n",
    "        # PE^{(1)}_{t,i} &= sin\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg) \\\\\n",
    "        # PE^{(2)}_{t,i} &= cos\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg)\n",
    "        # \\end{align}\n",
    "        #\n",
    "        # where $d$ is `half_dim`\n",
    "        half_dim = self.n_channels // 8\n",
    "        emb = math.log(10_000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n",
    "\n",
    "        # Transform with the MLP\n",
    "        emb = self.act(self.lin1(emb))\n",
    "        emb = self.lin2(emb)\n",
    "\n",
    "        #\n",
    "        return emb\n",
    "\n",
    "# Residual blocks include 'skip' connections\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Residual block\n",
    "    A residual block has two convolution layers with group normalization.\n",
    "    Each resolution is processed with two residual blocks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, n_groups: int = 32):\n",
    "        \"\"\"\n",
    "        * `in_channels` is the number of input channels\n",
    "        * `out_channels` is the number of input channels\n",
    "        * `time_channels` is the number channels in the time step ($t$) embeddings\n",
    "        * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Group normalization and the first convolution layer\n",
    "        self.norm1 = nn.GroupNorm(n_groups, in_channels)\n",
    "        self.act1 = Swish()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "        # Group normalization and the second convolution layer\n",
    "        self.norm2 = nn.GroupNorm(n_groups, out_channels)\n",
    "        self.act2 = Swish()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "        # If the number of input channels is not equal to the number of output channels we have to\n",
    "        # project the shortcut connection\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        # Linear layer for time embeddings\n",
    "        self.time_emb = nn.Linear(time_channels, out_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size, time_channels]`\n",
    "        \"\"\"\n",
    "        # First convolution layer\n",
    "        h = self.conv1(self.act1(self.norm1(x)))\n",
    "        # Add time embeddings\n",
    "        h += self.time_emb(t)[:, :, None, None]\n",
    "        # Second convolution layer\n",
    "        h = self.conv2(self.act2(self.norm2(h)))\n",
    "\n",
    "        # Add the shortcut connection and return\n",
    "        return h + self.shortcut(x)\n",
    "\n",
    "# Ahh yes, magical attention...\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Attention block\n",
    "    This is similar to [transformer multi-head attention](../../transformers/mha.html).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int, n_heads: int = 1, d_k: int = None, n_groups: int = 32):\n",
    "        \"\"\"\n",
    "        * `n_channels` is the number of channels in the input\n",
    "        * `n_heads` is the number of heads in multi-head attention\n",
    "        * `d_k` is the number of dimensions in each head\n",
    "        * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Default `d_k`\n",
    "        if d_k is None:\n",
    "            d_k = n_channels\n",
    "        # Normalization layer\n",
    "        self.norm = nn.GroupNorm(n_groups, n_channels)\n",
    "        # Projections for query, key and values\n",
    "        self.projection = nn.Linear(n_channels, n_heads * d_k * 3)\n",
    "        # Linear layer for final transformation\n",
    "        self.output = nn.Linear(n_heads * d_k, n_channels)\n",
    "        # Scale for dot-product attention\n",
    "        self.scale = d_k ** -0.5\n",
    "        #\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size, time_channels]`\n",
    "        \"\"\"\n",
    "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = t\n",
    "        # Get shape\n",
    "        batch_size, n_channels, height, width = x.shape\n",
    "        # Change `x` to shape `[batch_size, seq, n_channels]`\n",
    "        x = x.view(batch_size, n_channels, -1).permute(0, 2, 1)\n",
    "        # Get query, key, and values (concatenated) and shape it to `[batch_size, seq, n_heads, 3 * d_k]`\n",
    "        qkv = self.projection(x).view(batch_size, -1, self.n_heads, 3 * self.d_k)\n",
    "        # Split query, key, and values. Each of them will have shape `[batch_size, seq, n_heads, d_k]`\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
    "        # Calculate scaled dot-product $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n",
    "        attn = torch.einsum('bihd,bjhd->bijh', q, k) * self.scale\n",
    "        # Softmax along the sequence dimension $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n",
    "        attn = attn.softmax(dim=1)\n",
    "        # Multiply by values\n",
    "        res = torch.einsum('bijh,bjhd->bihd', attn, v)\n",
    "        # Reshape to `[batch_size, seq, n_heads * d_k]`\n",
    "        res = res.view(batch_size, -1, self.n_heads * self.d_k)\n",
    "        # Transform to `[batch_size, seq, n_channels]`\n",
    "        res = self.output(res)\n",
    "\n",
    "        # Add skip connection\n",
    "        res += x\n",
    "\n",
    "        # Change to shape `[batch_size, in_channels, height, width]`\n",
    "        res = res.permute(0, 2, 1).view(batch_size, n_channels, height, width)\n",
    "\n",
    "        #\n",
    "        return res\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Down block\n",
    "    This combines `ResidualBlock` and `AttentionBlock`. These are used in the first half of U-Net at each resolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n",
    "        super().__init__()\n",
    "        self.res = ResidualBlock(in_channels, out_channels, time_channels)\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        x = self.res(x, t)\n",
    "        x = self.attn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Up block\n",
    "    This combines `ResidualBlock` and `AttentionBlock`. These are used in the second half of U-Net at each resolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n",
    "        super().__init__()\n",
    "        # The input has `in_channels + out_channels` because we concatenate the output of the same resolution\n",
    "        # from the first half of the U-Net\n",
    "        self.res = ResidualBlock(in_channels + out_channels, out_channels, time_channels)\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        x = self.res(x, t)\n",
    "        x = self.attn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MiddleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Middle block\n",
    "    It combines a `ResidualBlock`, `AttentionBlock`, followed by another `ResidualBlock`.\n",
    "    This block is applied at the lowest resolution of the U-Net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int, time_channels: int):\n",
    "        super().__init__()\n",
    "        self.res1 = ResidualBlock(n_channels, n_channels, time_channels)\n",
    "        self.attn = AttentionBlock(n_channels)\n",
    "        self.res2 = ResidualBlock(n_channels, n_channels, time_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        x = self.res1(x, t)\n",
    "        x = self.attn(x)\n",
    "        x = self.res2(x, t)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Scale up the feature map by $2 \\times$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(n_channels, n_channels, (4, 4), (2, 2), (1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = t\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Scale down the feature map by $\\frac{1}{2} \\times$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_channels, n_channels, (3, 3), (2, 2), (1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = t\n",
    "        return self.conv(x)\n",
    "\n",
    "# The core class definition (aka the important bit)\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    ## U-Net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_channels: int = 3, n_channels: int = 64,\n",
    "                 ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n",
    "                 is_attn: Union[Tuple[bool, ...], List[int]] = (False, False, True, True),\n",
    "                 n_blocks: int = 2):\n",
    "        \"\"\"\n",
    "        * `image_channels` is the number of channels in the image. $3$ for RGB.\n",
    "        * `n_channels` is number of channels in the initial feature map that we transform the image into\n",
    "        * `ch_mults` is the list of channel numbers at each resolution. The number of channels is `ch_mults[i] * n_channels`\n",
    "        * `is_attn` is a list of booleans that indicate whether to use attention at each resolution\n",
    "        * `n_blocks` is the number of `UpDownBlocks` at each resolution\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Number of resolutions\n",
    "        n_resolutions = len(ch_mults)\n",
    "\n",
    "        # Project image into feature map\n",
    "        self.image_proj = nn.Conv2d(image_channels, n_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "        # Time embedding layer. Time embedding has `n_channels * 4` channels\n",
    "        self.time_emb = TimeEmbedding(n_channels * 4)\n",
    "\n",
    "        # #### First half of U-Net - decreasing resolution\n",
    "        down = []\n",
    "        # Number of channels\n",
    "        out_channels = in_channels = n_channels\n",
    "        # For each resolution\n",
    "        for i in range(n_resolutions):\n",
    "            # Number of output channels at this resolution\n",
    "            out_channels = in_channels * ch_mults[i]\n",
    "            # Add `n_blocks`\n",
    "            for _ in range(n_blocks):\n",
    "                down.append(DownBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
    "                in_channels = out_channels\n",
    "            # Down sample at all resolutions except the last\n",
    "            if i < n_resolutions - 1:\n",
    "                down.append(Downsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.down = nn.ModuleList(down)\n",
    "\n",
    "        # Middle block\n",
    "        self.middle = MiddleBlock(out_channels, n_channels * 4, )\n",
    "\n",
    "        # #### Second half of U-Net - increasing resolution\n",
    "        up = []\n",
    "        # Number of channels\n",
    "        in_channels = out_channels\n",
    "        # For each resolution\n",
    "        for i in reversed(range(n_resolutions)):\n",
    "            # `n_blocks` at the same resolution\n",
    "            out_channels = in_channels\n",
    "            for _ in range(n_blocks):\n",
    "                up.append(UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
    "            # Final block to reduce the number of channels\n",
    "            out_channels = in_channels // ch_mults[i]\n",
    "            up.append(UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
    "            in_channels = out_channels\n",
    "            # Up sample at all resolutions except last\n",
    "            if i > 0:\n",
    "                up.append(Upsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.up = nn.ModuleList(up)\n",
    "\n",
    "        # Final normalization and convolution layer\n",
    "        self.norm = nn.GroupNorm(8, n_channels)\n",
    "        self.act = Swish()\n",
    "        self.final = nn.Conv2d(in_channels, image_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size]`\n",
    "        \"\"\"\n",
    "\n",
    "        # Get time-step embeddings\n",
    "        t = self.time_emb(t)\n",
    "\n",
    "        # Get image projection\n",
    "        x = self.image_proj(x)\n",
    "\n",
    "        # `h` will store outputs at each resolution for skip connection\n",
    "        h = [x]\n",
    "        # First half of U-Net\n",
    "        for m in self.down:\n",
    "            x = m(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "        # Middle (bottom)\n",
    "        x = self.middle(x, t)\n",
    "\n",
    "        # Second half of U-Net\n",
    "        for m in self.up:\n",
    "            if isinstance(m, Upsample):\n",
    "                x = m(x, t)\n",
    "            else:\n",
    "                # Get the skip connection from first half of U-Net and concatenate\n",
    "                s = h.pop()\n",
    "                x = torch.cat((x, s), dim=1)\n",
    "                #\n",
    "                x = m(x, t)\n",
    "\n",
    "        # Final normalization and convolution\n",
    "        return self.final(self.act(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 144 but got size 143 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m unet \u001b[39m=\u001b[39m UNet()\n\u001b[0;32m     12\u001b[0m \u001b[39m# The foreward pass (takes both x and t)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m model_output \u001b[39m=\u001b[39m unet(x, t)\n\u001b[0;32m     15\u001b[0m \u001b[39m# The output shape matches the input.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m model_output\u001b[39m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Users\\homeuser\\Documents\\ml_cw2\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[48], line 378\u001b[0m, in \u001b[0;36mUNet.forward\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    376\u001b[0m     \u001b[39m# Get the skip connection from first half of U-Net and concatenate\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     s \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39mpop()\n\u001b[1;32m--> 378\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((x, s), dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    379\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[0;32m    380\u001b[0m     x \u001b[39m=\u001b[39m m(x, t)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 144 but got size 143 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# Let's see it in action on dummy data:\n",
    "\n",
    "# A dummy batch of 10 3-channel 32px images\n",
    "x = torch.randn(1, 3, 572, 572)\n",
    "\n",
    "# 't' - what timestep are we on\n",
    "t = torch.tensor([50.], dtype=torch.long)\n",
    "\n",
    "# Define the unet model\n",
    "unet = UNet()\n",
    "\n",
    "# The foreward pass (takes both x and t)\n",
    "model_output = unet(x, t)\n",
    "\n",
    "# The output shape matches the input.\n",
    "model_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, f_g, f_l, f_int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.w_g = nn.Sequential(\n",
    "                                nn.Conv2d(f_g, f_int,\n",
    "                                            kernel_size=1, stride=1,\n",
    "                                            padding=0, bias=True),\n",
    "                                nn.BatchNorm2d(f_int)\n",
    "        )\n",
    "        \n",
    "        self.w_x = nn.Sequential(\n",
    "                                nn.Conv2d(f_l, f_int,\n",
    "                                            kernel_size=1, stride=1,\n",
    "                                            padding=0, bias=True),\n",
    "                                nn.BatchNorm2d(f_int)\n",
    "        )\n",
    "        \n",
    "        self.psi = nn.Sequential(\n",
    "                                nn.Conv2d(f_int, 1,\n",
    "                                            kernel_size=1, stride=1,\n",
    "                                            padding=0,  bias=True),\n",
    "                                nn.BatchNorm2d(1),\n",
    "                                nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, g, x):\n",
    "        g1 = self.w_g(g)\n",
    "        x1 = self.w_x(x)\n",
    "        psi = self.relu(g1+x1)\n",
    "        psi = self.psi(psi)\n",
    "        \n",
    "        return psi*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 28, 28])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_g=512\n",
    "f_l=512\n",
    "f_int = 256\n",
    "#  decoder + concat\n",
    "# d5 = self.up5(x5)\n",
    "# x4 = self.att5(g=d5, x=x4)\n",
    "g = torch.randn(1, 512, 28, 28)\n",
    "x = torch.randn(1, 512, 28, 28)\n",
    "att5 = AttentionBlock(f_g, f_l, f_int)\n",
    "att5(g, x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 28, 28])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1 = nn.Conv2d(f_g, f_int, kernel_size=1, stride=1, padding=0, bias=True)(g)\n",
    "g1 = nn.BatchNorm2d(f_int)(g1)\n",
    "x1 = nn.Conv2d(f_l, f_int, kernel_size=1, stride=1, padding=0, bias=True)(x)\n",
    "x1 = nn.BatchNorm2d(f_int)(x1)\n",
    "psi = nn.ReLU(inplace=True)(g1+x1)\n",
    "psi = nn.Conv2d(f_int, 1, kernel_size=1, stride=1, padding=0,  bias=True)(psi)\n",
    "psi = nn.BatchNorm2d(1)(psi)\n",
    "psi = nn.Sigmoid()(psi)\n",
    "out = torch.mul(psi, x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 28, 28])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttentionBlock(\n",
       "  (w_g): Sequential(\n",
       "    (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (w_x): Sequential(\n",
       "    (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (psi): Sequential(\n",
       "    (0): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Sigmoid()\n",
       "  )\n",
       "  (relu): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
