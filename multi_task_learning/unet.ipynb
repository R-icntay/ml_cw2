{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a U-Net model\n",
    "\n",
    "This code implements a U-Net model for semantic segmentation from the paper [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597).\n",
    "\n",
    "A U-Net consists of an encoder - a series of convolution and pooling layers  which reduce the spatial resolution of the input, followed by a decoder - a series of transposed convolution and upsampling layers which increase the spatial resolution of the input. The encoder and decoder are connected by a bottleneck layer which is responsible for reducing the number of channels in the input.\n",
    "\n",
    "The key innovation of U-Net is the addition of skip connections that connect the contracting path to the corresponding layers in the expanding path, allowing the network to recover fine-grained details lost during downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional\n",
    "\n",
    "# Implement the double 3X3 convolution blocks\n",
    "# The original paper did not use padding, but we will use padding to keep the image size the same\n",
    "\n",
    "class double_convolution(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the double convolution block which consists of two 3X3 convolution layers,\n",
    "    each followed by a ReLU activation function.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels): # Initialize the class\n",
    "        super().__init__() # Initialize the parent class\n",
    "\n",
    "        # First 3X3 convolution layer\n",
    "        self.first_cnn = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 3, padding = 1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        # Second 3X3 convolution layer\n",
    "        self.second_cnn = nn.Conv2d(in_channels = out_channels, out_channels = out_channels, kernel_size = 3, padding = 1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "    # Pass the input through the double convolution block\n",
    "    def forward(self, x):\n",
    "        x = self.first_cnn(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.act2(self.second_cnn(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Implement the Downsample block that occurs after each double convolution block\n",
    "class down_sample(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the downsample block which consists of a Max Pooling layer with a kernel size of 2.\n",
    "    The Max Pooling layer halves the image size reducing the spatial resolution of the feature maps\n",
    "    while retaining the most important features.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "    \n",
    "    # Pass the input through the downsample block\n",
    "    def forward(self, x):\n",
    "        x = self.max_pool(x)\n",
    "        return x\n",
    "    \n",
    "# Implement the UpSample block that occurs in the decoder part of the network\n",
    "class up_sample(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the upsample block which consists of a convolution transpose layer with a kernel size of 2.\n",
    "    The convolution transpose layer doubles the image size increasing the spatial resolution of the feature maps\n",
    "    while retaining the learned features.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolution transpose layer\n",
    "        self.up_sample = nn.ConvTranspose2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 2, stride = 2)\n",
    "\n",
    "    # Pass the input through the upsample block\n",
    "    def forward(self, x):\n",
    "        x = self.up_sample(x)\n",
    "        return x \n",
    "\n",
    "# Implement the crop and concatenate block that occurs in the decoder part of the network\n",
    "# This block concatenates the output of the upsample block with the output of the corresponding downsample block\n",
    "# The output of the crop and concatenate block is then passed through a double convolution block\n",
    "class crop_and_concatenate(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the crop and concatenate block which combines the output of the upsample block\n",
    "    with the corresponding features from the contracting path through skip connections,\n",
    "    allowing the network to recover the fine-grained details lost during downsampling\n",
    "    and produce a high-resolution output segmentation map.\n",
    "    \"\"\" \n",
    "    def forward(self, upsampled, bypass):\n",
    "        # Crop the feature map from the contacting path to match the size of the upsampled feature map\n",
    "        bypass = torchvision.transforms.functional.center_crop(img = bypass, output_size = [upsampled.shape[2], upsampled.shape[3]]) \n",
    "        # Concatenate the upsampled feature map with the cropped feature map from the contracting path\n",
    "        x = torch.cat([upsampled, bypass], dim = 1) # Concatenate along the channel dimension\n",
    "        return x\n",
    "\n",
    "# m = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "# input = torch.randn(1, 1024, 28, 28)\n",
    "# m(input).shape \n",
    "\n",
    "# m = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "# xx = torch.randn(1, 1, 143, 143)\n",
    "# m(xx).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implement the UNet architecture\n",
    "class UNet(nn.Module):\n",
    "    # in_channels: number of channels in the input image\n",
    "    # out_channels: number of channels in the output image\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the contracting path: convolution blocks followed by downsample blocks\n",
    "        self.down_conv = nn.ModuleList(double_convolution(in_chans, out_chans) for in_chans, out_chans in\n",
    "                                       [(in_channels, 64), (64, 128), (128, 256), (256, 512)]) # List of downsample blocks\n",
    "        \n",
    "        self.down_samples = nn.ModuleList(down_sample() for _ in range(4))\n",
    "\n",
    "        # Define the bottleneck layer\n",
    "        self.bottleneck = double_convolution(in_channels = 512, out_channels = 1024)\n",
    "\n",
    "        # Define the expanding path: upsample blocks followed by convolution blocks\n",
    "        self.up_samples = nn.ModuleList(up_sample(in_chans, out_chans) for in_chans, out_chans in\n",
    "                                        [(1024, 512), (512, 256), (256, 128), (128, 64)]) # List of upsample blocks\n",
    "        \n",
    "        self.concat = nn.ModuleList(crop_and_concatenate() for _ in range(4))\n",
    "\n",
    "        self.up_conv = nn.ModuleList(double_convolution(in_chans, out_chans) for in_chans, out_chans in\n",
    "                                        [(1024, 512), (512, 256), (256, 128), (128, 64)]) # List of convolution blocks\n",
    "        \n",
    "        # Final 1X1 convolution layer to produce the output segmentation map:\n",
    "        # The primary purpose of 1x1 convolutions is to transform the channel dimension of the feature map,\n",
    "        # while leaving the spatial dimensions unchanged.\n",
    "        self.final_conv = nn.Conv2d(in_channels = 64, out_channels = out_channels, kernel_size = 1)\n",
    "\n",
    "    # Pass the input through the UNet architecture\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the contacting path\n",
    "        skip_connections = [] # List to store the outputs of the downsample blocks\n",
    "        for down_conv, down_sample in zip(self.down_conv, self.down_samples):\n",
    "            x = down_conv(x)\n",
    "            skip_connections.append(x)\n",
    "            x = down_sample(x)\n",
    "        \n",
    "        # Pass the output of the contacting path through the bottleneck layer\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        # Pass the output of the bottleneck layer through the expanding path\n",
    "        skip_connections = skip_connections[::-1] # Reverse the list of skip connections\n",
    "        for up_sample, concat, up_conv in zip(self.up_samples, self.concat, self.up_conv):\n",
    "            x = up_sample(x)\n",
    "            x = concat(x, skip_connections.pop(0)) # Remove the first element from the list of skip connections\n",
    "            x = up_conv(x)\n",
    "        \n",
    "        # Pass the output of the expanding path through the final convolution layer\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the input through the contracting path\n",
    "        skip_connections = []\n",
    "        for down_conv, down_sample in zip(self.down_conv, self.down_samples):\n",
    "            x = down_conv(x)\n",
    "            skip_connections.append(x)\n",
    "            x = down_sample(x)\n",
    "\n",
    "        # Pass the output of the contracting path through the bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        # Pass the output of the bottleneck through the expanding path\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for up_sample, concat, up_conv in zip(self.up_samples, self.concat, self.up_conv):\n",
    "            x = up_sample(x)\n",
    "            x = concat(x, skip_connections.pop(0))\n",
    "            x = up_conv(x)\n",
    "\n",
    "        # Pass the output of the expanding path through the final convolution layer\n",
    "        x = self.final_conv(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 6, 12, 10]\n",
      "[1, 7, 13, 20]\n",
      "[2, 8, 14, 30]\n",
      "[3, 9, 15, 40]\n",
      "[4, 10, 16, 50]\n",
      "[5, 11, 17, 60]\n"
     ]
    }
   ],
   "source": [
    "skp = [10, 20, 30, 40, 50, 60]\n",
    "# Demonstrate zip function\n",
    "a = [0, 1, 2, 3, 4, 5]\n",
    "b = [6, 7, 8, 9, 10, 11]\n",
    "c = [12, 13, 14, 15, 16, 17]\n",
    "for i, j, k in zip(a, b, c):\n",
    "    # Concate the elements of the three lists\n",
    "    print([i, j, k] + [skp.pop(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, [20, 30, 40, 50, 60])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skp = [10, 20, 30, 40, 50, 60]\n",
    "s=skp.pop(0)\n",
    "s, skp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 388, 388])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt = nn.Conv2d(in_channels = 64, out_channels= 1, kernel_size = 1)\n",
    "imgg = torch.randn(1, 64, 388, 388)\n",
    "xt(imgg).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-3): 4 x crop_and_concatenate()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.ModuleList(crop_and_concatenate() for _ in range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): double_convolution(\n",
       "    (first_cnn): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act1): ReLU()\n",
       "    (second_cnn): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act2): ReLU()\n",
       "  )\n",
       "  (1): double_convolution(\n",
       "    (first_cnn): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act1): ReLU()\n",
       "    (second_cnn): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act2): ReLU()\n",
       "  )\n",
       "  (2): double_convolution(\n",
       "    (first_cnn): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act1): ReLU()\n",
       "    (second_cnn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act2): ReLU()\n",
       "  )\n",
       "  (3): double_convolution(\n",
       "    (first_cnn): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act1): ReLU()\n",
       "    (second_cnn): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act2): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.ModuleList(double_convolution(i, o) for i, o in\n",
    "                                       [(1, 64), (64, 128), (128, 256), (256, 512)]) # List of downsample blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape[2:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
