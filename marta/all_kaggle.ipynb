{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_labels(labels, organs):\n",
    "    \"\"\"\n",
    "    Change labels so that the targetted sections are numbered from 1.0.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_org = organs['all']\n",
    "    main = organs['main']\n",
    "    aux  = organs['aux']\n",
    "    dict = organs['dict']\n",
    "    \n",
    "    # Modify the main labels to match the output of the main decoder\n",
    "    main_labels = labels.clone()\n",
    "    aux_labels = labels.clone()\n",
    "\n",
    "    count_main = 1.0\n",
    "    count_aux  = 1.0\n",
    "    \n",
    "    for organ in all_org:\n",
    "        # Modify the labels to match the output of the decoder\n",
    "        if organ in main:\n",
    "            main_labels[main_labels == dict[organ]] = count_main\n",
    "            count_main += 1.0\n",
    "        else:\n",
    "            main_labels[main_labels != dict[organ]] = 0.0\n",
    "            \n",
    "        if organ in aux:\n",
    "            aux_labels[aux_labels == dict[organ]] = count_aux\n",
    "            count_aux += 1.0\n",
    "        else:\n",
    "            aux_labels[aux_labels != dict[organ]] = 0.0\n",
    "    \n",
    "    return main_labels, aux_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "from shutil import copyfile\n",
    "import os\n",
    "\n",
    "def split_data(img_path, scale=1):\n",
    "    \"\"\"\n",
    "    Read all images and divide into training, validation and test sets.\n",
    "    Scale to test models on fewer data.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(\"Splitting data into train-validate-test sets...\")\n",
    "    \n",
    "    # Delete two images that do not have segmentation masks\n",
    "    for file in ['001001_img.nii', '005057_img.nii']:\n",
    "        if os.path.exists(Path(img_path / file)):\n",
    "            os.remove(Path(img_path / file))\n",
    "        else:\n",
    "            print(\"The file does not exist\")\n",
    "        \n",
    "    # Read all files ending with _img.nii\n",
    "    img_files   = list(img_path.glob(\"*_img.nii\")) # Image and mask are in the same folder\n",
    "    num_images  = len(img_files)\n",
    "\n",
    "    # Create train, validation and test splits\n",
    "    train_split = int(0.8 * num_images / scale)\n",
    "    val_split   = int(0.1 * num_images / scale)\n",
    "    test_split  = int((num_images - (train_split + val_split)* scale) / scale)\n",
    "\n",
    "    # Set the random seed for reproducibility\n",
    "    random.seed(2022)\n",
    "    \n",
    "    # Shuffle the image files\n",
    "    random.shuffle(img_files)\n",
    "\n",
    "    # Split the dataset\n",
    "    \"\"\"train_images    = img_files[:train_split]\n",
    "    val_images      = img_files[train_split:(train_split + val_split)]\n",
    "    test_images     = img_files[(train_split + val_split): int(num_images/scale)]\"\"\"\n",
    "    \n",
    "    train_images    = img_files[:2]\n",
    "    val_images      = img_files[2:4]\n",
    "    test_images     = img_files[4:6]\n",
    "\n",
    "    # Create train, validation and test directories\n",
    "    train_image_dir     = Path(img_path / \"train_images\")\n",
    "    train_mask_dir      = Path(img_path / \"train_masks\")\n",
    "    val_image_dir       = Path(img_path / \"val_images\")\n",
    "    val_mask_dir        = Path(img_path / \"val_masks\")\n",
    "    test_image_dir      = Path(img_path / \"test_images\")\n",
    "    test_mask_dir       = Path(img_path / \"test_masks\")\n",
    "\n",
    "    # Create the directories if they don't exist\n",
    "    if not os.path.exists(train_image_dir) and not os.path.exists(train_mask_dir) and not os.path.exists(val_image_dir) and not os.path.exists(val_mask_dir) and not os.path.exists(test_image_dir) and not os.path.exists(test_mask_dir):\n",
    "        for directory in [train_image_dir, train_mask_dir, val_image_dir, val_mask_dir, test_image_dir, test_mask_dir]:\n",
    "            directory.mkdir(exist_ok = True, parents = True)\n",
    "\n",
    "        # Copy the images and their corresponding segmentation masks to their respective directories\n",
    "        for directory, images in zip([train_image_dir, val_image_dir, test_image_dir], [train_images, val_images, test_images]):\n",
    "            for image in images:\n",
    "                # Copy image\n",
    "                copyfile(image, directory / image.name)\n",
    "\n",
    "                # Get corresponding segmentation mask\n",
    "                mask = image.name.replace(\"_img.nii\", \"_mask.nii\")\n",
    "\n",
    "                # Copy segmentation mask\n",
    "                copyfile(image.parent / mask, image.parent / directory.name.replace(\"images\", \"masks\") / mask)\n",
    "\n",
    "    # Put the train images and masks in a dictionary\n",
    "    train_images    = sorted(train_image_dir.glob(\"*\"))\n",
    "    train_masks     = sorted(train_mask_dir.glob(\"*\"))\n",
    "    train_files     = [{\"image\": image_name, \"mask\": mask_name} for image_name, mask_name in zip(train_images, train_masks)]\n",
    "    \n",
    "    # Put the validation images and masks in a dictionary\n",
    "    val_images      = sorted(val_image_dir.glob(\"*\"))\n",
    "    val_masks       = sorted(val_mask_dir.glob(\"*\"))\n",
    "    val_files       = [{\"image\": image_name, \"mask\": mask_name} for image_name, mask_name in zip(val_images, val_masks)]\n",
    "    \n",
    "    # Put the test images and masks in a dictionary\n",
    "    test_images     = sorted(test_image_dir.glob(\"*\"))\n",
    "    test_masks      = sorted(test_mask_dir.glob(\"*\"))\n",
    "    test_files      = [{\"image\": image_name, \"mask\": mask_name} for image_name, mask_name in zip(test_images, test_masks)]\n",
    "        \n",
    "    print('Images have been divided into train-validate-test sets.')\n",
    "    print('Total number of images: ', num_images)\n",
    "    print('Number of images train-validate-test: ', train_split, '-', val_split, '-', test_split)\n",
    "\n",
    "    return train_files, val_files, test_files\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from monai.data             import DataLoader, Dataset, decollate_batch\n",
    "from monai.metrics          import DiceMetric\n",
    "from pathlib                import Path\n",
    "from labels                 import modify_labels\n",
    "\n",
    "def set_data(val_files, val_transforms, BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Create dataloader for test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    val_ds = Dataset(data = val_files, transform = val_transforms)\n",
    "    val_dl = DataLoader(dataset = val_ds, batch_size = BATCH_SIZE, num_workers = 4, shuffle = False)\n",
    "    \n",
    "    return val_dl\n",
    "\n",
    "\n",
    "def set_model_params():\n",
    "    \"\"\"\n",
    "    Set metrics for evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input image has eight anatomical structures of planning interest\n",
    "    dice_metric_main    = DiceMetric(include_background=False, reduction=\"mean\") # Collect the loss and metric values for every iteration\n",
    "    \n",
    "    return dice_metric_main\n",
    "\n",
    "\n",
    "def save_results(MODEL_NAME, MODEL_PATH, main_metric_values):\n",
    "    \"\"\"\n",
    "    Save performance metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save metric values\n",
    "    pref = f\"{MODEL_NAME.split('.')[0]}\"\n",
    "    with open(MODEL_PATH/f\"{pref}_test.pkl\", \"wb\") as f:\n",
    "        pickle.dump(main_metric_values, f)\n",
    "\n",
    "\n",
    "def test_model_base(model, device, params, val_files, val_transforms, organs_dict, pred_main, label_main, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate the test dataset\n",
    "    \"\"\"\n",
    "    BATCH_SIZE = params['BATCH_SIZE']\n",
    "    \n",
    "    val_dl              = set_data(val_files, val_transforms, BATCH_SIZE)\n",
    "    dice_metric_main    = set_model_params()\n",
    "    \n",
    "    # Model save path\n",
    "    MODEL_PATH = Path(\"models\")\n",
    "    MODEL_NAME = model_name + \".pth\"\n",
    "    MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "    \n",
    "    model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "    model.eval()\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Starting model testing...\")\n",
    "    \n",
    "    # Disable gradient calculation\n",
    "    with torch.inference_mode():\n",
    "        # Loop through the validation data\n",
    "        for val_data in val_dl:\n",
    "            val_inputs, val_labels = val_data[\"image\"].permute(0, 1, 4, 2, 3).to(device), val_data[\"mask\"].to(device)\n",
    "            val_main_labels, _     = modify_labels(val_labels, organs_dict)\n",
    "\n",
    "            # Forward pass\n",
    "            val_main_outputs = model(val_inputs)\n",
    "            val_main_outputs = val_main_outputs.permute(0, 1, 3, 4, 2)\n",
    "            \n",
    "            # Transform main outputs and labels to calculate inference loss\n",
    "            val_main_outputs    = [pred_main(i) for i in decollate_batch(val_main_outputs)]\n",
    "            val_main_labels     = [label_main(i) for i in decollate_batch(val_main_labels)]\n",
    "\n",
    "            # Compute dice metric for current iteration\n",
    "            dice_metric_main(y_pred = val_main_outputs, y = val_main_labels)\n",
    "            \n",
    "        # Compute the average metric value across all iterations\n",
    "        main_metric = dice_metric_main.aggregate().item()\n",
    "        \n",
    "    print(\n",
    "        f\"\\nMean dice for main task: {main_metric:.4f}\"\n",
    "        )\n",
    "    \n",
    "    save_results(MODEL_NAME, MODEL_PATH, main_metric)\n",
    "    \n",
    "\n",
    "                    \n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from monai.data                 import DataLoader, Dataset, decollate_batch\n",
    "from monai.metrics              import DiceMetric, MSEMetric\n",
    "from monai.metrics.regression   import SSIMMetric\n",
    "from pathlib                    import Path\n",
    "from labels                     import modify_labels\n",
    "\n",
    "def set_data(val_files, val_transforms, BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Create dataloader for test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    val_ds = Dataset(data = val_files, transform = val_transforms)\n",
    "    val_dl = DataLoader(dataset = val_ds, batch_size = BATCH_SIZE, num_workers = 4, shuffle = False)\n",
    "    \n",
    "    return val_dl\n",
    "\n",
    "\n",
    "def set_model_params(TASK):\n",
    "    \"\"\"\n",
    "    Set metrics for evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input image has eight anatomical structures of planning interest\n",
    "    metric_main    = DiceMetric(include_background=False, reduction=\"mean\")# Collect the loss and metric values for every iteration\n",
    "    if TASK == 'SEGMENT':\n",
    "        metric_aux  = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "    else:\n",
    "        metric_aux  = MSEMetric()\n",
    "    \n",
    "    return metric_main, metric_aux\n",
    "\n",
    "\n",
    "def save_results(MODEL_NAME, MODEL_PATH, main_metric_values, aux_metric_values):\n",
    "    \"\"\"\n",
    "    Save performance metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save metric values\n",
    "    pref = f\"{MODEL_NAME.split('.')[0]}\"\n",
    "    with open(MODEL_PATH/f\"{pref}_main_test.pkl\", \"wb\") as f:\n",
    "        pickle.dump(main_metric_values, f)\n",
    "    with open(MODEL_PATH/f\"{pref}_aux_test.pkl\", \"wb\") as f:\n",
    "        pickle.dump(aux_metric_values, f)\n",
    "\n",
    "\n",
    "def test_model(model, device, params, val_files, val_transforms, organs_dict, pred_main, label_main, pred_aux, label_aux, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate the test dataset\n",
    "    \"\"\"\n",
    "    BATCH_SIZE = params['BATCH_SIZE']\n",
    "    TASK       = params['TASK']\n",
    "    \n",
    "    val_dl                  = set_data(val_files, val_transforms, BATCH_SIZE)\n",
    "    metric_main, metric_aux = set_model_params(TASK)\n",
    "    \n",
    "    # Model save path\n",
    "    MODEL_PATH = Path(\"models\")\n",
    "    MODEL_NAME = model_name + \".pth\"\n",
    "    MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "    \n",
    "    model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "    model.eval()\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Starting model testing...\")\n",
    "    \n",
    "    # Disable gradient calculation\n",
    "    with torch.inference_mode():\n",
    "        # Loop through the validation data\n",
    "        for val_data in val_dl:\n",
    "            val_inputs, val_labels = val_data[\"image\"].permute(0, 1, 4, 2, 3).to(device), val_data[\"mask\"].to(device)\n",
    "            val_main_labels, val_aux_labels = modify_labels(val_labels, organs_dict)\n",
    "\n",
    "            # Forward pass\n",
    "            val_main_outputs, val_aux_outputs = model(val_inputs)\n",
    "            val_main_outputs, val_aux_outputs = val_main_outputs.permute(0, 1, 3, 4, 2), val_aux_outputs.permute(0, 1, 3, 4, 2)\n",
    "\n",
    "            # Transform main outputs and labels to calculate inference loss\n",
    "            val_main_outputs    = [pred_main(i) for i in decollate_batch(val_main_outputs)]\n",
    "            val_main_labels     = [label_main(i) for i in decollate_batch(val_main_labels)]\n",
    "\n",
    "            # Compute dice metric for current iteration\n",
    "            metric_main(y_pred = val_main_outputs, y = val_main_labels)\n",
    "            if TASK == 'SEGMENT':\n",
    "                # Transform aux outputs and labels to calculate inference loss\n",
    "                val_aux_outputs     = [pred_aux(i) for i in decollate_batch(val_aux_outputs)]\n",
    "                val_aux_labels      = [label_aux(i) for i in decollate_batch(val_aux_labels)]\n",
    "            \n",
    "                metric_aux(y_pred = val_aux_outputs, y = val_aux_labels)\n",
    "            else:\n",
    "                metric_aux(y_pred = val_aux_outputs, y = val_inputs.permute(0, 1, 3, 4, 2))\n",
    "            \n",
    "        # Compute the average metric value across all iterations\n",
    "        main_metric = metric_main.aggregate().item()\n",
    "        aux_metric  = metric_aux.aggregate().item()\n",
    "        \n",
    "    print(\n",
    "        f\"\\nMean dice for main task: {main_metric:.4f}\"\n",
    "        f\"\\nMean metric for aux task: {aux_metric:.4f}\"\n",
    "        )\n",
    "    \n",
    "    save_results(MODEL_NAME, MODEL_PATH, main_metric, aux_metric)\n",
    "    \n",
    "\n",
    "                    \n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from monai.data             import DataLoader, Dataset, decollate_batch\n",
    "from monai.losses           import DiceLoss\n",
    "from monai.metrics          import DiceMetric\n",
    "from pathlib                import Path\n",
    "from labels                 import modify_labels\n",
    "\n",
    "\n",
    "def set_data(train_files, train_transforms, val_files, val_transforms, BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Create dataloader for test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    train_ds = Dataset(data = train_files, transform = train_transforms)\n",
    "    train_dl = DataLoader(dataset = train_ds, batch_size = BATCH_SIZE, shuffle = True, num_workers = 4)\n",
    "\n",
    "    val_ds = Dataset(data = val_files, transform = val_transforms)\n",
    "    val_dl = DataLoader(dataset = val_ds, batch_size = BATCH_SIZE, num_workers = 4, shuffle = False)\n",
    "    \n",
    "    return train_dl, val_dl\n",
    "\n",
    "\n",
    "def set_model_params(model):\n",
    "    \"\"\"\n",
    "    Set model parameters and metrics for evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input image has eight anatomical structures of planning interest\n",
    "    loss_function       = DiceLoss(to_onehot_y = True, softmax = True, include_background=False) # For segmentation Expects BNHW[D] input i.e. batch, channel, height, width, depth, performs softmax on the channel dimension to get a probability distribution\n",
    "    optimizer           = torch.optim.Adam(model.parameters(), (1e-3)/4) # Decreased the loss after getting a somewhat good model\n",
    "    dice_metric_main    = DiceMetric(include_background=False, reduction=\"mean\")# Collect the loss and metric values for every iteration\n",
    "    scheduler           = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 60, eta_min = 1e-6) #** Adopt a cosine annealing learning rate schedule which reduces the learning rate as the training progresses\n",
    "    \n",
    "    return loss_function, optimizer, dice_metric_main, scheduler\n",
    "\n",
    "\n",
    "def save_results(MODEL_NAME, MODEL_PATH, epoch_loss_values, main_metric_values):\n",
    "    \"\"\"\n",
    "    Save performance metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save epoch loss and metric values\n",
    "    pref = f\"{MODEL_NAME.split('.')[0]}\"\n",
    "    with open(MODEL_PATH/f\"{pref}_epoch_loss_train.pkl\", \"wb\") as f:\n",
    "        pickle.dump(epoch_loss_values, f)\n",
    "    with open(MODEL_PATH/f\"{pref}_validate.pkl\", \"wb\") as f:\n",
    "        pickle.dump(main_metric_values, f)\n",
    "\n",
    "\n",
    "def train_model_base(model, device, params, train_files, train_transforms, val_files, val_transforms, organs, pred_main, label_main, model_name):\n",
    "    \"\"\"\n",
    "    Train the model on the training dataset and evaluate the validation dataset.\n",
    "    \"\"\"\n",
    "    BATCH_SIZE      = params['BATCH_SIZE']\n",
    "    MAX_EPOCHS      = params['MAX_EPOCHS']\n",
    "    VAL_INTERVAL    = params['VAL_INTERVAL']\n",
    "    PRINT_INTERVAL  = params['PRINT_INTERVAL']\n",
    "    \n",
    "    train_dl, val_dl = set_data(train_files, train_transforms, val_files, val_transforms, BATCH_SIZE)\n",
    "    loss_function, optimizer, dice_metric_main, scheduler = set_model_params(model)\n",
    "    \n",
    "    # Create model directory\n",
    "    MODEL_PATH = Path(\"models\")\n",
    "    MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create model save path\n",
    "    MODEL_NAME = model_name + \".pth\"\n",
    "    MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "    best_metric             = -1\n",
    "    best_metric_epoch       = -1\n",
    "    epoch_loss_values       = []\n",
    "    main_metric_values      = []\n",
    "    \n",
    "    print(\"-\" * 20)\n",
    "    print(\"Starting model training...\")\n",
    "    \n",
    "    for epoch in range(1,MAX_EPOCHS):\n",
    "        if epoch % PRINT_INTERVAL == 0:\n",
    "            print(\"-\" * 20)\n",
    "            print(f\"Epoch {epoch} / {MAX_EPOCHS}\")\n",
    "        \n",
    "        # Put the model into training mode\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        \n",
    "        for batch in train_dl:\n",
    "            step = step + 1\n",
    "            inputs = batch[\"image\"].permute(0, 1, 4, 2, 3).to(device)\n",
    "            labels = batch[\"mask\"].to(device) # Permute beccause of torch upsample\n",
    "            \n",
    "            main_labels, _ = modify_labels(labels, organs)\n",
    "\n",
    "            # Forward pass\n",
    "            main_seg = model(inputs) \n",
    "            main_seg = main_seg.permute(0, 1, 3, 4, 2) # Permute back to BNHWD\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_function(main_seg, main_labels) \n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Find the gradients of the loss w.r.t the model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Add the loss to the epoch loss\n",
    "            epoch_loss = epoch_loss + loss.item()\n",
    "        \n",
    "        # Compute the average loss of the epoch\n",
    "        epoch_loss = epoch_loss        / step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "\n",
    "        if epoch % PRINT_INTERVAL == 0:\n",
    "            # Print the average loss of the epoch\n",
    "            print(f\"\\nEpoch {epoch} average dice loss for main task: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Step the scheduler after every epoch\n",
    "        scheduler.step()\n",
    "\n",
    "        # Print loss and evaluate model when epoch is divisible by val_interval\n",
    "        if epoch % VAL_INTERVAL == 0:\n",
    "            print(\"-\" * 40)\n",
    "            print(\"Testing on validation data...\")\n",
    "            \n",
    "            # Put the model into evaluation mode\n",
    "            model.eval()\n",
    "            # Disable gradient calculation\n",
    "            with torch.inference_mode():\n",
    "                # Loop through the validation data\n",
    "                for val_data in val_dl:\n",
    "                    val_inputs, val_labels = val_data[\"image\"].permute(0, 1, 4, 2, 3).to(device), val_data[\"mask\"].to(device)\n",
    "                    val_main_labels, _     = modify_labels(val_labels, organs)\n",
    "\n",
    "                    # Forward pass\n",
    "                    val_main_outputs = model(val_inputs)\n",
    "                    val_main_outputs = val_main_outputs.permute(0, 1, 3, 4, 2)\n",
    "\n",
    "                    # Transform main outputs and labels to calculate inference loss\n",
    "                    val_main_outputs    = [pred_main(i) for i in decollate_batch(val_main_outputs)]\n",
    "                    val_main_labels     = [label_main(i) for i in decollate_batch(val_main_labels)]\n",
    "\n",
    "                    # Compute dice metric for current iteration\n",
    "                    dice_metric_main(y_pred = val_main_outputs, y = val_main_labels)\n",
    "\n",
    "                # Compute the average metric value across all iterations\n",
    "                main_metric = dice_metric_main.aggregate().item()\n",
    "                main_metric_values.append(main_metric)\n",
    "                \n",
    "                # Reset the metric for next validation run\n",
    "                dice_metric_main.reset()\n",
    "\n",
    "                # If the metric is better than the best seen so far, save the model\n",
    "                if main_metric > best_metric:\n",
    "                    best_metric = main_metric\n",
    "                    best_metric_epoch = epoch\n",
    "                    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "                    print(\"saved new best metric model\")\n",
    "                \n",
    "                print(\n",
    "                    f\"\\nCurrent epoch: {epoch} current mean dice for main task: {main_metric:.4f}\"\n",
    "                    f\"\\nBest mean dice for main task: {best_metric:.4f} at epoch: {best_metric_epoch}\"\n",
    "                    )\n",
    "                \n",
    "    # When training is complete:\n",
    "    print(f\"Done training! Best mean dice: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
    "    \n",
    "    save_results(MODEL_NAME, MODEL_PATH, epoch_loss_values, main_metric_values)\n",
    "    \n",
    "\n",
    "                    \n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from monai.data                 import DataLoader, Dataset, decollate_batch\n",
    "from monai.losses               import DiceLoss\n",
    "from monai.losses.ssim_loss     import SSIMLoss\n",
    "from monai.metrics              import DiceMetric, MSEMetric\n",
    "from monai.metrics.regression   import SSIMMetric\n",
    "from pathlib                    import Path\n",
    "from labels                     import modify_labels\n",
    "\n",
    "\n",
    "def set_data(train_files, train_transforms, val_files, val_transforms, BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Create dataloader for test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    train_ds = Dataset(data = train_files, transform = train_transforms)\n",
    "    train_dl = DataLoader(dataset = train_ds, batch_size = BATCH_SIZE, shuffle = True, num_workers = 4)\n",
    "\n",
    "    val_ds = Dataset(data = val_files, transform = val_transforms)\n",
    "    val_dl = DataLoader(dataset = val_ds, batch_size = BATCH_SIZE, num_workers = 4, shuffle = False)\n",
    "    \n",
    "    return train_dl, val_dl\n",
    "\n",
    "\n",
    "def set_model_params(model, TASK):\n",
    "    \"\"\"\n",
    "    Set model parameters and metrics for evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input image has eight anatomical structures of planning interest\n",
    "    loss_main       = DiceLoss(to_onehot_y = True, softmax = True, include_background=False) \n",
    "    metric_main     = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "    \n",
    "    if TASK == 'SEGMENT':\n",
    "        loss_aux    = DiceLoss(to_onehot_y = True, softmax = True, include_background=False) \n",
    "        metric_aux  = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "    else:\n",
    "        #loss_aux    = SSIMLoss()\n",
    "        loss_aux    = torch.nn.L1Loss()\n",
    "        metric_aux  = MSEMetric()\n",
    "        \n",
    "    optimizer       = torch.optim.Adam(model.parameters(), (1e-3)/4) # Decreased the loss after getting a somewhat good model\n",
    "    scheduler       = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 60, eta_min = 1e-6) #** Adopt a cosine annealing learning rate schedule which reduces the learning rate as the training progresses\n",
    "    \n",
    "    return loss_main, loss_aux, metric_main, metric_aux, optimizer, scheduler\n",
    "\n",
    "\n",
    "def save_results(MODEL_NAME, MODEL_PATH, epoch_loss_values, epoch_aux_loss_values, epoch_total_loss_values, main_metric_values, aux_metric_values):\n",
    "    \"\"\"\n",
    "    Save performance metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save epoch loss and metric values\n",
    "    pref = f\"{MODEL_NAME.split('.')[0]}\"\n",
    "    with open(MODEL_PATH/f\"{pref}_epoch_loss.pkl\", \"wb\") as f:\n",
    "        pickle.dump(epoch_loss_values, f)\n",
    "    with open(MODEL_PATH/f\"{pref}_epoch_aux_loss.pkl\", \"wb\") as f:\n",
    "        pickle.dump(epoch_aux_loss_values, f)\n",
    "    with open(MODEL_PATH/f\"{pref}_epoch_total_loss.pkl\", \"wb\") as f:\n",
    "        pickle.dump(epoch_total_loss_values, f)\n",
    "    with open(MODEL_PATH/f\"{pref}_main_validation.pkl\", \"wb\") as f:\n",
    "        pickle.dump(main_metric_values, f)\n",
    "    with open(MODEL_PATH/f\"{pref}_aux_validation.pkl\", \"wb\") as f:\n",
    "        pickle.dump(aux_metric_values, f)\n",
    "\n",
    "\n",
    "def train_model(model, device, params, train_files, train_transforms, val_files, val_transforms, organs, pred_main, label_main, pred_aux, label_aux, model_name):\n",
    "    \"\"\"\n",
    "    Train the model on the training dataset and evaluate the validation dataset.\n",
    "    \"\"\"\n",
    "    BATCH_SIZE      = params['BATCH_SIZE']\n",
    "    MAX_EPOCHS      = params['MAX_EPOCHS']\n",
    "    VAL_INTERVAL    = params['VAL_INTERVAL']\n",
    "    PRINT_INTERVAL  = params['PRINT_INTERVAL']\n",
    "    TASK            = params['TASK']\n",
    "    \n",
    "    train_dl, val_dl = set_data(train_files, train_transforms, val_files, val_transforms, BATCH_SIZE)\n",
    "    loss_main, loss_aux, metric_main, metric_aux, optimizer, scheduler = set_model_params(model, TASK)\n",
    "    \n",
    "    # Create model directory\n",
    "    MODEL_PATH = Path(\"models\")\n",
    "    MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create model save path\n",
    "    MODEL_NAME = model_name + \".pth\"\n",
    "    MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "    best_metric             = -1\n",
    "    best_metric_epoch       = -1\n",
    "    epoch_loss_values       = []\n",
    "    epoch_aux_loss_values   = []\n",
    "    epoch_total_loss_values = []\n",
    "    main_metric_values      = []\n",
    "    aux_metric_values       = []\n",
    "\n",
    "    # Loss weights\n",
    "    main_weight = 1.1\n",
    "    aux_weight  = 1.5\n",
    "    \n",
    "    print(\"-\" * 20)\n",
    "    print(\"Starting model training...\")\n",
    "    \n",
    "    for epoch in range(1,MAX_EPOCHS):\n",
    "        if epoch % PRINT_INTERVAL == 0:\n",
    "            print(\"-\" * 20)\n",
    "            print(f\"Epoch {epoch} / {MAX_EPOCHS}\")\n",
    "        \n",
    "        # Put the model into training mode\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_aux_loss = 0\n",
    "        epoch_total_loss = 0\n",
    "        step = 0\n",
    "        \n",
    "        for batch in train_dl:\n",
    "            step = step + 1\n",
    "            inputs = batch[\"image\"].permute(0, 1, 4, 2, 3).to(device)\n",
    "            labels = batch[\"mask\"].to(device) # Permute beccause of torch upsample\n",
    "            \n",
    "            main_labels, aux_labels = modify_labels(labels, organs)\n",
    "\n",
    "            # Forward pass\n",
    "            main_seg, aux_seg = model(inputs) \n",
    "            main_seg, aux_seg = main_seg.permute(0, 1, 3, 4, 2), aux_seg.permute(0, 1, 3, 4, 2) # Permute back to BNHWD\n",
    "\n",
    "            # Compute the loss functions\n",
    "            main_seg_loss = loss_main(main_seg, main_labels)\n",
    "            if TASK == 'SEGMENT':\n",
    "                aux_seg_loss = loss_aux(aux_seg, aux_labels)\n",
    "            else:\n",
    "                aux_seg_loss = loss_aux(aux_seg, inputs.permute(0, 1, 3, 4, 2))\n",
    "                \n",
    "            # Compute the total loss\n",
    "            loss = main_weight * main_seg_loss + aux_weight * aux_seg_loss\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Find the gradients of the loss w.r.t the model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Add the loss to the epoch loss\n",
    "            epoch_loss = epoch_loss + main_seg_loss.item()\n",
    "            epoch_aux_loss = epoch_aux_loss + aux_seg_loss.item()\n",
    "            epoch_total_loss = epoch_total_loss + loss.item()\n",
    "        \n",
    "        # Compute the average loss of the epoch\n",
    "        epoch_loss          = epoch_loss        / step\n",
    "        epoch_aux_loss      = epoch_aux_loss    / step\n",
    "        epoch_total_loss    = epoch_total_loss  / step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        epoch_total_loss_values.append(epoch_total_loss)\n",
    "        epoch_aux_loss_values.append(epoch_aux_loss)\n",
    "\n",
    "        if epoch % PRINT_INTERVAL == 0:\n",
    "            # Print the average loss of the epoch\n",
    "            print(f\"\\nEpoch {epoch} average loss for main task: {epoch_loss:.4f}\")\n",
    "            print(f\"\\nEpoch {epoch} average loss for aux task: {epoch_aux_loss:.4f}\")\n",
    "            print(f\"\\nEpoch {epoch} average total loss for both tasks: {epoch_total_loss:.4f}\")\n",
    "\n",
    "        # Step the scheduler after every epoch\n",
    "        scheduler.step()\n",
    "\n",
    "        # Print loss and evaluate model when epoch is divisible by val_interval\n",
    "        if epoch % VAL_INTERVAL == 0:\n",
    "            print(\"-\" * 40)\n",
    "            print(\"Testing on validation data...\")\n",
    "            \n",
    "            # Put the model into evaluation mode\n",
    "            model.eval()\n",
    "            # Disable gradient calculation\n",
    "            with torch.inference_mode():\n",
    "                # Loop through the validation data\n",
    "                for val_data in val_dl:\n",
    "                    val_inputs = val_data[\"image\"].permute(0, 1, 4, 2, 3).to(device)\n",
    "                    val_labels = val_data[\"mask\"].to(device)\n",
    "                    \n",
    "                    val_main_labels, val_aux_labels = modify_labels(val_labels, organs)\n",
    "\n",
    "                    # Forward pass\n",
    "                    val_main_outputs, val_aux_outputs = model(val_inputs)\n",
    "                    val_main_outputs, val_aux_outputs = val_main_outputs.permute(0, 1, 3, 4, 2), val_aux_outputs.permute(0, 1, 3, 4, 2)\n",
    "\n",
    "                    # Transform main outputs and labels to calculate inference loss\n",
    "                    val_main_outputs    = [pred_main(i) for i in decollate_batch(val_main_outputs)]\n",
    "                    val_main_labels     = [label_main(i) for i in decollate_batch(val_main_labels)]\n",
    "\n",
    "                    # Compute metric for current iteration\n",
    "                    metric_main(y_pred = val_main_outputs, y = val_main_labels)\n",
    "                    if TASK == 'SEGMENT':\n",
    "                        # Transform aux outputs and labels to calculate inference loss\n",
    "                        val_aux_outputs     = [pred_aux(i) for i in decollate_batch(val_aux_outputs)]\n",
    "                        val_aux_labels      = [label_aux(i) for i in decollate_batch(val_aux_labels)]\n",
    "                        \n",
    "                        metric_aux(y_pred = val_aux_outputs, y = val_aux_labels)\n",
    "                    else:\n",
    "                        metric_aux(y_pred = val_aux_outputs, y = inputs.permute(0, 1, 3, 4, 2))\n",
    "                        \n",
    "                # Compute the average metric value across all iterations\n",
    "                main_metric = metric_main.aggregate().item()\n",
    "                aux_metric  = metric_aux.aggregate().item()\n",
    "                main_metric_values.append(main_metric)\n",
    "                aux_metric_values.append(aux_metric)\n",
    "                \n",
    "                # Reset the metric for next validation run\n",
    "                metric_main.reset()\n",
    "                metric_aux.reset()\n",
    "\n",
    "                # If the metric is better than the best seen so far, save the model\n",
    "                if main_metric > best_metric:\n",
    "                    best_metric = main_metric\n",
    "                    best_metric_epoch = epoch\n",
    "                    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "                    print(\"saved new best metric model\")\n",
    "                \n",
    "                print(\n",
    "                    f\"\\nCurrent epoch: {epoch} current mean dice for main task: {main_metric:.4f}\"\n",
    "                    f\"\\nBest mean dice for main task: {best_metric:.4f} at epoch: {best_metric_epoch}\"\n",
    "                    f\"\\nCurrent epoch: {epoch} current mean metric for aux task: {aux_metric:.4f}\"\n",
    "                    )\n",
    "                \n",
    "    # When training is complete:\n",
    "    print(f\"Done training! Best mean dice: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
    "    \n",
    "    save_results(MODEL_NAME, MODEL_PATH, epoch_loss_values, epoch_aux_loss_values, epoch_total_loss_values, main_metric_values, aux_metric_values)\n",
    "    \n",
    "\n",
    "                    \n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirstd, # Adjust or add the channel dimension of input data to ensure channel_first shape.\n",
    "    CenterSpatialCropd,\n",
    "    Compose,\n",
    "    AsDiscrete,\n",
    "    LoadImaged,\n",
    "    ScaleIntensityd,\n",
    "    Spacingd,\n",
    "    SpatialPadd,\n",
    "    RandAffined, \n",
    "    CropForegroundd, # Crop the foreground region of the input image based on the provided mask to help training and evaluation if the valid part is small in the whole medical image\n",
    "    RandGaussianNoised, # Randomly add Gaussian noise to image.\n",
    "    RandGaussianSmoothd, # Randomly smooth image with Gaussian filter.\n",
    "    AdjustContrastd, # Adjust image contrast by gamma value.\n",
    ")\n",
    "\n",
    "def get_transforms():\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Creating transformations...\")\n",
    "    \n",
    "    # Create transforms for training\n",
    "    train_transforms = Compose(\n",
    "        [\n",
    "            LoadImaged(keys = [\"image\", \"mask\"]),\n",
    "            EnsureChannelFirstd(keys = [\"image\", \"mask\"]),\n",
    "            ScaleIntensityd(keys = \"image\"),\n",
    "            CropForegroundd(keys = [\"image\", \"mask\"], source_key = \"image\"),\n",
    "            Spacingd(\n",
    "                keys = [\"image\", \"mask\"],\n",
    "                pixdim = [0.75, 0.75, 2.5],\n",
    "                mode = (\"bilinear\", \"nearest\"), # Interpolation mode for image and mask\n",
    "            ),\n",
    "            RandAffined(\n",
    "                keys = [\"image\", \"mask\"],\n",
    "                mode = (\"bilinear\", \"nearest\"),\n",
    "                prob = 1.0,\n",
    "                spatial_size = (256, 256, 40), # Output size of the image [height, width, depth]\n",
    "                rotate_range = (np.pi / 36, np.pi / 36, np.pi / 36), # Rotation range\n",
    "                scale_range = (0.1, 0.1, 0.1), # will do [-0.1, 0.1] scaling then add 1 so a scaling in the range [0.9, 1.1]\n",
    "                padding_mode=\"zeros\", # This means that the image will be padded with zeros, some images are smaller than 256x256x40\n",
    "            ),\n",
    "            RandGaussianNoised(\n",
    "                keys = \"image\",\n",
    "                prob = 0.15,\n",
    "                mean = 0.0,\n",
    "                std = 0.1\n",
    "            ),\n",
    "            RandGaussianSmoothd(\n",
    "                keys = \"image\",\n",
    "                prob = 0.1,\n",
    "                sigma_x=(0.5, 1.5),\n",
    "                sigma_y=(0.5, 1.5),\n",
    "                sigma_z=(0.5, 1.5)\n",
    "            ),\n",
    "            AdjustContrastd(\n",
    "                keys = \"image\",\n",
    "                gamma = 1.3\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create transforms for validation\n",
    "    val_transforms = Compose(\n",
    "        [\n",
    "            LoadImaged(keys = [\"image\", \"mask\"]),\n",
    "            EnsureChannelFirstd(keys = [\"image\", \"mask\"]),\n",
    "            ScaleIntensityd(keys = \"image\"),\n",
    "            Spacingd(\n",
    "                keys = [\"image\", \"mask\"],\n",
    "                pixdim = [0.75, 0.75, 2.5],\n",
    "                mode = (\"bilinear\", \"nearest\"),\n",
    "            ),\n",
    "            # since we are not doing data augmentation during validation,\n",
    "            #we simply center crop the image and mask to the specified size of [256, 256, 40]\n",
    "            CenterSpatialCropd(keys = [\"image\", \"mask\"], roi_size = (256, 256, 40)), \n",
    "            SpatialPadd(keys = [\"image\", \"mask\"], spatial_size= (256, 256, 40)) # Some images are smaller than 256x256x40, so we pad them to this size\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Post transforms for the main prostate zones: 2 classes + background\n",
    "    post_pred_transform_main    = Compose([AsDiscrete(argmax = True, to_onehot = 3)])\n",
    "    post_label_transform_main   = Compose([AsDiscrete(to_onehot = 3)])\n",
    "\n",
    "    # Post transforms for the auxilliary prostate zones: 3 classes + background\n",
    "    post_pred_transform_aux     = Compose([AsDiscrete(argmax = True, to_onehot = 4)])\n",
    "    post_label_transform_aux    = Compose([AsDiscrete(to_onehot = 4)])\n",
    "    \n",
    "    print('Transforms have been defined.')\n",
    "    \n",
    "    return train_transforms, val_transforms, post_pred_transform_main, post_label_transform_main, post_pred_transform_aux, post_label_transform_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from monai.utils        import set_determinism  \n",
    "from split_data         import split_data\n",
    "from transforms         import get_transforms\n",
    "from model              import ResidualAttention3DUnet, MTLResidualAttention3DUnet, MTLResidualAttentionRecon3DUnet\n",
    "from train_model        import train_model\n",
    "from test_model         import test_model\n",
    "from train_model_base   import train_model_base\n",
    "from test_model_base    import test_model_base\n",
    "\n",
    "# Choose whether to train and/or test model(s)\n",
    "TRAIN           = 1\n",
    "TEST            = 1\n",
    "\n",
    "# Choose which models to test\n",
    "BASE_CASE       = 1\n",
    "AUX_SEGMENT_3   = 1\n",
    "AUX_SEGMENT_6   = 1\n",
    "AUX_RECONSTRUCT = 1\n",
    "\n",
    "# Parameters\n",
    "params = {\n",
    "    'BATCH_SIZE':       2,\n",
    "    'MAX_EPOCHS':       100,\n",
    "    'VAL_INT':          10,\n",
    "    'PRINT_INT':        10\n",
    "}\n",
    "\n",
    "# Set deterministic training for reproducibility\n",
    "set_determinism(seed = 2056)\n",
    "\n",
    "# Path to data\n",
    "img_path = Path(\"../data\")\n",
    "train_files, val_files, test_files = split_data(img_path, scale=28)\n",
    "\n",
    "# Create transforms for training\n",
    "train_transforms, val_transforms, pred_main, label_main, pred_aux, label_aux = get_transforms()\n",
    "\n",
    "# Use CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define organ names in the segmentation task\n",
    "all_organs =  [\"Background\", \"Bladder\", \"Bone\", \"Obturator internus\", \"Transition zone\", \"Central gland\", \"Rectum\", \"Seminal vesicle\", \"Neurovascular bundle\"]\n",
    "organs = {\n",
    "    'all': all_organs,\n",
    "    'main': [\"Transition zone\", \"Central gland\"],\n",
    "    'dict': {organ: idx for idx, organ in enumerate(all_organs)}\n",
    "    }\n",
    "\n",
    "############# BASE CASE #############\n",
    "if BASE_CASE:\n",
    "    organs['aux']  = []\n",
    "    params['TASK'] = 'BASE_CASE'\n",
    "    model_name     = 'base_case'\n",
    "    model  = ResidualAttention3DUnet(in_channels = 1, out_channels = len(organs['main'])+1, device=device).to(device) \n",
    "    \n",
    "    if TRAIN:\n",
    "        torch.cuda.empty_cache()\n",
    "        train_model_base(model, device, params, train_files, train_transforms, val_files, val_transforms, organs, pred_main, label_main, model_name)\n",
    "    if TEST:\n",
    "        torch.cuda.empty_cache()\n",
    "        test_model_base(model, device, params, test_files, val_transforms, organs, pred_main, label_main, model_name)\n",
    "\n",
    "\n",
    "############# AUXILIARY TASK - SEGMENT 3 EXTRA STRUCTURES #############\n",
    "if AUX_SEGMENT_3:\n",
    "    organs['aux']  = [\"Rectum\", \"Seminal vesicle\", \"Neurovascular bundle\"]\n",
    "    params['TASK'] = 'SEGMENT'\n",
    "    model_name     = 'auxiliary_segment_3'\n",
    "    model = MTLResidualAttention3DUnet(in_channels = 1, main_out_channels = len(organs['main'])+1, aux_out_channels = len(organs['aux'])+1, device=device).to(device) \n",
    "    \n",
    "    if TRAIN:\n",
    "        torch.cuda.empty_cache()\n",
    "        train_model(model, device, params, train_files, train_transforms, val_files, val_transforms, organs, pred_main, label_main, pred_aux, label_aux, model_name)\n",
    "    if TEST:\n",
    "        torch.cuda.empty_cache()\n",
    "        test_model(model, device, params, test_files, val_transforms, organs, pred_main, label_main, pred_aux, label_aux, model_name)\n",
    "        \n",
    "        \n",
    "############# AUXILIARY TASK - SEGMENT 6 EXTRA STRUCTURES #############\n",
    "if AUX_SEGMENT_6:\n",
    "    organs['aux']  = [\"Rectum\", \"Seminal vesicle\", \"Neurovascular bundle\", \"Bladder\", \"Bone\", \"Obturator internus\"]\n",
    "    params['TASK'] = 'SEGMENT'\n",
    "    model_name     = 'auxiliary_segment_6'\n",
    "    model = MTLResidualAttention3DUnet(in_channels = 1, main_out_channels = len(organs['main'])+1, aux_out_channels = len(organs['aux'])+1, device=device).to(device) \n",
    "    \n",
    "    if TRAIN:\n",
    "        torch.cuda.empty_cache()\n",
    "        train_model(model, device, params, train_files, train_transforms, val_files, val_transforms, organs, pred_main, label_main, pred_aux, label_aux, model_name)\n",
    "    if TEST:\n",
    "        torch.cuda.empty_cache()\n",
    "        test_model(model, device, params, test_files, val_transforms, organs, pred_main, label_main, pred_aux, label_aux, model_name)\n",
    "    \n",
    "    \n",
    "############# AUXILIARY TASK - RECONSTRUCTION #############\n",
    "if AUX_RECONSTRUCT:\n",
    "    organs['aux']   = []\n",
    "    params['TASK'] = 'RECONSTRUCT'\n",
    "    model_name     = 'auxiliary_reconstruct'\n",
    "    model = MTLResidualAttentionRecon3DUnet(in_channels = 1, out_channels = len(organs['main'])+1, device=device).to(device) \n",
    "    \n",
    "    if TRAIN:\n",
    "        torch.cuda.empty_cache()\n",
    "        train_model(model, device, params, train_files, train_transforms, val_files, val_transforms, organs, pred_main, label_main, pred_aux, label_aux, model_name)\n",
    "    if TEST:\n",
    "        torch.cuda.empty_cache()\n",
    "        test_model(model, device, params, test_files, val_transforms, organs, pred_main, label_main, pred_aux, label_aux, model_name)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
