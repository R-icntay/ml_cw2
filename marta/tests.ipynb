{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from monai.utils        import set_determinism  \n",
    "from split_data         import split_data\n",
    "from transforms         import get_transforms\n",
    "from model              import ResidualAttention3DUnet, MTLResidualAttention3DUnet, MTLResidualAttentionRecon3DUnet\n",
    "from train_model        import train_model\n",
    "from test_model         import test_model\n",
    "from train_model_base   import train_model_base\n",
    "from test_model_base    import test_model_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Splitting data into train-validate-test sets...\n",
      "The file does not exist\n",
      "The file does not exist\n",
      "Images have been divided into train-validate-test sets.\n",
      "Total number of images:  585\n",
      "Number of images train-validate-test:  16 - 2 - 2\n",
      "----------------------------------------\n",
      "Creating transformations...\n",
      "Transforms have been defined.\n"
     ]
    }
   ],
   "source": [
    "# Choose whether to train and/or test model(s)\n",
    "TRAIN           = 1\n",
    "TEST            = 1\n",
    "\n",
    "# Choose which models to test\n",
    "BASE_CASE       = 1\n",
    "AUX_SEGMENT     = 1\n",
    "AUX_RECONSTRUCT = 1\n",
    "\n",
    "# Parameters\n",
    "params = {\n",
    "    'BATCH_SIZE':       2,\n",
    "    'MAX_EPOCHS':       2,\n",
    "    'VAL_INTERVAL':     1,\n",
    "    'PRINT_INTERVAL':   1\n",
    "}\n",
    "\n",
    "# Set deterministic training for reproducibility\n",
    "set_determinism(seed = 2056)\n",
    "\n",
    "# Path to data\n",
    "img_path = Path(\"../data\")\n",
    "train_files, val_files, test_files = split_data(img_path, scale=28)\n",
    "\n",
    "# Create transforms for training\n",
    "train_transforms, val_transforms, pred_main, label_main, pred_aux, label_aux = get_transforms()\n",
    "\n",
    "# Use CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define organ names in the segmentation task\n",
    "all_organs =  [\"Background\", \"Bladder\", \"Bone\", \"Obturator internus\", \"Transition zone\", \"Central gland\", \"Rectum\", \"Seminal vesicle\", \"Neurovascular bundle\"]\n",
    "organs = {\n",
    "    'all': all_organs,\n",
    "    'main': [\"Transition zone\", \"Central gland\"],\n",
    "    'aux': [],\n",
    "    'dict': {organ: idx for idx, organ in enumerate(all_organs)}\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASE CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = ResidualAttention3DUnet(in_channels = 1, out_channels = len(organs['main'])+1, device=device).to(device) \n",
    "\n",
    "if TRAIN:\n",
    "    torch.cuda.empty_cache()\n",
    "    train_model_base(model, device, params, train_files, train_transforms, val_files, val_transforms, organs, pred_main, label_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:        \n",
    "        torch.cuda.empty_cache()\n",
    "        test_model_base(model, device, params, test_files, val_transforms, organs, pred_main, label_main)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUXILIARY - SEGMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Starting model training...\n",
      "--------------------\n",
      "Epoch 1 / 2\n",
      "2023-04-25 17:08:54,091 - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:1295: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  ret = func(*args, **kwargs)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/monai/data/__init__.py:127: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if storage.is_cuda:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 average loss for main task: 1.0000\n",
      "\n",
      "Epoch 1 average loss for aux task: 1.0000\n",
      "\n",
      "Epoch 1 average total loss for both tasks: 2.6000\n",
      "----------------------------------------\n",
      "Testing on validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:1295: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  ret = func(*args, **kwargs)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/monai/data/__init__.py:127: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if storage.is_cuda:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "\n",
      "Current epoch: 1 current mean dice for main task: 0.0000\n",
      "Best mean dice for main task: 0.0000 at epoch: 1\n",
      "Current epoch: 1 current mean metric for aux task: 0.0000\n",
      "Done training! Best mean dice: 0.0000 at epoch: 1\n"
     ]
    }
   ],
   "source": [
    "organs['aux']  = [\"Rectum\", \"Seminal vesicle\", \"Neurovascular bundle\"]\n",
    "params['TASK'] = 'SEGMENT'\n",
    "model = MTLResidualAttention3DUnet(in_channels = 1, main_out_channels = len(organs['main'])+1, aux_out_channels = len(organs['aux'])+1, device=device).to(device) \n",
    "\n",
    "if TRAIN:\n",
    "    torch.cuda.empty_cache()\n",
    "    train_model(model, device, params, train_files, train_transforms, val_files, val_transforms, organs, pred_main, label_main, pred_aux, label_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Starting model testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-25 17:16:41,198 - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:1295: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  ret = func(*args, **kwargs)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/monai/data/__init__.py:127: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if storage.is_cuda:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean dice for main task: 0.0000\n",
      "Mean metric for aux task: 0.0000\n"
     ]
    }
   ],
   "source": [
    "if TEST:\n",
    "    torch.cuda.empty_cache()\n",
    "    test_model(model, device, params, test_files, val_transforms, organs, pred_main, label_main, pred_aux, label_aux)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUXILIARY - RECONSTRUCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Starting model training...\n",
      "--------------------\n",
      "Epoch 1 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-25 20:04:50,226 - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:1295: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  ret = func(*args, **kwargs)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/monai/data/__init__.py:127: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if storage.is_cuda:\n",
      "/opt/anaconda3/lib/python3.9/site-packages/monai/data/__init__.py:120: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  t = cls([], dtype=storage.dtype, device=storage.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 average loss for main task: 1.0000\n",
      "\n",
      "Epoch 1 average loss for aux task: 0.1795\n",
      "\n",
      "Epoch 1 average total loss for both tasks: 1.3693\n",
      "----------------------------------------\n",
      "Testing on validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:1295: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  ret = func(*args, **kwargs)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/monai/data/__init__.py:127: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if storage.is_cuda:\n"
     ]
    }
   ],
   "source": [
    "organs['aux'] = []\n",
    "params['TASK'] = 'RECONSTRUCT'\n",
    "    \n",
    "model = MTLResidualAttentionRecon3DUnet(in_channels = 1, out_channels = len(organs['main'])+1, device=device).to(device) \n",
    "\n",
    "if TRAIN:\n",
    "    torch.cuda.empty_cache()\n",
    "    train_model(model, device, params, train_files, train_transforms, val_files, val_transforms, organs, pred_main, label_main, pred_aux, label_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MTLResidualAttentionRecon3DUnet:\n\tMissing key(s) in state_dict: \"attention_blocks.0.W_g.up_sample.weight\", \"attention_blocks.0.W_g_norm.weight\", \"attention_blocks.0.W_g_norm.bias\", \"attention_blocks.0.W_x.weight\", \"attention_blocks.0.W_x_norm.weight\", \"attention_blocks.0.W_x_norm.bias\", \"attention_blocks.0.phi.weight\", \"attention_blocks.0.final_norm.weight\", \"attention_blocks.0.final_norm.bias\", \"attention_blocks.1.W_g.up_sample.weight\", \"attention_blocks.1.W_g_norm.weight\", \"attention_blocks.1.W_g_norm.bias\", \"attention_blocks.1.W_x.weight\", \"attention_blocks.1.W_x_norm.weight\", \"attention_blocks.1.W_x_norm.bias\", \"attention_blocks.1.phi.weight\", \"attention_blocks.1.final_norm.weight\", \"attention_blocks.1.final_norm.bias\", \"attention_blocks.2.W_g.up_sample.weight\", \"attention_blocks.2.W_g_norm.weight\", \"attention_blocks.2.W_g_norm.bias\", \"attention_blocks.2.W_x.weight\", \"attention_blocks.2.W_x_norm.weight\", \"attention_blocks.2.W_x_norm.bias\", \"attention_blocks.2.phi.weight\", \"attention_blocks.2.final_norm.weight\", \"attention_blocks.2.final_norm.bias\", \"attention_blocks.3.W_g.up_sample.weight\", \"attention_blocks.3.W_g_norm.weight\", \"attention_blocks.3.W_g_norm.bias\", \"attention_blocks.3.W_x.weight\", \"attention_blocks.3.W_x_norm.weight\", \"attention_blocks.3.W_x_norm.bias\", \"attention_blocks.3.phi.weight\", \"attention_blocks.3.final_norm.weight\", \"attention_blocks.3.final_norm.bias\", \"upsamples.0.up_sample.weight\", \"upsamples.1.up_sample.weight\", \"upsamples.2.up_sample.weight\", \"upsamples.3.up_sample.weight\", \"up_conv.0.first_conv.weight\", \"up_conv.0.first_norm.weight\", \"up_conv.0.first_norm.bias\", \"up_conv.0.second_conv.weight\", \"up_conv.0.second_norm.weight\", \"up_conv.0.second_norm.bias\", \"up_conv.0.shortcut.weight\", \"up_conv.1.first_conv.weight\", \"up_conv.1.first_norm.weight\", \"up_conv.1.first_norm.bias\", \"up_conv.1.second_conv.weight\", \"up_conv.1.second_norm.weight\", \"up_conv.1.second_norm.bias\", \"up_conv.1.shortcut.weight\", \"up_conv.2.first_conv.weight\", \"up_conv.2.first_norm.weight\", \"up_conv.2.first_norm.bias\", \"up_conv.2.second_conv.weight\", \"up_conv.2.second_norm.weight\", \"up_conv.2.second_norm.bias\", \"up_conv.2.shortcut.weight\", \"up_conv.3.first_conv.weight\", \"up_conv.3.first_norm.weight\", \"up_conv.3.first_norm.bias\", \"up_conv.3.second_conv.weight\", \"up_conv.3.second_norm.weight\", \"up_conv.3.second_norm.bias\", \"up_conv.3.shortcut.weight\", \"final_conv.weight\", \"attention_blocks_recon.0.W_g.up_sample.weight\", \"attention_blocks_recon.0.W_g_norm.weight\", \"attention_blocks_recon.0.W_g_norm.bias\", \"attention_blocks_recon.0.W_x.weight\", \"attention_blocks_recon.0.W_x_norm.weight\", \"attention_blocks_recon.0.W_x_norm.bias\", \"attention_blocks_recon.0.phi.weight\", \"attention_blocks_recon.0.final_norm.weight\", \"attention_blocks_recon.0.final_norm.bias\", \"attention_blocks_recon.1.W_g.up_sample.weight\", \"attention_blocks_recon.1.W_g_norm.weight\", \"attention_blocks_recon.1.W_g_norm.bias\", \"attention_blocks_recon.1.W_x.weight\", \"attention_blocks_recon.1.W_x_norm.weight\", \"attention_blocks_recon.1.W_x_norm.bias\", \"attention_blocks_recon.1.phi.weight\", \"attention_blocks_recon.1.final_norm.weight\", \"attention_blocks_recon.1.final_norm.bias\", \"attention_blocks_recon.2.W_g.up_sample.weight\", \"attention_blocks_recon.2.W_g_norm.weight\", \"attention_blocks_recon.2.W_g_norm.bias\", \"attention_blocks_recon.2.W_x.weight\", \"attention_blocks_recon.2.W_x_norm.weight\", \"attention_blocks_recon.2.W_x_norm.bias\", \"attention_blocks_recon.2.phi.weight\", \"attention_blocks_recon.2.final_norm.weight\", \"attention_blocks_recon.2.final_norm.bias\", \"attention_blocks_recon.3.W_g.up_sample.weight\", \"attention_blocks_recon.3.W_g_norm.weight\", \"attention_blocks_recon.3.W_g_norm.bias\", \"attention_blocks_recon.3.W_x.weight\", \"attention_blocks_recon.3.W_x_norm.weight\", \"attention_blocks_recon.3.W_x_norm.bias\", \"attention_blocks_recon.3.phi.weight\", \"attention_blocks_recon.3.final_norm.weight\", \"attention_blocks_recon.3.final_norm.bias\", \"upsamples_recon.0.up_sample.weight\", \"upsamples_recon.1.up_sample.weight\", \"upsamples_recon.2.up_sample.weight\", \"upsamples_recon.3.up_sample.weight\", \"up_conv_recon.0.first_conv.weight\", \"up_conv_recon.0.first_norm.weight\", \"up_conv_recon.0.first_norm.bias\", \"up_conv_recon.0.second_conv.weight\", \"up_conv_recon.0.second_norm.weight\", \"up_conv_recon.0.second_norm.bias\", \"up_conv_recon.0.shortcut.weight\", \"up_conv_recon.1.first_conv.weight\", \"up_conv_recon.1.first_norm.weight\", \"up_conv_recon.1.first_norm.bias\", \"up_conv_recon.1.second_conv.weight\", \"up_conv_recon.1.second_norm.weight\", \"up_conv_recon.1.second_norm.bias\", \"up_conv_recon.1.shortcut.weight\", \"up_conv_recon.2.first_conv.weight\", \"up_conv_recon.2.first_norm.weight\", \"up_conv_recon.2.first_norm.bias\", \"up_conv_recon.2.second_conv.weight\", \"up_conv_recon.2.second_norm.weight\", \"up_conv_recon.2.second_norm.bias\", \"up_conv_recon.2.shortcut.weight\", \"up_conv_recon.3.first_conv.weight\", \"up_conv_recon.3.first_norm.weight\", \"up_conv_recon.3.first_norm.bias\", \"up_conv_recon.3.second_conv.weight\", \"up_conv_recon.3.second_norm.weight\", \"up_conv_recon.3.second_norm.bias\", \"up_conv_recon.3.shortcut.weight\", \"final_conv_recon.weight\". \n\tUnexpected key(s) in state_dict: \"attention_blocks_main.0.W_g.up_sample.weight\", \"attention_blocks_main.0.W_g_norm.weight\", \"attention_blocks_main.0.W_g_norm.bias\", \"attention_blocks_main.0.W_x.weight\", \"attention_blocks_main.0.W_x_norm.weight\", \"attention_blocks_main.0.W_x_norm.bias\", \"attention_blocks_main.0.phi.weight\", \"attention_blocks_main.0.final_norm.weight\", \"attention_blocks_main.0.final_norm.bias\", \"attention_blocks_main.1.W_g.up_sample.weight\", \"attention_blocks_main.1.W_g_norm.weight\", \"attention_blocks_main.1.W_g_norm.bias\", \"attention_blocks_main.1.W_x.weight\", \"attention_blocks_main.1.W_x_norm.weight\", \"attention_blocks_main.1.W_x_norm.bias\", \"attention_blocks_main.1.phi.weight\", \"attention_blocks_main.1.final_norm.weight\", \"attention_blocks_main.1.final_norm.bias\", \"attention_blocks_main.2.W_g.up_sample.weight\", \"attention_blocks_main.2.W_g_norm.weight\", \"attention_blocks_main.2.W_g_norm.bias\", \"attention_blocks_main.2.W_x.weight\", \"attention_blocks_main.2.W_x_norm.weight\", \"attention_blocks_main.2.W_x_norm.bias\", \"attention_blocks_main.2.phi.weight\", \"attention_blocks_main.2.final_norm.weight\", \"attention_blocks_main.2.final_norm.bias\", \"attention_blocks_main.3.W_g.up_sample.weight\", \"attention_blocks_main.3.W_g_norm.weight\", \"attention_blocks_main.3.W_g_norm.bias\", \"attention_blocks_main.3.W_x.weight\", \"attention_blocks_main.3.W_x_norm.weight\", \"attention_blocks_main.3.W_x_norm.bias\", \"attention_blocks_main.3.phi.weight\", \"attention_blocks_main.3.final_norm.weight\", \"attention_blocks_main.3.final_norm.bias\", \"upsamples_main.0.up_sample.weight\", \"upsamples_main.1.up_sample.weight\", \"upsamples_main.2.up_sample.weight\", \"upsamples_main.3.up_sample.weight\", \"up_conv_main.0.first_conv.weight\", \"up_conv_main.0.first_norm.weight\", \"up_conv_main.0.first_norm.bias\", \"up_conv_main.0.second_conv.weight\", \"up_conv_main.0.second_norm.weight\", \"up_conv_main.0.second_norm.bias\", \"up_conv_main.0.shortcut.weight\", \"up_conv_main.1.first_conv.weight\", \"up_conv_main.1.first_norm.weight\", \"up_conv_main.1.first_norm.bias\", \"up_conv_main.1.second_conv.weight\", \"up_conv_main.1.second_norm.weight\", \"up_conv_main.1.second_norm.bias\", \"up_conv_main.1.shortcut.weight\", \"up_conv_main.2.first_conv.weight\", \"up_conv_main.2.first_norm.weight\", \"up_conv_main.2.first_norm.bias\", \"up_conv_main.2.second_conv.weight\", \"up_conv_main.2.second_norm.weight\", \"up_conv_main.2.second_norm.bias\", \"up_conv_main.2.shortcut.weight\", \"up_conv_main.3.first_conv.weight\", \"up_conv_main.3.first_norm.weight\", \"up_conv_main.3.first_norm.bias\", \"up_conv_main.3.second_conv.weight\", \"up_conv_main.3.second_norm.weight\", \"up_conv_main.3.second_norm.bias\", \"up_conv_main.3.shortcut.weight\", \"final_conv_main.weight\", \"attention_blocks_aux.0.W_g.up_sample.weight\", \"attention_blocks_aux.0.W_g_norm.weight\", \"attention_blocks_aux.0.W_g_norm.bias\", \"attention_blocks_aux.0.W_x.weight\", \"attention_blocks_aux.0.W_x_norm.weight\", \"attention_blocks_aux.0.W_x_norm.bias\", \"attention_blocks_aux.0.phi.weight\", \"attention_blocks_aux.0.final_norm.weight\", \"attention_blocks_aux.0.final_norm.bias\", \"attention_blocks_aux.1.W_g.up_sample.weight\", \"attention_blocks_aux.1.W_g_norm.weight\", \"attention_blocks_aux.1.W_g_norm.bias\", \"attention_blocks_aux.1.W_x.weight\", \"attention_blocks_aux.1.W_x_norm.weight\", \"attention_blocks_aux.1.W_x_norm.bias\", \"attention_blocks_aux.1.phi.weight\", \"attention_blocks_aux.1.final_norm.weight\", \"attention_blocks_aux.1.final_norm.bias\", \"attention_blocks_aux.2.W_g.up_sample.weight\", \"attention_blocks_aux.2.W_g_norm.weight\", \"attention_blocks_aux.2.W_g_norm.bias\", \"attention_blocks_aux.2.W_x.weight\", \"attention_blocks_aux.2.W_x_norm.weight\", \"attention_blocks_aux.2.W_x_norm.bias\", \"attention_blocks_aux.2.phi.weight\", \"attention_blocks_aux.2.final_norm.weight\", \"attention_blocks_aux.2.final_norm.bias\", \"attention_blocks_aux.3.W_g.up_sample.weight\", \"attention_blocks_aux.3.W_g_norm.weight\", \"attention_blocks_aux.3.W_g_norm.bias\", \"attention_blocks_aux.3.W_x.weight\", \"attention_blocks_aux.3.W_x_norm.weight\", \"attention_blocks_aux.3.W_x_norm.bias\", \"attention_blocks_aux.3.phi.weight\", \"attention_blocks_aux.3.final_norm.weight\", \"attention_blocks_aux.3.final_norm.bias\", \"upsamples_aux.0.up_sample.weight\", \"upsamples_aux.1.up_sample.weight\", \"upsamples_aux.2.up_sample.weight\", \"upsamples_aux.3.up_sample.weight\", \"up_conv_aux.0.first_conv.weight\", \"up_conv_aux.0.first_norm.weight\", \"up_conv_aux.0.first_norm.bias\", \"up_conv_aux.0.second_conv.weight\", \"up_conv_aux.0.second_norm.weight\", \"up_conv_aux.0.second_norm.bias\", \"up_conv_aux.0.shortcut.weight\", \"up_conv_aux.1.first_conv.weight\", \"up_conv_aux.1.first_norm.weight\", \"up_conv_aux.1.first_norm.bias\", \"up_conv_aux.1.second_conv.weight\", \"up_conv_aux.1.second_norm.weight\", \"up_conv_aux.1.second_norm.bias\", \"up_conv_aux.1.shortcut.weight\", \"up_conv_aux.2.first_conv.weight\", \"up_conv_aux.2.first_norm.weight\", \"up_conv_aux.2.first_norm.bias\", \"up_conv_aux.2.second_conv.weight\", \"up_conv_aux.2.second_norm.weight\", \"up_conv_aux.2.second_norm.bias\", \"up_conv_aux.2.shortcut.weight\", \"up_conv_aux.3.first_conv.weight\", \"up_conv_aux.3.first_norm.weight\", \"up_conv_aux.3.first_norm.bias\", \"up_conv_aux.3.second_conv.weight\", \"up_conv_aux.3.second_norm.weight\", \"up_conv_aux.3.second_norm.bias\", \"up_conv_aux.3.shortcut.weight\", \"final_conv_aux.weight\". \n\tsize mismatch for down_conv.0.first_conv.weight: copying a param with shape torch.Size([32, 1, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 1, 3, 3, 3]).\n\tsize mismatch for down_conv.0.first_norm.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for down_conv.0.first_norm.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for down_conv.0.second_conv.weight: copying a param with shape torch.Size([32, 32, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3, 3]).\n\tsize mismatch for down_conv.0.second_norm.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for down_conv.0.second_norm.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for down_conv.0.shortcut.weight: copying a param with shape torch.Size([32, 1, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([16, 1, 1, 1, 1]).\n\tsize mismatch for down_conv.1.first_conv.weight: copying a param with shape torch.Size([64, 32, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 16, 3, 3, 3]).\n\tsize mismatch for down_conv.1.first_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for down_conv.1.first_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for down_conv.1.second_conv.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3, 3]).\n\tsize mismatch for down_conv.1.second_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for down_conv.1.second_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for down_conv.1.shortcut.weight: copying a param with shape torch.Size([64, 32, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 16, 1, 1, 1]).\n\tsize mismatch for down_conv.2.first_conv.weight: copying a param with shape torch.Size([128, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 32, 3, 3, 3]).\n\tsize mismatch for down_conv.2.first_norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for down_conv.2.first_norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for down_conv.2.second_conv.weight: copying a param with shape torch.Size([128, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3, 3]).\n\tsize mismatch for down_conv.2.second_norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for down_conv.2.second_norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for down_conv.2.shortcut.weight: copying a param with shape torch.Size([128, 64, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 32, 1, 1, 1]).\n\tsize mismatch for down_conv.3.first_conv.weight: copying a param with shape torch.Size([256, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3, 3]).\n\tsize mismatch for down_conv.3.first_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for down_conv.3.first_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for down_conv.3.second_conv.weight: copying a param with shape torch.Size([256, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for down_conv.3.second_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for down_conv.3.second_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for down_conv.3.shortcut.weight: copying a param with shape torch.Size([256, 128, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1, 1]).\n\tsize mismatch for bottleneck.first_conv.weight: copying a param with shape torch.Size([512, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3, 3]).\n\tsize mismatch for bottleneck.first_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for bottleneck.first_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for bottleneck.second_conv.weight: copying a param with shape torch.Size([512, 512, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for bottleneck.second_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for bottleneck.second_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for bottleneck.shortcut.weight: copying a param with shape torch.Size([512, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1, 1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mif\u001b[39;00m TEST:\n\u001b[1;32m      6\u001b[0m     torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[0;32m----> 7\u001b[0m     test_model(model, device, params, test_files, val_transforms, organs, pred_main, label_main, pred_aux, label_aux)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-ImperialCollegeLondon/ML in Medical Imaging/Group work/ml_cw2/marta/test_model.py:64\u001b[0m, in \u001b[0;36mtest_model\u001b[0;34m(model, device, params, val_files, val_transforms, organs_dict, pred_main, label_main, pred_aux, label_aux)\u001b[0m\n\u001b[1;32m     61\u001b[0m MODEL_NAME \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodel.pth\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m MODEL_SAVE_PATH \u001b[39m=\u001b[39m MODEL_PATH \u001b[39m/\u001b[39m MODEL_NAME\n\u001b[0;32m---> 64\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(MODEL_SAVE_PATH))\n\u001b[1;32m     65\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     67\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m40\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MTLResidualAttentionRecon3DUnet:\n\tMissing key(s) in state_dict: \"attention_blocks.0.W_g.up_sample.weight\", \"attention_blocks.0.W_g_norm.weight\", \"attention_blocks.0.W_g_norm.bias\", \"attention_blocks.0.W_x.weight\", \"attention_blocks.0.W_x_norm.weight\", \"attention_blocks.0.W_x_norm.bias\", \"attention_blocks.0.phi.weight\", \"attention_blocks.0.final_norm.weight\", \"attention_blocks.0.final_norm.bias\", \"attention_blocks.1.W_g.up_sample.weight\", \"attention_blocks.1.W_g_norm.weight\", \"attention_blocks.1.W_g_norm.bias\", \"attention_blocks.1.W_x.weight\", \"attention_blocks.1.W_x_norm.weight\", \"attention_blocks.1.W_x_norm.bias\", \"attention_blocks.1.phi.weight\", \"attention_blocks.1.final_norm.weight\", \"attention_blocks.1.final_norm.bias\", \"attention_blocks.2.W_g.up_sample.weight\", \"attention_blocks.2.W_g_norm.weight\", \"attention_blocks.2.W_g_norm.bias\", \"attention_blocks.2.W_x.weight\", \"attention_blocks.2.W_x_norm.weight\", \"attention_blocks.2.W_x_norm.bias\", \"attention_blocks.2.phi.weight\", \"attention_blocks.2.final_norm.weight\", \"attention_blocks.2.final_norm.bias\", \"attention_blocks.3.W_g.up_sample.weight\", \"attention_blocks.3.W_g_norm.weight\", \"attention_blocks.3.W_g_norm.bias\", \"attention_blocks.3.W_x.weight\", \"attention_blocks.3.W_x_norm.weight\", \"attention_blocks.3.W_x_norm.bias\", \"attention_blocks.3.phi.weight\", \"attention_blocks.3.final_norm.weight\", \"attention_blocks.3.final_norm.bias\", \"upsamples.0.up_sample.weight\", \"upsamples.1.up_sample.weight\", \"upsamples.2.up_sample.weight\", \"upsamples.3.up_sample.weight\", \"up_conv.0.first_conv.weight\", \"up_conv.0.first_norm.weight\", \"up_conv.0.first_norm.bias\", \"up_conv.0.second_conv.weight\", \"up_conv.0.second_norm.weight\", \"up_conv.0.second_norm.bias\", \"up_conv.0.shortcut.weight\", \"up_conv.1.first_conv.weight\", \"up_conv.1.first_norm.weight\", \"up_conv.1.first_norm.bias\", \"up_conv.1.second_conv.weight\", \"up_conv.1.second_norm.weight\", \"up_conv.1.second_norm.bias\", \"up_conv.1.shortcut.weight\", \"up_conv.2.first_conv.weight\", \"up_conv.2.first_norm.weight\", \"up_conv.2.first_norm.bias\", \"up_conv.2.second_conv.weight\", \"up_conv.2.second_norm.weight\", \"up_conv.2.second_norm.bias\", \"up_conv.2.shortcut.weight\", \"up_conv.3.first_conv.weight\", \"up_conv.3.first_norm.weight\", \"up_conv.3.first_norm.bias\", \"up_conv.3.second_conv.weight\", \"up_conv.3.second_norm.weight\", \"up_conv.3.second_norm.bias\", \"up_conv.3.shortcut.weight\", \"final_conv.weight\", \"attention_blocks_recon.0.W_g.up_sample.weight\", \"attention_blocks_recon.0.W_g_norm.weight\", \"attention_blocks_recon.0.W_g_norm.bias\", \"attention_blocks_recon.0.W_x.weight\", \"attention_blocks_recon.0.W_x_norm.weight\", \"attention_blocks_recon.0.W_x_norm.bias\", \"attention_blocks_recon.0.phi.weight\", \"attention_blocks_recon.0.final_norm.weight\", \"attention_blocks_recon.0.final_norm.bias\", \"attention_blocks_recon.1.W_g.up_sample.weight\", \"attention_blocks_recon.1.W_g_norm.weight\", \"attention_blocks_recon.1.W_g_norm.bias\", \"attention_blocks_recon.1.W_x.weight\", \"attention_blocks_recon.1.W_x_norm.weight\", \"attention_blocks_recon.1.W_x_norm.bias\", \"attention_blocks_recon.1.phi.weight\", \"attention_blocks_recon.1.final_norm.weight\", \"attention_blocks_recon.1.final_norm.bias\", \"attention_blocks_recon.2.W_g.up_sample.weight\", \"attention_blocks_recon.2.W_g_norm.weight\", \"attention_blocks_recon.2.W_g_norm.bias\", \"attention_blocks_recon.2.W_x.weight\", \"attention_blocks_recon.2.W_x_norm.weight\", \"attention_blocks_recon.2.W_x_norm.bias\", \"attention_blocks_recon.2.phi.weight\", \"attention_blocks_recon.2.final_norm.weight\", \"attention_blocks_recon.2.final_norm.bias\", \"attention_blocks_recon.3.W_g.up_sample.weight\", \"attention_blocks_recon.3.W_g_norm.weight\", \"attention_blocks_recon.3.W_g_norm.bias\", \"attention_blocks_recon.3.W_x.weight\", \"attention_blocks_recon.3.W_x_norm.weight\", \"attention_blocks_recon.3.W_x_norm.bias\", \"attention_blocks_recon.3.phi.weight\", \"attention_blocks_recon.3.final_norm.weight\", \"attention_blocks_recon.3.final_norm.bias\", \"upsamples_recon.0.up_sample.weight\", \"upsamples_recon.1.up_sample.weight\", \"upsamples_recon.2.up_sample.weight\", \"upsamples_recon.3.up_sample.weight\", \"up_conv_recon.0.first_conv.weight\", \"up_conv_recon.0.first_norm.weight\", \"up_conv_recon.0.first_norm.bias\", \"up_conv_recon.0.second_conv.weight\", \"up_conv_recon.0.second_norm.weight\", \"up_conv_recon.0.second_norm.bias\", \"up_conv_recon.0.shortcut.weight\", \"up_conv_recon.1.first_conv.weight\", \"up_conv_recon.1.first_norm.weight\", \"up_conv_recon.1.first_norm.bias\", \"up_conv_recon.1.second_conv.weight\", \"up_conv_recon.1.second_norm.weight\", \"up_conv_recon.1.second_norm.bias\", \"up_conv_recon.1.shortcut.weight\", \"up_conv_recon.2.first_conv.weight\", \"up_conv_recon.2.first_norm.weight\", \"up_conv_recon.2.first_norm.bias\", \"up_conv_recon.2.second_conv.weight\", \"up_conv_recon.2.second_norm.weight\", \"up_conv_recon.2.second_norm.bias\", \"up_conv_recon.2.shortcut.weight\", \"up_conv_recon.3.first_conv.weight\", \"up_conv_recon.3.first_norm.weight\", \"up_conv_recon.3.first_norm.bias\", \"up_conv_recon.3.second_conv.weight\", \"up_conv_recon.3.second_norm.weight\", \"up_conv_recon.3.second_norm.bias\", \"up_conv_recon.3.shortcut.weight\", \"final_conv_recon.weight\". \n\tUnexpected key(s) in state_dict: \"attention_blocks_main.0.W_g.up_sample.weight\", \"attention_blocks_main.0.W_g_norm.weight\", \"attention_blocks_main.0.W_g_norm.bias\", \"attention_blocks_main.0.W_x.weight\", \"attention_blocks_main.0.W_x_norm.weight\", \"attention_blocks_main.0.W_x_norm.bias\", \"attention_blocks_main.0.phi.weight\", \"attention_blocks_main.0.final_norm.weight\", \"attention_blocks_main.0.final_norm.bias\", \"attention_blocks_main.1.W_g.up_sample.weight\", \"attention_blocks_main.1.W_g_norm.weight\", \"attention_blocks_main.1.W_g_norm.bias\", \"attention_blocks_main.1.W_x.weight\", \"attention_blocks_main.1.W_x_norm.weight\", \"attention_blocks_main.1.W_x_norm.bias\", \"attention_blocks_main.1.phi.weight\", \"attention_blocks_main.1.final_norm.weight\", \"attention_blocks_main.1.final_norm.bias\", \"attention_blocks_main.2.W_g.up_sample.weight\", \"attention_blocks_main.2.W_g_norm.weight\", \"attention_blocks_main.2.W_g_norm.bias\", \"attention_blocks_main.2.W_x.weight\", \"attention_blocks_main.2.W_x_norm.weight\", \"attention_blocks_main.2.W_x_norm.bias\", \"attention_blocks_main.2.phi.weight\", \"attention_blocks_main.2.final_norm.weight\", \"attention_blocks_main.2.final_norm.bias\", \"attention_blocks_main.3.W_g.up_sample.weight\", \"attention_blocks_main.3.W_g_norm.weight\", \"attention_blocks_main.3.W_g_norm.bias\", \"attention_blocks_main.3.W_x.weight\", \"attention_blocks_main.3.W_x_norm.weight\", \"attention_blocks_main.3.W_x_norm.bias\", \"attention_blocks_main.3.phi.weight\", \"attention_blocks_main.3.final_norm.weight\", \"attention_blocks_main.3.final_norm.bias\", \"upsamples_main.0.up_sample.weight\", \"upsamples_main.1.up_sample.weight\", \"upsamples_main.2.up_sample.weight\", \"upsamples_main.3.up_sample.weight\", \"up_conv_main.0.first_conv.weight\", \"up_conv_main.0.first_norm.weight\", \"up_conv_main.0.first_norm.bias\", \"up_conv_main.0.second_conv.weight\", \"up_conv_main.0.second_norm.weight\", \"up_conv_main.0.second_norm.bias\", \"up_conv_main.0.shortcut.weight\", \"up_conv_main.1.first_conv.weight\", \"up_conv_main.1.first_norm.weight\", \"up_conv_main.1.first_norm.bias\", \"up_conv_main.1.second_conv.weight\", \"up_conv_main.1.second_norm.weight\", \"up_conv_main.1.second_norm.bias\", \"up_conv_main.1.shortcut.weight\", \"up_conv_main.2.first_conv.weight\", \"up_conv_main.2.first_norm.weight\", \"up_conv_main.2.first_norm.bias\", \"up_conv_main.2.second_conv.weight\", \"up_conv_main.2.second_norm.weight\", \"up_conv_main.2.second_norm.bias\", \"up_conv_main.2.shortcut.weight\", \"up_conv_main.3.first_conv.weight\", \"up_conv_main.3.first_norm.weight\", \"up_conv_main.3.first_norm.bias\", \"up_conv_main.3.second_conv.weight\", \"up_conv_main.3.second_norm.weight\", \"up_conv_main.3.second_norm.bias\", \"up_conv_main.3.shortcut.weight\", \"final_conv_main.weight\", \"attention_blocks_aux.0.W_g.up_sample.weight\", \"attention_blocks_aux.0.W_g_norm.weight\", \"attention_blocks_aux.0.W_g_norm.bias\", \"attention_blocks_aux.0.W_x.weight\", \"attention_blocks_aux.0.W_x_norm.weight\", \"attention_blocks_aux.0.W_x_norm.bias\", \"attention_blocks_aux.0.phi.weight\", \"attention_blocks_aux.0.final_norm.weight\", \"attention_blocks_aux.0.final_norm.bias\", \"attention_blocks_aux.1.W_g.up_sample.weight\", \"attention_blocks_aux.1.W_g_norm.weight\", \"attention_blocks_aux.1.W_g_norm.bias\", \"attention_blocks_aux.1.W_x.weight\", \"attention_blocks_aux.1.W_x_norm.weight\", \"attention_blocks_aux.1.W_x_norm.bias\", \"attention_blocks_aux.1.phi.weight\", \"attention_blocks_aux.1.final_norm.weight\", \"attention_blocks_aux.1.final_norm.bias\", \"attention_blocks_aux.2.W_g.up_sample.weight\", \"attention_blocks_aux.2.W_g_norm.weight\", \"attention_blocks_aux.2.W_g_norm.bias\", \"attention_blocks_aux.2.W_x.weight\", \"attention_blocks_aux.2.W_x_norm.weight\", \"attention_blocks_aux.2.W_x_norm.bias\", \"attention_blocks_aux.2.phi.weight\", \"attention_blocks_aux.2.final_norm.weight\", \"attention_blocks_aux.2.final_norm.bias\", \"attention_blocks_aux.3.W_g.up_sample.weight\", \"attention_blocks_aux.3.W_g_norm.weight\", \"attention_blocks_aux.3.W_g_norm.bias\", \"attention_blocks_aux.3.W_x.weight\", \"attention_blocks_aux.3.W_x_norm.weight\", \"attention_blocks_aux.3.W_x_norm.bias\", \"attention_blocks_aux.3.phi.weight\", \"attention_blocks_aux.3.final_norm.weight\", \"attention_blocks_aux.3.final_norm.bias\", \"upsamples_aux.0.up_sample.weight\", \"upsamples_aux.1.up_sample.weight\", \"upsamples_aux.2.up_sample.weight\", \"upsamples_aux.3.up_sample.weight\", \"up_conv_aux.0.first_conv.weight\", \"up_conv_aux.0.first_norm.weight\", \"up_conv_aux.0.first_norm.bias\", \"up_conv_aux.0.second_conv.weight\", \"up_conv_aux.0.second_norm.weight\", \"up_conv_aux.0.second_norm.bias\", \"up_conv_aux.0.shortcut.weight\", \"up_conv_aux.1.first_conv.weight\", \"up_conv_aux.1.first_norm.weight\", \"up_conv_aux.1.first_norm.bias\", \"up_conv_aux.1.second_conv.weight\", \"up_conv_aux.1.second_norm.weight\", \"up_conv_aux.1.second_norm.bias\", \"up_conv_aux.1.shortcut.weight\", \"up_conv_aux.2.first_conv.weight\", \"up_conv_aux.2.first_norm.weight\", \"up_conv_aux.2.first_norm.bias\", \"up_conv_aux.2.second_conv.weight\", \"up_conv_aux.2.second_norm.weight\", \"up_conv_aux.2.second_norm.bias\", \"up_conv_aux.2.shortcut.weight\", \"up_conv_aux.3.first_conv.weight\", \"up_conv_aux.3.first_norm.weight\", \"up_conv_aux.3.first_norm.bias\", \"up_conv_aux.3.second_conv.weight\", \"up_conv_aux.3.second_norm.weight\", \"up_conv_aux.3.second_norm.bias\", \"up_conv_aux.3.shortcut.weight\", \"final_conv_aux.weight\". \n\tsize mismatch for down_conv.0.first_conv.weight: copying a param with shape torch.Size([32, 1, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 1, 3, 3, 3]).\n\tsize mismatch for down_conv.0.first_norm.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for down_conv.0.first_norm.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for down_conv.0.second_conv.weight: copying a param with shape torch.Size([32, 32, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3, 3]).\n\tsize mismatch for down_conv.0.second_norm.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for down_conv.0.second_norm.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for down_conv.0.shortcut.weight: copying a param with shape torch.Size([32, 1, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([16, 1, 1, 1, 1]).\n\tsize mismatch for down_conv.1.first_conv.weight: copying a param with shape torch.Size([64, 32, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 16, 3, 3, 3]).\n\tsize mismatch for down_conv.1.first_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for down_conv.1.first_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for down_conv.1.second_conv.weight: copying a param with shape torch.Size([64, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3, 3]).\n\tsize mismatch for down_conv.1.second_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for down_conv.1.second_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for down_conv.1.shortcut.weight: copying a param with shape torch.Size([64, 32, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 16, 1, 1, 1]).\n\tsize mismatch for down_conv.2.first_conv.weight: copying a param with shape torch.Size([128, 64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 32, 3, 3, 3]).\n\tsize mismatch for down_conv.2.first_norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for down_conv.2.first_norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for down_conv.2.second_conv.weight: copying a param with shape torch.Size([128, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3, 3]).\n\tsize mismatch for down_conv.2.second_norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for down_conv.2.second_norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for down_conv.2.shortcut.weight: copying a param with shape torch.Size([128, 64, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 32, 1, 1, 1]).\n\tsize mismatch for down_conv.3.first_conv.weight: copying a param with shape torch.Size([256, 128, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3, 3]).\n\tsize mismatch for down_conv.3.first_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for down_conv.3.first_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for down_conv.3.second_conv.weight: copying a param with shape torch.Size([256, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for down_conv.3.second_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for down_conv.3.second_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for down_conv.3.shortcut.weight: copying a param with shape torch.Size([256, 128, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1, 1]).\n\tsize mismatch for bottleneck.first_conv.weight: copying a param with shape torch.Size([512, 256, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3, 3]).\n\tsize mismatch for bottleneck.first_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for bottleneck.first_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for bottleneck.second_conv.weight: copying a param with shape torch.Size([512, 512, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for bottleneck.second_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for bottleneck.second_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for bottleneck.shortcut.weight: copying a param with shape torch.Size([512, 256, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1, 1])."
     ]
    }
   ],
   "source": [
    "organs['aux'] = []\n",
    "params['TASK'] = 'RECONSTRUCT'\n",
    "model = MTLResidualAttentionRecon3DUnet(in_channels = 1, out_channels = len(organs['main'])+1, device=device).to(device) \n",
    "\n",
    "if TEST:\n",
    "    torch.cuda.empty_cache()\n",
    "    test_model(model, device, params, test_files, val_transforms, organs, pred_main, label_main, pred_aux, label_aux)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
